<!DOCTYPE html>
<html lang="ko-KR" data-theme="light">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0,viewport-fit=cover">
		<meta http-equiv="X-UA-Compatible" content="IE=edge">
		<title>심층 신경망(딥러닝) 학습 관련 기술(optimization) - 매개변수 갱신 - Pandoly2 - 생각나는대로</title>
		<meta name="description" content="생각나는대로 적는 공간">
		<meta name="keywords" content="pandoly2machine learningAI">
		<base href="http://localhost:4000" />
		
    	<meta content="2022-04-23T00:00:00+00:00" property="article:published_time">
    	<meta content="http://localhost:4000/about/" property="article:author">
  		
		<meta property="og:site_name" content="Pandoly2 - 생각나는대로">
		<meta property="og:type" content="article" />
		<meta property="og:url" content="http://localhost:4000/optimization-parameter/"/>
		<meta property="og:title" content="심층 신경망(딥러닝) 학습 관련 기술(optimization) - 매개변수 갱신 - Pandoly2 - 생각나는대로" />
		<meta property="og:description" content="생각나는대로 적는 공간" />
		<meta property="og:image" content="http://localhost:4000/assets/images/favicon.png"/>
		<meta name="twitter:card" content="summary"/>
		<meta property="twitter:title" content="심층 신경망(딥러닝) 학습 관련 기술(optimization) - 매개변수 갱신 - Pandoly2 - 생각나는대로" />
		<meta property="twitter:description" content="생각나는대로 적는 공간" />
		<meta property="twitter:image" content="http://localhost:4000/assets/images/favicon.png" />
		<link rel="stylesheet" href="assets/css/highlight.css">
		<link rel="stylesheet" href="assets/css/style.css">
		<link rel="shortcut icon" href="assets/images/favicon.png" />
		<link rel="alternate" type="application/atom+xml" title="Pandoly2 - 생각나는대로" href="http://localhost:4000/atom.xml">
		<link rel="alternate" type="application/json" title="Pandoly2 - 생각나는대로" href="http://localhost:4000/feed.json" />
		<link rel="sitemap" type="application/xml" title="sitemap" href="http://localhost:4000/sitemap.xml" />
		<meta name="google-site-verification" content="XXX" />
	</head>
<body>
<div class="container">
	<div class="profile">
		<a href="http://localhost:4000"><img src="assets/images/profile_paris.jpg" class="profile-image"></a>
		<div class="profile-about">
			<h2 style="margin-bottom: 0; font-weight: 700;">pandoly2</h2>
			
			
			
				<a href="https://instagram.com/pandoly2" target="_blank"><img src="assets/images/icon/instagram.svg" class="social-icon"></a>
			
			
			
				<a href="https://github.com/pandoly2" target="_blank"><img src="assets/images/icon/github.svg" class="social-icon"></a>
			
			
			<a href="about"><img src="assets/images/icon/me.svg" class="social-icon"></a>
			<br>
			Software Engineer
			<br>
			<div class="mode" id="mode-switcher" onclick="toggleNightMode();">
				<span></span>
			</div>
		</div>
	</div>
	
<div class="post-header">
	<div class="post-date">23.04.2022</div>
	<div class="post-share">
		Share:&nbsp; 
		<a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/01-신경망학습기술" target="_blank"><img src="assets/images/icon/facebook.svg" class="social-icon"></a>
		<a href="https://twitter.com/intent/tweet?source=tweetbutton&amp;original_referer=http://localhost:4000/01-신경망학습기술&amp;text=심층 신경망(딥러닝) 학습 관련 기술(optimization) - 매개변수 갱신 - http://localhost:4000/01-신경망학습기술" target="_blank"><img src="assets/images/icon/twitter.svg" class="social-icon"></a>
		<a href="https://api.whatsapp.com/send?text=http://localhost:4000/01-신경망학습기술" target="_blank"><img src="assets/images/icon/whatsapp.svg" class="social-icon"></a>
	</div>
</div>

<div class="blog-post-content">
	<h1>심층 신경망(딥러닝) 학습 관련 기술(optimization) - 매개변수 갱신</h1>
	<ul>
  <li>심층 신경망에서의 학습은 실제값과 우리모델이 예측한 예측값의 차이를 줄여주는것에 목적이 있다.</li>
  <li>우리는 이미 linear regression이나 logistic regression에서 학습할때 손실함수(loss fuction)을 이용하여 예측값 y와 손실함수의 결과값의 차이가 작아 질 수 있도록 W(weight: 가중치)와 b(bias : 바이어스)를 업데이트하면서 성공률을 높이는 방법을 공부했다.</li>
  <li>우리가 linear regression에서 사용한 손실함수(loss function)를 줄이는 기술은 <strong>경사 하강법 (Gradient decent)</strong>이다.</li>
  <li>우리는 심층 신경망에서 보다 더 정확도가 높고, 안정적이며, 학습 속도가 빠른 학습방법 및 기술이 어떤것이 있는지 공부해볼 필요가 있다.</li>
</ul>

<h2 id="1-매개변수-갱신">1. 매개변수 갱신</h2>
<h3 id="경사하강법">경사하강법</h3>
<h4 id="경사하강법-gradient-decent">경사하강법 (Gradient Decent)</h4>
<ul>
  <li>평균제곱 오차 <a href="https://pandoly2.github.io/Regression-and-lossfunction/">[https://pandoly2.github.io/Regression-and-lossfunction/]</a></li>
</ul>

<h4 id="배치-경사-하강법-batch-gradient-decent">배치 경사 하강법 (Batch Gradient Decent)</h4>
<ul>
  <li>경사 하강법의 손실 함수의 기울기 계산에 배치를 전체 학습 데이터셋의 크기와 동일하게 잡는 방법이다.</li>
  <li>즉, 경사 하강법에 사용되는 전체 데이터의 크기가 배치 크기와 동일하다는 것이다.</li>
  <li><strong>장점</strong> : 모든데이터가 대상이므로 안정적으로 수렴한다. <br />
      수렴(최소값)까지 발생하는 가중치 및 바이어스 업데이트 수가 매우 적다.</li>
  <li><strong>단점</strong> : 데이터셋 전체를 대상으로 하다 보니 가중치 및 바이어스가 변경될때마다, 계산해야 할 값이 많으므로, 계산 시간도 길어지고, 소모되는 메모리도 많다. <br />
 지역 최소해(local Minimum)에 빠지면 빠저나오기가 힘들다. <br />
 학습 데이서텟이 커지면 커질수록 시간과 리소스 소모가 크다.</li>
</ul>

<p><img src="/assets/images/blog_images/DeepNeuralNetwork/gd_batch.png" alt="Deep_batch_gradient" />
출처 : 만년필잉크의 데이터 분석 지식 저장소</p>

<h4 id="확률적-경사-하강법-stochastic-gradient-descent-sdg">확률적 경사 하강법 (Stochastic Gradient Descent, SDG)</h4>
<ul>
  <li>학습 데이터셋에서 무작위로 한 개의 샘플 데이터 셋을 추출하고, 그 <strong>샘플에 대해서만 기울기를 계산</strong> 하느것</li>
  <li>
    <p><strong>장점</strong> : 샘플 데이터 셋에 대해서만 기울기를 계산하므로 계산해야할 데이터 수가 적다. <br />
 큰 데이터셋이라도 하나의 샘플씩 계산하기때문에 메모리 소모량이 매우 낮다. <br />
 무작위 샘플 계산으로 불안정하지만 지역 최소값에서 빠져나올 가능성이 BGD보다 높다.</p>
  </li>
  <li><strong>단점</strong> : 무작위 추출로 데이터를 계산하기때문에 불안정하게 움직이며 수렴한다.  <br />
 무작위 샘플링이기때문에 최적해에 도달하지 못할 수도 있다.</li>
</ul>

<p><img src="/assets/images/blog_images/DeepNeuralNetwork/gd_sto.png" alt="Deep_stochastic_gradient" />
출처 : 만년필잉크의 데이터 분석 지식 저장소</p>

<h4 id="미니-배치-경사-하강법-mini-batch-gradient-descent">미니 배치 경사 하강법 (mini-Batch Gradient Descent)</h4>
<ul>
  <li>BGD의 배치크기를 줄이고 SGD를 활용하는 방법 (BGD, SGD 둘다 사용)</li>
  <li>예) 1000개의 전체 학습데이터가 있다면, 100개씩 10개의 mini batch로 나누어 SGD를 진행한다.</li>
  <li>일반적으로 확률적 경사 하강법(SGD)은 실제로 미니 배치 경사 하강법(mini-BGD)이다. 통상 SDG == mini-BDG 이다.</li>
  <li><strong>장점</strong> : mini batch 크기에대해 SDG를 진행할때, 한 mini batch의 평균에대한 경사도로 하강을 진행하기 때문에 SDG(mini batch없는)보다 안정적으로 최적값에 수렴한다.  <br />
 그로인해 안정적으로 최적값에 수렴할수있으나, 지역 최소값(local minimum) 현상이 발생할 수는 있다.</li>
  <li><strong>팁</strong> : 배치 크기는 총 학습 데이터셋의 크기를 배치 크기로 나눴을 때, 딱 떨어지는 크기로 하는것이 좋다.  <br />
 만약 1050개면 뒤에 50개는 버리도록 랜덤으로 데이터셋에서 버리도록 한다.</li>
</ul>

<p><img src="/assets/images/blog_images/DeepNeuralNetwork/gd_mini.png" alt="Deep_mini_gradient" /></p>

<p>참고 : <a href="https://gooopy.tistory.com/70?category=824281">https://gooopy.tistory.com/70?category=824281</a></p>

<!--4월 21일-->
<h4 id="sgd확률적-경사-하강법은-왜-지그재그로-움직이는가">SGD(확률적 경사 하강법)은 왜 지그재그로 움직이는가</h4>
<ul>
  <li>집합 : 
\(\left\{x\ \in \ {R}^n\ :\ f\left(x\right)\ =\ k\right\}\)
을 함수 f의 k-등위선이라고 부른다.</li>
  <li>점 x를 지나는 <strong>등위선과 gradient Df(x)는 항상 수직</strong>이다. <br />
<img src="/assets/images/blog_images/DeepNeuralNetwork/optimizer_gradient_vertical.png" alt="optimizer_gradient_vertical" /></li>
  <li>등위선에 수직으로 점을 움직여 나가면 지그재그로 움직이게 된다.
따라서, Gradient Descent는 진동 현상을 겪으며 매우 비효율적으로 움직인다.</li>
</ul>

<p>참고 : <a href="https://www.youtube.com/watch?v=5fwD1p9ymx8">https://www.youtube.com/watch?v=5fwD1p9ymx8</a></p>

<h3 id="모멘텀momentum">모멘텀(Momentum)</h3>
<ul>
  <li>모멘텀은 ‘운동량’을 뜻하는 단어로, 물리와 관계가 있다.</li>
  <li>물리계에서는 공이 굴러가는 방향은 중력뿐 아니라 기존의 관성에도 영향을 받는다. 
<img src="/assets/images/blog_images/DeepNeuralNetwork/fig%206-4.png" alt="dl_momentum_ball" /> <br />
참고 : 밑바닥부터 시작하는 딥러닝</li>
  <li>Momentum은 Gradient Descent에 현재의 관성을 추가한다.</li>
  <li>Momentum의 수식 :  <br />
\(V_n\ =\ \alpha V_{n-1}\ -\eta \ \nabla f\left(X_n\right),\ \left(v_{-1}\ =\ 0\right)\) <br />
\(X_{n\ +\ 1}\ =\ X_n\ +\ V_n\)</li>
  <li>V_n은 속도 V_n-1은 관성, α는 관성계수</li>
  <li>관성계수 α가 클수록 속도가 관성에 더 많은 영향을 받는다.</li>
  <li>델 \(\nabla\) 연산자 : 참고 <a href="https://daewonjang.gitbooks.io/vector-calculus/content/chapter2.html">https://daewonjang.gitbooks.io/vector-calculus/content/chapter2.html</a></li>
</ul>

<!--4월 22일-->
<p><img src="/assets/images/blog_images/DeepNeuralNetwork/fig%206-5.png" alt="dl_momentum_graph" /> <br />
참고 : 밑바닥부터 시작하는 딥러닝</p>

<h4 id="sdg-vs-momentum-이동-곡선-비교">SDG vs Momentum 이동 곡선 비교</h4>
<p><img src="/assets/images/blog_images/DeepNeuralNetwork/dp_sdg_momentum_compare.png" alt="dl_compare_sgd_momentum" /></p>

<h4 id="nag-nesterov-accelated-gradient---모멘텀momentum의-변형">NAG (Nesterov Accelated Gradient) - (모멘텀(Momentum)의 변형)</h4>
<ul>
  <li>Momentum은 현재위치에서 관성과 gradient의 반대방향을 합한다.</li>
  <li>NAG는 Momentum을 <strong>공격적인 방식</strong>으로 변형한다.</li>
  <li>현재 위치에서의 관성과 관성방향으로 움직인 후의 위치에서의 gradient의 반대방향을 합한다.</li>
  <li>(개인적으로 이해한것 정리) <strong>미리 이동할 곳을가서 그때의 경사에따라서 지금의 위치에서 내가 이동할 방향을 판단한다.</strong></li>
</ul>

<p><img src="/assets/images/blog_images/DeepNeuralNetwork/dl_momentum_compare_nag.jpg" alt="dl_momentum_nag_compare" /></p>

<ul>
  <li>점화식 :  <br />
\(V_n\ =\ \alpha V_{n-1}\ -\ \eta \ \nabla f\left(x_n\ +\ \alpha V_{n-1}\right)\ ,\ \ v_{-1}\ =\ 0\) <br />
\(X_{n+1}\ =\ X_n\ +\ V_n\)</li>
  <li>즉, 현재위치 f(X_n)에서 관성만큼 α*V_n-1 이동후, 그 위치의 gradient값을 기준으로  gradient반대방향으로 이동하겠다는 이론.</li>
</ul>

<h4 id="bengio의-근사적-접근">Bengio의 근사적 접근</h4>
<ul>
  <li>X_n이 아닌 다른 점(X의 다음 이동점)의 gradient를 구하기때문에 신경망에서 구현하기 적합하지않다. (현재 위치가 아니기때문에)</li>
  <li>Xn 에서 momentum step후의 위치를 X’n이라 하자.</li>
  <li>점화식 : <br />
<img src="/assets/images/blog_images/DeepNeuralNetwork/dl_bengio_fx.png" alt="dl_bengio_fx" /></li>
  <li>풀이 : <br />
<img src="/assets/images/blog_images/DeepNeuralNetwork/dl_bengio_fx_sol.png" alt="dl_bengio_fx_sol" /></li>
</ul>

<p>참고 : <a href="https://www.youtube.com/watch?v=TQmCLe1BCJk&amp;list=PLBiQZMT3oSxXNGcmAwI7vzh2LzwcwJpxU&amp;index=2">https://www.youtube.com/watch?v=TQmCLe1BCJk&amp;list=PLBiQZMT3oSxXNGcmAwI7vzh2LzwcwJpxU&amp;index=2</a></p>

<h3 id="adagrad-adaptive-gradient">AdaGrad (Adaptive Gradient)</h3>
<ul>
  <li>아이디어 : <br />
<img src="/assets/images/blog_images/DeepNeuralNetwork/dl_adagrad_picture_01.png" alt="dl_adaptive_gradient_adia" /></li>
  <li>일정한 learning rate를 사용하지 않고 변수마다 그리고, 스텝마다 learning rate가 바뀐다.</li>
  <li>즉, <strong>목표(최소값)에 많이 다다른쪽은 적게 곱하고, 목표점에 가까워지면 가까워질수록 step의 크기를 줄이겠다는 아이디어.</strong></li>
  <li>시간이 지날수록 learning rate는 줄어드는데 큰 변화를 겪은 변수의 learning rate는 대폭 작아지고 작은 변화를 겪은 변수의 learning rate는 소폭으로 작아진다.</li>
  <li>큰 변화를 겪은 변수는 이미 최적에 가까워졌고 작은 변화를 겪은 변수는 최적에 아직 멀다고 생각하기 때문</li>
  <li>점화식 :  <br />
\(h_n\ =\ h_{n-1}\ +\ \nabla f\left(x_n\right)\ \odot \ \nabla f\left(x_n\right),\ \ h_{-1}\ =\ 0\) <br />
\(x_{n+1}\ =\ x_n\ -\ \eta \ \frac{1}{\sqrt{h_n}}\ \odot \ \nabla f\left(x_n\right)\)</li>
  <li>벡터 h_n에는 gradient의 좌표별 제곱이 누적되어 있다. 즉,
\(h_n\ =\ \sum _{k=0}^n\nabla f\left(x_k\right)\ \odot \ \nabla \ f\left(x_k\right)\)</li>
  <li>연산자 \(\odot\) :  <br />
참고 : <a href="https://en.wikipedia.org/wiki/Hadamard_product_(matrices)">(https://en.wikipedia.org/wiki/Hadamard_product_(matrices)</a></li>
</ul>

<p><img src="/assets/images/blog_images/DeepNeuralNetwork/fig%206-6.png" alt="dl_adaptive_gradient_graph" />
출처 : 밑바닥부터 시작하는 딥러닝</p>

<h3 id="rmsprop">RMSProp</h3>
<ul>
  <li>AdaGrad는 과거의 기울기를 제곱하여 계속 더한다. 그래서 학습을 진행할수록 step이 줄어든다.</li>
  <li>만약 무한대로 계속 학습한다면, step이 0에 수렴할것이고 그때부터는 갱신되지 않을것이다.</li>
  <li>즉 오래 학습을 한다고 의미 있는 값이 나오는것이 아니다.</li>
  <li>이문제를 개선한 기법이 <strong>RMSProp</strong>이다.</li>
  <li>이전 누적치와 현재 gradient의 좌표별 제곱의 가중치 평균을 생각한다.</li>
  <li>점화식 :  <br />
\(h_n\ =\ \gamma h_{n-1}\ +\ \left(1\ -\ \gamma \right)\nabla f\left(x_n\right)\ \odot \ \nabla f\left(x_n\right),\ \ h_{-1}\ =\ 0\) <br />
\(x_{n+1}\ =\ x_n\ -\ \eta \ \frac{1}{\sqrt{h_n}}\odot \ \nabla f\left(x_n\right)\)</li>
  <li>forgetting factor (decay rate) γ가 클수록 과거가 중요하고 작을수록 현재가 중요하다.</li>
  <li>내분을 하면 H_n이 한없이 커지지않는다.<strong>(공부할것)</strong></li>
</ul>

<h3 id="adam">Adam</h3>
<ul>
  <li>Momentum과 RMSProp 두가지 방식을 합친것</li>
  <li>복잡하다. 하지만 현재 가장 많이 쓰이는 Optimizer다. (전혀 아담하지않다(?))</li>
  <li>가중치 β_1으로 Momentum을 변형하여 점화식 : 
\(m_n\ =\ \beta _1m_{n-1}\ +\ \left(1-\beta _1\right)\nabla f\left(x_n\right),\ m_{-1}\ =\ 0\)</li>
  <li>을 생각하고 가중치 β_2로 AdaGrad를 변형하여 점화식 : 
\(v_n\ =\ \beta _2v_{n-1}\ +\ \left(1-\beta _2\right)\nabla f\left(x_n\right)\odot \nabla f\left(x_n\right),\ \ v_{-1}\ =\ 0\)</li>
  <li>을 생각한다.</li>
  <li>Adam은 2015년에 제안된 새로운 방법이다.</li>
</ul>

<p>※ 코드 테스트 해볼것 chapter06 - test</p>

<p><img src="/assets/images/blog_images/DeepNeuralNetwork/fig%206-7.png" alt="dl_adam_graph" />
출처 : 밑바닥부터 시작하는 딥러닝</p>

<h3 id="최적화-기법-비교--sgd-momentum-adagrad-adam">최적화 기법 비교 : SGD, Momentum, AdaGrad, Adam</h3>
<p><img src="/assets/images/blog_images/DeepNeuralNetwork/fig%206-8.png" alt="dl_compare_minimun" />
출처 : 밑바닥부터 시작하는 딥러닝</p>

</div>
<div class="tags-container">
	
		<a href="http://localhost:4000/tags/#pandoly2" class="post-tag">pandoly2</a>, 
	
		<a href="http://localhost:4000/tags/#machine+learning" class="post-tag">machine learning</a>, 
	
		<a href="http://localhost:4000/tags/#AI" class="post-tag">AI</a>
	
</div>
<div class="navigation">
	
		<a class="prev" href="/back-propagation/">< Feed Forward and Back Propagation</a>
	 
	<a class="next" href="/optimization-parameter-2/">심층 신경망(딥러닝) 학습 관련 기술(optimization) - 가중치 초깃값 ></a>

</div>

	<section class="comment-area w-100">
		<div class="comment-wrapper">
			<div id="disqus_thread" class="article-comments"></div>
			<script>
			(function() {
			var d = document, s = d.createElement('script');
			s.src = '//XXX.disqus.com/embed.js';
			s.setAttribute('data-timestamp', +new Date());
			(d.head || d.body).appendChild(s);
			})();
			</script>
			<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
		</div>
	</section>

	<div class="footer">
		<span> 생각나는대로 적는 나만의 공간 <font color="red"> ♥ </font> <a href="https://pandoly2.github.io"><b>Blog pandoly2</b></a></span>
	</div>
</div>

<script type="text/javascript">
    window.MathJax = {
      tex: {
        packages: ['base', 'ams']
      },
      loader: {
        load: ['ui/menu', '[tex]/ams']
      },
      startup: {
        ready() {
          MathJax.startup.defaultReady();
          const Macro = MathJax._.input.tex.Symbol.Macro;
          const MapHandler = MathJax._.input.tex.MapHandler.MapHandler;
          const Array = MathJax._.input.tex.ams.AmsMethods.default.Array;
          const env = new Macro('psmallmatrix', Array, [null, '(', ')', 'c', '.333em', '.2em', 'S', 1]);
          MapHandler.getMap('AMSmath-environment').add('psmallmatrix', env);
        }
      }
    };
  </script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>

	<script src="assets/js/ephesus.js"></script>
	<script type="text/javascript">
		if (localStorage.theme === 'dark' || (!('theme' in localStorage) && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
			document.documentElement.setAttribute('data-theme', 'dark');
			document.getElementById('mode-switcher').classList.add('active');
		}
	</script>

	
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXXX-00"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-XXXXXXXX-00');
    </script>
	
</body>
</html>