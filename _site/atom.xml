<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Pandoly2 - 생각나는대로</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2022-04-24T08:11:20+00:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>pandoly2</name>
   <email></email>
 </author>

 
 <entry>
   <title>심층 신경망(딥러닝) 학습 관련 기술(optimization) - 가중치 초깃값</title>
   <link href="http://localhost:4000/optimization-parameter-2/"/>
   <updated>2022-04-24T00:00:00+00:00</updated>
   <id>http://localhost:4000/01-신경망학습기술-2</id>
   <content type="html">&lt;h2 id=&quot;2-가중치의-초깃값&quot;&gt;2. 가중치의 초깃값&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;학습이 이루어지지않은 초기에 W(weight)와 b(bias)를 어떻게 설정하는게 좋을까.&lt;/li&gt;
  &lt;li&gt;초기값 설정의 예
    &lt;ol&gt;
      &lt;li&gt;가중치(Weight)를 평균이 0, 표준편차가 1인 정규분포로 초기화할때의 각층의 데이터 활성화값 분포
&lt;img src=&quot;/assets/images/blog_images/DeepNeuralNetwork2/fig%206-10.png&quot; alt=&quot;dl2_initial_param_ex&quot; /&gt; &lt;br /&gt;
 문제 : &lt;strong&gt;Vanishing Gradient Problem&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;가중치(Weight)를 평균이 0, 표준편차가 0.01인 정규분포로 초기화할때의 각층의 데이터 확성화값 분포
&lt;img src=&quot;/assets/images/blog_images/DeepNeuralNetwork2/fig%206-11.png&quot; alt=&quot;dl2_initial_param_ex&quot; /&gt; &lt;br /&gt;
 문제 : &lt;strong&gt;표현력 제한&lt;/strong&gt;&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;표현력-제한&quot;&gt;표현력 제한&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;첫번째 가중치 행렬의 각 행이 동일하고 두번째부터는 각 단계마다 가중치가 동일한 신경망을 가정해보면, 이 신경망은 layer 1부터 뉴런의 개수가 모두 1개인 신경망과 본질적으로 동일하다.&lt;/li&gt;
  &lt;li&gt;즉, 정보를 담을 수 있는 노드(or 뉴런)의 가중치가 제한된다.&lt;/li&gt;
  &lt;li&gt;표준편차를 너무작게 잡게되면 정보를 담을 수 있는 노드가 부족해지는것을 주의해야 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/DeepNeuralNetwork2/%ED%91%9C%ED%98%84%EB%A0%A5%EC%A0%9C%ED%95%9C.png&quot; alt=&quot;dl2_initial_param_limit_expression&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;vanishing-gradient-problem&quot;&gt;Vanishing gradient problem&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;sigmoid 미분값을 전파함.&lt;/li&gt;
  &lt;li&gt;데이터 셋이 y값 1/0부분에 군집되어있음.&lt;/li&gt;
  &lt;li&gt;sigmoid 미분값을 계속 곱하다보면 0에 수렴하는 값이됨.&lt;/li&gt;
  &lt;li&gt;층이 뒤로갈수록 gradient값이 작아서 학습이 이루어 지지않음.&lt;/li&gt;
  &lt;li&gt;인공신경망의 2번째 겨울에 하나의 주된 원인 (꽤 오랫동안 문제였다고함)
&lt;img src=&quot;/assets/images/blog_images/DeepNeuralNetwork2/vanishing_gradient.PNG&quot; alt=&quot;dl2_vanish_gradient&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lecun-초기값--xavier-초기값&quot;&gt;LeCun 초기값 / Xavier 초기값&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;활성화 함수 Sigmoid일때,&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;평균이 0인 정규분포, 입력 뉴런과 출력 뉴런의 평균의 역수 : 분산
&lt;img src=&quot;/assets/images/blog_images/DeepNeuralNetwork2/Lecun_Xavier.PNG&quot; alt=&quot;dl_lecun_xavier&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;가중치 초가값으로 LeCun/Xavier 초기값을 이용했을때 각층의 데이터 활성화값 분포
&lt;img src=&quot;/assets/images/blog_images/DeepNeuralNetwork2/Xavier_n.PNG&quot; alt=&quot;dl_xavier_n&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;he-초기값&quot;&gt;He 초기값&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;활성화 함수 ReLU 일때, 
&lt;img src=&quot;/assets/images/blog_images/DeepNeuralNetwork2/he_initial.PNG&quot; alt=&quot;dl_he_initial&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Lecun 초기값을 사용했을때,
&lt;img src=&quot;/assets/images/blog_images/DeepNeuralNetwork2/relu_lecun.PNG&quot; alt=&quot;dl_ReLU_lecun&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;He 초기값을 사용했을때, 
&lt;img src=&quot;/assets/images/blog_images/DeepNeuralNetwork2/relu_he.PNG&quot; alt=&quot;dl_ReLU_He&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-배치-정규화&quot;&gt;3. 배치 정규화&lt;/h2&gt;

&lt;h2 id=&quot;4-바른-학습을-위해&quot;&gt;4. 바른 학습을 위해&lt;/h2&gt;
&lt;h3 id=&quot;오버피팅&quot;&gt;오버피팅&lt;/h3&gt;

&lt;h3 id=&quot;가중치-감소&quot;&gt;가중치 감소&lt;/h3&gt;

&lt;h3 id=&quot;드롭아웃&quot;&gt;드롭아웃&lt;/h3&gt;

&lt;h2 id=&quot;5-적절한-하이퍼파라미터-값-찾기&quot;&gt;5. 적절한 하이퍼파라미터 값 찾기&lt;/h2&gt;
</content>
 </entry>
 
 <entry>
   <title>심층 신경망(딥러닝) 학습 관련 기술(optimization) - 매개변수 갱신</title>
   <link href="http://localhost:4000/optimization-parameter/"/>
   <updated>2022-04-23T00:00:00+00:00</updated>
   <id>http://localhost:4000/01-신경망학습기술</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;심층 신경망에서의 학습은 실제값과 우리모델이 예측한 예측값의 차이를 줄여주는것에 목적이 있다.&lt;/li&gt;
  &lt;li&gt;우리는 이미 linear regression이나 logistic regression에서 학습할때 손실함수(loss fuction)을 이용하여 예측값 y와 손실함수의 결과값의 차이가 작아 질 수 있도록 W(weight: 가중치)와 b(bias : 바이어스)를 업데이트하면서 성공률을 높이는 방법을 공부했다.&lt;/li&gt;
  &lt;li&gt;우리가 linear regression에서 사용한 손실함수(loss function)를 줄이는 기술은 &lt;strong&gt;경사 하강법 (Gradient decent)&lt;/strong&gt;이다.&lt;/li&gt;
  &lt;li&gt;우리는 심층 신경망에서 보다 더 정확도가 높고, 안정적이며, 학습 속도가 빠른 학습방법 및 기술이 어떤것이 있는지 공부해볼 필요가 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1-매개변수-갱신&quot;&gt;1. 매개변수 갱신&lt;/h2&gt;
&lt;h3 id=&quot;경사하강법&quot;&gt;경사하강법&lt;/h3&gt;
&lt;h4 id=&quot;경사하강법-gradient-decent&quot;&gt;경사하강법 (Gradient Decent)&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;평균제곱 오차 &lt;a href=&quot;https://pandoly2.github.io/Regression-and-lossfunction/&quot;&gt;[https://pandoly2.github.io/Regression-and-lossfunction/]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;배치-경사-하강법-batch-gradient-decent&quot;&gt;배치 경사 하강법 (Batch Gradient Decent)&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;경사 하강법의 손실 함수의 기울기 계산에 배치를 전체 학습 데이터셋의 크기와 동일하게 잡는 방법이다.&lt;/li&gt;
  &lt;li&gt;즉, 경사 하강법에 사용되는 전체 데이터의 크기가 배치 크기와 동일하다는 것이다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;장점&lt;/strong&gt; : 모든데이터가 대상이므로 안정적으로 수렴한다. &lt;br /&gt;
      수렴(최소값)까지 발생하는 가중치 및 바이어스 업데이트 수가 매우 적다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;단점&lt;/strong&gt; : 데이터셋 전체를 대상으로 하다 보니 가중치 및 바이어스가 변경될때마다, 계산해야 할 값이 많으므로, 계산 시간도 길어지고, 소모되는 메모리도 많다. &lt;br /&gt;
 지역 최소해(local Minimum)에 빠지면 빠저나오기가 힘들다. &lt;br /&gt;
 학습 데이서텟이 커지면 커질수록 시간과 리소스 소모가 크다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/DeepNeuralNetwork/gd_batch.png&quot; alt=&quot;Deep_batch_gradient&quot; /&gt;
출처 : 만년필잉크의 데이터 분석 지식 저장소&lt;/p&gt;

&lt;h4 id=&quot;확률적-경사-하강법-stochastic-gradient-descent-sdg&quot;&gt;확률적 경사 하강법 (Stochastic Gradient Descent, SDG)&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;학습 데이터셋에서 무작위로 한 개의 샘플 데이터 셋을 추출하고, 그 &lt;strong&gt;샘플에 대해서만 기울기를 계산&lt;/strong&gt; 하느것&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;장점&lt;/strong&gt; : 샘플 데이터 셋에 대해서만 기울기를 계산하므로 계산해야할 데이터 수가 적다. &lt;br /&gt;
 큰 데이터셋이라도 하나의 샘플씩 계산하기때문에 메모리 소모량이 매우 낮다. &lt;br /&gt;
 무작위 샘플 계산으로 불안정하지만 지역 최소값에서 빠져나올 가능성이 BGD보다 높다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;단점&lt;/strong&gt; : 무작위 추출로 데이터를 계산하기때문에 불안정하게 움직이며 수렴한다.  &lt;br /&gt;
 무작위 샘플링이기때문에 최적해에 도달하지 못할 수도 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/DeepNeuralNetwork/gd_sto.png&quot; alt=&quot;Deep_stochastic_gradient&quot; /&gt;
출처 : 만년필잉크의 데이터 분석 지식 저장소&lt;/p&gt;

&lt;h4 id=&quot;미니-배치-경사-하강법-mini-batch-gradient-descent&quot;&gt;미니 배치 경사 하강법 (mini-Batch Gradient Descent)&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;BGD의 배치크기를 줄이고 SGD를 활용하는 방법 (BGD, SGD 둘다 사용)&lt;/li&gt;
  &lt;li&gt;예) 1000개의 전체 학습데이터가 있다면, 100개씩 10개의 mini batch로 나누어 SGD를 진행한다.&lt;/li&gt;
  &lt;li&gt;일반적으로 확률적 경사 하강법(SGD)은 실제로 미니 배치 경사 하강법(mini-BGD)이다. 통상 SDG == mini-BDG 이다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;장점&lt;/strong&gt; : mini batch 크기에대해 SDG를 진행할때, 한 mini batch의 평균에대한 경사도로 하강을 진행하기 때문에 SDG(mini batch없는)보다 안정적으로 최적값에 수렴한다.  &lt;br /&gt;
 그로인해 안정적으로 최적값에 수렴할수있으나, 지역 최소값(local minimum) 현상이 발생할 수는 있다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;팁&lt;/strong&gt; : 배치 크기는 총 학습 데이터셋의 크기를 배치 크기로 나눴을 때, 딱 떨어지는 크기로 하는것이 좋다.  &lt;br /&gt;
 만약 1050개면 뒤에 50개는 버리도록 랜덤으로 데이터셋에서 버리도록 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/DeepNeuralNetwork/gd_mini.png&quot; alt=&quot;Deep_mini_gradient&quot; /&gt;&lt;/p&gt;

&lt;p&gt;참고 : &lt;a href=&quot;https://gooopy.tistory.com/70?category=824281&quot;&gt;https://gooopy.tistory.com/70?category=824281&lt;/a&gt;&lt;/p&gt;

&lt;!--4월 21일--&gt;
&lt;h4 id=&quot;sgd확률적-경사-하강법은-왜-지그재그로-움직이는가&quot;&gt;SGD(확률적 경사 하강법)은 왜 지그재그로 움직이는가&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;집합 : 
\(\left\{x\ \in \ {R}^n\ :\ f\left(x\right)\ =\ k\right\}\)
을 함수 f의 k-등위선이라고 부른다.&lt;/li&gt;
  &lt;li&gt;점 x를 지나는 &lt;strong&gt;등위선과 gradient Df(x)는 항상 수직&lt;/strong&gt;이다. &lt;br /&gt;
&lt;img src=&quot;/assets/images/blog_images/DeepNeuralNetwork/optimizer_gradient_vertical.png&quot; alt=&quot;optimizer_gradient_vertical&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;등위선에 수직으로 점을 움직여 나가면 지그재그로 움직이게 된다.
따라서, Gradient Descent는 진동 현상을 겪으며 매우 비효율적으로 움직인다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;참고 : &lt;a href=&quot;https://www.youtube.com/watch?v=5fwD1p9ymx8&quot;&gt;https://www.youtube.com/watch?v=5fwD1p9ymx8&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;모멘텀momentum&quot;&gt;모멘텀(Momentum)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;모멘텀은 ‘운동량’을 뜻하는 단어로, 물리와 관계가 있다.&lt;/li&gt;
  &lt;li&gt;물리계에서는 공이 굴러가는 방향은 중력뿐 아니라 기존의 관성에도 영향을 받는다. 
&lt;img src=&quot;/assets/images/blog_images/DeepNeuralNetwork/fig%206-4.png&quot; alt=&quot;dl_momentum_ball&quot; /&gt; &lt;br /&gt;
참고 : 밑바닥부터 시작하는 딥러닝&lt;/li&gt;
  &lt;li&gt;Momentum은 Gradient Descent에 현재의 관성을 추가한다.&lt;/li&gt;
  &lt;li&gt;Momentum의 수식 :  &lt;br /&gt;
\(V_n\ =\ \alpha V_{n-1}\ -\eta \ \nabla f\left(X_n\right),\ \left(v_{-1}\ =\ 0\right)\) &lt;br /&gt;
\(X_{n\ +\ 1}\ =\ X_n\ +\ V_n\)&lt;/li&gt;
  &lt;li&gt;V_n은 속도 V_n-1은 관성, α는 관성계수&lt;/li&gt;
  &lt;li&gt;관성계수 α가 클수록 속도가 관성에 더 많은 영향을 받는다.&lt;/li&gt;
  &lt;li&gt;델 \(\nabla\) 연산자 : 참고 &lt;a href=&quot;https://daewonjang.gitbooks.io/vector-calculus/content/chapter2.html&quot;&gt;https://daewonjang.gitbooks.io/vector-calculus/content/chapter2.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--4월 22일--&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/DeepNeuralNetwork/fig%206-5.png&quot; alt=&quot;dl_momentum_graph&quot; /&gt; &lt;br /&gt;
참고 : 밑바닥부터 시작하는 딥러닝&lt;/p&gt;

&lt;h4 id=&quot;sdg-vs-momentum-이동-곡선-비교&quot;&gt;SDG vs Momentum 이동 곡선 비교&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/DeepNeuralNetwork/dp_sdg_momentum_compare.png&quot; alt=&quot;dl_compare_sgd_momentum&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;nag-nesterov-accelated-gradient---모멘텀momentum의-변형&quot;&gt;NAG (Nesterov Accelated Gradient) - (모멘텀(Momentum)의 변형)&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Momentum은 현재위치에서 관성과 gradient의 반대방향을 합한다.&lt;/li&gt;
  &lt;li&gt;NAG는 Momentum을 &lt;strong&gt;공격적인 방식&lt;/strong&gt;으로 변형한다.&lt;/li&gt;
  &lt;li&gt;현재 위치에서의 관성과 관성방향으로 움직인 후의 위치에서의 gradient의 반대방향을 합한다.&lt;/li&gt;
  &lt;li&gt;(개인적으로 이해한것 정리) &lt;strong&gt;미리 이동할 곳을가서 그때의 경사에따라서 지금의 위치에서 내가 이동할 방향을 판단한다.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/DeepNeuralNetwork/dl_momentum_compare_nag.jpg&quot; alt=&quot;dl_momentum_nag_compare&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;점화식 :  &lt;br /&gt;
\(V_n\ =\ \alpha V_{n-1}\ -\ \eta \ \nabla f\left(x_n\ +\ \alpha V_{n-1}\right)\ ,\ \ v_{-1}\ =\ 0\) &lt;br /&gt;
\(X_{n+1}\ =\ X_n\ +\ V_n\)&lt;/li&gt;
  &lt;li&gt;즉, 현재위치 f(X_n)에서 관성만큼 α*V_n-1 이동후, 그 위치의 gradient값을 기준으로  gradient반대방향으로 이동하겠다는 이론.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;bengio의-근사적-접근&quot;&gt;Bengio의 근사적 접근&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;X_n이 아닌 다른 점(X의 다음 이동점)의 gradient를 구하기때문에 신경망에서 구현하기 적합하지않다. (현재 위치가 아니기때문에)&lt;/li&gt;
  &lt;li&gt;Xn 에서 momentum step후의 위치를 X’n이라 하자.&lt;/li&gt;
  &lt;li&gt;점화식 : &lt;br /&gt;
&lt;img src=&quot;/assets/images/blog_images/DeepNeuralNetwork/dl_bengio_fx.png&quot; alt=&quot;dl_bengio_fx&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;풀이 : &lt;br /&gt;
&lt;img src=&quot;/assets/images/blog_images/DeepNeuralNetwork/dl_bengio_fx_sol.png&quot; alt=&quot;dl_bengio_fx_sol&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;참고 : &lt;a href=&quot;https://www.youtube.com/watch?v=TQmCLe1BCJk&amp;amp;list=PLBiQZMT3oSxXNGcmAwI7vzh2LzwcwJpxU&amp;amp;index=2&quot;&gt;https://www.youtube.com/watch?v=TQmCLe1BCJk&amp;amp;list=PLBiQZMT3oSxXNGcmAwI7vzh2LzwcwJpxU&amp;amp;index=2&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;adagrad-adaptive-gradient&quot;&gt;AdaGrad (Adaptive Gradient)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;아이디어 : &lt;br /&gt;
&lt;img src=&quot;/assets/images/blog_images/DeepNeuralNetwork/dl_adagrad_picture_01.png&quot; alt=&quot;dl_adaptive_gradient_adia&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;일정한 learning rate를 사용하지 않고 변수마다 그리고, 스텝마다 learning rate가 바뀐다.&lt;/li&gt;
  &lt;li&gt;즉, &lt;strong&gt;목표(최소값)에 많이 다다른쪽은 적게 곱하고, 목표점에 가까워지면 가까워질수록 step의 크기를 줄이겠다는 아이디어.&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;시간이 지날수록 learning rate는 줄어드는데 큰 변화를 겪은 변수의 learning rate는 대폭 작아지고 작은 변화를 겪은 변수의 learning rate는 소폭으로 작아진다.&lt;/li&gt;
  &lt;li&gt;큰 변화를 겪은 변수는 이미 최적에 가까워졌고 작은 변화를 겪은 변수는 최적에 아직 멀다고 생각하기 때문&lt;/li&gt;
  &lt;li&gt;점화식 :  &lt;br /&gt;
\(h_n\ =\ h_{n-1}\ +\ \nabla f\left(x_n\right)\ \odot \ \nabla f\left(x_n\right),\ \ h_{-1}\ =\ 0\) &lt;br /&gt;
\(x_{n+1}\ =\ x_n\ -\ \eta \ \frac{1}{\sqrt{h_n}}\ \odot \ \nabla f\left(x_n\right)\)&lt;/li&gt;
  &lt;li&gt;벡터 h_n에는 gradient의 좌표별 제곱이 누적되어 있다. 즉,
\(h_n\ =\ \sum _{k=0}^n\nabla f\left(x_k\right)\ \odot \ \nabla \ f\left(x_k\right)\)&lt;/li&gt;
  &lt;li&gt;연산자 \(\odot\) :  &lt;br /&gt;
참고 : &lt;a href=&quot;https://en.wikipedia.org/wiki/Hadamard_product_(matrices)&quot;&gt;(https://en.wikipedia.org/wiki/Hadamard_product_(matrices)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/DeepNeuralNetwork/fig%206-6.png&quot; alt=&quot;dl_adaptive_gradient_graph&quot; /&gt;
출처 : 밑바닥부터 시작하는 딥러닝&lt;/p&gt;

&lt;h3 id=&quot;rmsprop&quot;&gt;RMSProp&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;AdaGrad는 과거의 기울기를 제곱하여 계속 더한다. 그래서 학습을 진행할수록 step이 줄어든다.&lt;/li&gt;
  &lt;li&gt;만약 무한대로 계속 학습한다면, step이 0에 수렴할것이고 그때부터는 갱신되지 않을것이다.&lt;/li&gt;
  &lt;li&gt;즉 오래 학습을 한다고 의미 있는 값이 나오는것이 아니다.&lt;/li&gt;
  &lt;li&gt;이문제를 개선한 기법이 &lt;strong&gt;RMSProp&lt;/strong&gt;이다.&lt;/li&gt;
  &lt;li&gt;이전 누적치와 현재 gradient의 좌표별 제곱의 가중치 평균을 생각한다.&lt;/li&gt;
  &lt;li&gt;점화식 :  &lt;br /&gt;
\(h_n\ =\ \gamma h_{n-1}\ +\ \left(1\ -\ \gamma \right)\nabla f\left(x_n\right)\ \odot \ \nabla f\left(x_n\right),\ \ h_{-1}\ =\ 0\) &lt;br /&gt;
\(x_{n+1}\ =\ x_n\ -\ \eta \ \frac{1}{\sqrt{h_n}}\odot \ \nabla f\left(x_n\right)\)&lt;/li&gt;
  &lt;li&gt;forgetting factor (decay rate) γ가 클수록 과거가 중요하고 작을수록 현재가 중요하다.&lt;/li&gt;
  &lt;li&gt;내분을 하면 H_n이 한없이 커지지않는다.&lt;strong&gt;(공부할것)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;adam&quot;&gt;Adam&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Momentum과 RMSProp 두가지 방식을 합친것&lt;/li&gt;
  &lt;li&gt;복잡하다. 하지만 현재 가장 많이 쓰이는 Optimizer다. (전혀 아담하지않다(?))&lt;/li&gt;
  &lt;li&gt;가중치 β_1으로 Momentum을 변형하여 점화식 : 
\(m_n\ =\ \beta _1m_{n-1}\ +\ \left(1-\beta _1\right)\nabla f\left(x_n\right),\ m_{-1}\ =\ 0\)&lt;/li&gt;
  &lt;li&gt;을 생각하고 가중치 β_2로 AdaGrad를 변형하여 점화식 : 
\(v_n\ =\ \beta _2v_{n-1}\ +\ \left(1-\beta _2\right)\nabla f\left(x_n\right)\odot \nabla f\left(x_n\right),\ \ v_{-1}\ =\ 0\)&lt;/li&gt;
  &lt;li&gt;을 생각한다.&lt;/li&gt;
  &lt;li&gt;Adam은 2015년에 제안된 새로운 방법이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;※ 코드 테스트 해볼것 chapter06 - test&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/DeepNeuralNetwork/fig%206-7.png&quot; alt=&quot;dl_adam_graph&quot; /&gt;
출처 : 밑바닥부터 시작하는 딥러닝&lt;/p&gt;

&lt;h3 id=&quot;최적화-기법-비교--sgd-momentum-adagrad-adam&quot;&gt;최적화 기법 비교 : SGD, Momentum, AdaGrad, Adam&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/DeepNeuralNetwork/fig%206-8.png&quot; alt=&quot;dl_compare_minimun&quot; /&gt;
출처 : 밑바닥부터 시작하는 딥러닝&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Feed Forward and Back Propagation</title>
   <link href="http://localhost:4000/back-propagation/"/>
   <updated>2022-04-20T00:00:00+00:00</updated>
   <id>http://localhost:4000/03-Feed-Forward-and-back-propagation copy</id>
   <content type="html">&lt;p&gt;TBD : Page&lt;/p&gt;

&lt;h2 id=&quot;feed-forward&quot;&gt;Feed Forward&lt;/h2&gt;

&lt;h2 id=&quot;back-propagation-오차역전법&quot;&gt;Back Propagation (오차역전법)&lt;/h2&gt;

</content>
 </entry>
 
 <entry>
   <title>신경망 (Neural Network)</title>
   <link href="http://localhost:4000/neural-network/"/>
   <updated>2022-04-20T00:00:00+00:00</updated>
   <id>http://localhost:4000/02-신경망(Neural Network) copy</id>
   <content type="html">&lt;h3 id=&quot;신경망의-구조&quot;&gt;신경망의 구조&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;퍼셉트론이 여러층 연결되어 구성되듯이 인간의 신경계도 여러개의 뉴런의 신호전달로 이루어진다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;인공신경망도 신경계와같이 여러개의 노드들간의 입력신호와 출력신호, 가중치, 바이어스에따라서 신호 전달이 이루어진다. &lt;br /&gt;
&lt;strong&gt;※ 인공 신경망과 퍼셉트론으 가장 큰 차이는, 퍼셉트론은 가중치를 결정할때 사람이 직접 관여를 해야하지만, 인공 신경망은 데이터를 통해 스스로 학습하여 가중치를 결정한다는 점이다.&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;다층 퍼셉트론 &lt;br /&gt;
&lt;img src=&quot;/assets/images/blog_images/Perceptron/perceptron_xor_detail_solution.png&quot; alt=&quot;perceptron_xor_detail_solution&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;신경망 &lt;br /&gt;
&lt;img src=&quot;/assets/images/blog_images/NeuralNetwork/nn_natural.png&quot; alt=&quot;nn_natural&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;인공 신경망의 구조 &lt;br /&gt;
&lt;img src=&quot;/assets/images/blog_images/NeuralNetwork/nn_artificial.png&quot; alt=&quot;nn_artificial&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;인공 신경망은 입력층, 은닉층, 출력층으로 구성된다.&lt;/li&gt;
  &lt;li&gt;각각의 원은 뉴런 또는 노드라는 용어로 사용된다&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;인공-신경망-구성&quot;&gt;인공 신경망 구성&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;퍼셉트론의 flow를 생각해보면, linear regression의 출력값을 classification 함수를 사용하여 분류를 했다.&lt;/li&gt;
  &lt;li&gt;신경망의 뉴런또한 어떠한 신호가 들어왔을때, 특정한 임계치를 넘지않으면 그다음 뉴런으로 신호가 발생 되지않는다.&lt;/li&gt;
  &lt;li&gt;이것과 마찬가지로 인공 신경망에서도 &lt;strong&gt;활성화 함수&lt;/strong&gt;라고 명칭하는 분류기 혹은 해당신호를 다음 노드의 입력으로 출력할 지를 결정하는 함수가 존재한다.&lt;/li&gt;
  &lt;li&gt;그림은 다음과 같다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/NeuralNetwork/nn_activation_struct.png&quot; alt=&quot;nn_activation_struct&quot; /&gt;
출처 : 밑바닥부터 시작하는 딥러닝&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;입력신호와 가중치 그리고 바이어스를 조합한 결과가 a노드가 되고 a노드는 h()라는 활성화 함수를 통과하여 y노드로 변환된다.&lt;/li&gt;
  &lt;li&gt;즉 활성화 함수를 통해 신호가 1/0으로 구분된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;활성화-함수의-종류&quot;&gt;활성화 함수의 종류&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Sigmoid&lt;/li&gt;
  &lt;li&gt;ReLU&lt;/li&gt;
  &lt;li&gt;Leaky ReLU&lt;/li&gt;
  &lt;li&gt;tanh&lt;/li&gt;
  &lt;li&gt;활성화 함수에대해서 좀더 자세히 알고싶다면 다음 블로그를 참고하면 된다.  &lt;a href=&quot;https://wooono.tistory.com/209&quot;&gt;활성화 함수 종류&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>퍼셉트론 (perceptron)</title>
   <link href="http://localhost:4000/perceptron/"/>
   <updated>2022-04-20T00:00:00+00:00</updated>
   <id>http://localhost:4000/01-퍼셉트론(perceptron)</id>
   <content type="html">&lt;h3 id=&quot;퍼셉트론-아이디어&quot;&gt;퍼셉트론 아이디어&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;지금까지 정리한 linear regression , logistic regression과 같은 개념으로 여러개의 입력데이터를 받아 하나의 출력을 갖는 구조를 퍼셉트론이라고 부름.&lt;/li&gt;
  &lt;li&gt;linear regression으로 최적의 구분선을 찾고 classification으로 데이터를 분류하는것이 개념적으로 동일해 보임.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;퍼셉트론-탄생&quot;&gt;퍼셉트론 탄생&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;퍼셉트론은 프랑크 로젠블라트가 1957년에 고안한 알고리즘입니다. (신경망(딥러닝)의 기원이 되는 알고리즘)&lt;/li&gt;
  &lt;li&gt;퍼셉트론의 구조를 배우는 것은 신경망과 딥러닝으로 나아가는데 중요한 아이디어를 배우는 일.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;퍼셉트론이란&quot;&gt;퍼셉트론이란?&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/Perceptron/perceptron_detail.png&quot; alt=&quot;perceptron_basic_0&quot; /&gt;
출처 : WikiDocs&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;퍼셉트론은 다수의 신호를 입력으로 받아 하나의 신호를 출력 (신호 : 전류나 강물처럼 흐름이 있는 것)&lt;/li&gt;
  &lt;li&gt;퍼셉트론의 신호는 ‘흐른다/안 흐른다(1이나 0)’의 두 가지 값을 가질수 있음.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/Perceptron/perceptron.png&quot; alt=&quot;perceptron_basic&quot; /&gt;
출처 : NeoWizard&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;x1과 x2는 입력신호, y는 출력 신호, w1과 w2는 가중치를 의미&lt;/li&gt;
  &lt;li&gt;원을 &lt;strong&gt;뉴런&lt;/strong&gt; 또는 &lt;strong&gt;노드&lt;/strong&gt;라고 부른다&lt;/li&gt;
  &lt;li&gt;입력 신호가 뉴런에 보내질 때는 각각 고유한 가중치가 곱해진다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;뉴런에서 전달 받은 신호의 총합이 임계값 θ를 넘을때만 1을 출력한다 (활성화 함수 or classification 함수)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;수식 : 
 \(y\ =\ \begin{cases}0\ \left({w}_1{x}_1\ +\ {w}_2{x}_2\ \le \ \theta \right)\\1\ \left({w}_1x_1\ +\ {w}_2x_2\ &amp;gt;\ \theta \right)\end{cases}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;퍼셉트론은 복수의 입력 신호 각각에 가중치를 부여한다. 가중치는 각 신호가 결과에 주는 영향력을 조절하는 요소로 작용하며, 가중치가 클수록 해당 신호가 그만큼 더 중요함을 뜻한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;퍼셉트론의-대표적인-예-단순논리회로---and-or-nad-xor-문제&quot;&gt;퍼셉트론의 대표적인 예 단순논리회로 - AND, OR, NAD, XOR 문제&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;우리는 다수의 입력에대하여 하나의 출력을 나타내는 단순한 논리회로를 퍼셉트론으로 나타낼수 있다. 
 (명칭은 퍼셉트론이라고 하지만 개념적으로는 적절한 선을 찾고(linear regression)그에 따른 분류를하는 것(classification))&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;and-gate&quot;&gt;AND Gate&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/Perceptron/perceptron_and.png&quot; alt=&quot;perceptron_and_gate&quot; /&gt;
출처 : NeoWizard&lt;/p&gt;

&lt;p&gt;입력 x1, x2에대해 아래의의 수식에 대입해보면 만족하는 w1, w2, θ값을 여러개 찾을 수 있다. &lt;br /&gt;
그중 하나의 예는 (0.5, 0.5, 0,7)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;수식 : 
 \(y\ =\ \begin{cases}0\ \left({w}_1{x}_1\ +\ {w}_2{x}_2\ \le \ \theta \right)\\1\ \left({w}_1x_1\ +\ {w}_2x_2\ &amp;gt;\ \theta \right)\end{cases}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이를 시각화하면 다음과 같은 그림이다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/Perceptron/perceptron_v_and.png&quot; alt=&quot;perceptron_v_and&quot; /&gt;
출처 : WikiDocs&lt;/p&gt;

&lt;h4 id=&quot;nand-gate&quot;&gt;NAND Gate&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/Perceptron/perceptron_nand.png&quot; alt=&quot;perceptron_nand_gate&quot; /&gt;
출처 : NeoWizard&lt;/p&gt;

&lt;p&gt;입력 x1, x2에대해 아래의의 수식에 대입해보면 만족하는 w1, w2, θ값을 여러개 찾을 수 있다. &lt;br /&gt;
그중 하나의 예는 (-0.5, -0.5, -0,7)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;시각화 &lt;br /&gt;
&lt;img src=&quot;/assets/images/blog_images/Perceptron/perceptron_v_nand.png&quot; alt=&quot;perceptron_v_nand&quot; /&gt;
출처 : WikiDocs&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;or-gate&quot;&gt;OR Gate&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/Perceptron/perceptron_or.png&quot; alt=&quot;perceptron_or_gate&quot; /&gt;
출처 : NeoWizard&lt;/p&gt;

&lt;p&gt;입력 x1, x2에대해 아래의의 수식에 대입해보면 만족하는 w1, w2, θ값을 여러개 찾을 수 있다. &lt;br /&gt;
그중 하나의 예는 (0.5, 0.5, 0,2)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;시각화 &lt;br /&gt;
&lt;img src=&quot;/assets/images/blog_images/Perceptron/perceptron_v_or.png&quot; alt=&quot;perceptron_v_or&quot; /&gt;
출처 : WikiDocs&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;가중치와-편향-수식&quot;&gt;가중치와 편향 수식&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;위의 식에서 θ를 b(bias)로 변경하여 식을 나타내면 다음과같다. 
 즉, regression 에서도 확인했듯이 최적의 가중치와 바이어스를 찾는 문제와 같다.
 찾은 가중치와 바이어스에 의한 결과값이 0보다 큰지 작은지에 대한 분류 문제인것이다.&lt;/li&gt;
&lt;/ul&gt;

\[y\ =\ \begin{cases}0\ \left({w}_1{x}_1\ +\ {w}_2{x}_2\ +\ b\le \ 0\right)\\1\ \left({w}_1x_1\ +\ {w}_2x_2\ \ +\ b&amp;gt;\ 0\right)\end{cases}\]

&lt;h3 id=&quot;단일-퍼셉트론의-문제---논리회로-xor&quot;&gt;단일 퍼셉트론의 문제 - 논리회로 XOR&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;다음은 XOR의 진리표이다.&lt;/li&gt;
  &lt;li&gt;배타적 논리합이라는 논리 회로로 x1, x2 한쪽이 1일때만 1을 출력한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/Perceptron/perceptron_xor.png&quot; alt=&quot;perceptron_xor&quot; /&gt;
출처 : NeoWizard&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;시각화 &lt;br /&gt;
&lt;img src=&quot;/assets/images/blog_images/Perceptron/perceptron_v_xor_0.png&quot; alt=&quot;perceptron_v_xor_0&quot; /&gt;
출처 : NeoWizard&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;XOR은 위의 그림과같이 하나의 직선으로 두영역을 나눌수없다.&lt;/li&gt;
  &lt;li&gt;만약 직선 즉, 선형이라는 제약이 없다면 비선형으로 다음과 같이 나눌수 있을것이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/Perceptron/perceptron_v_xor_1.png&quot; alt=&quot;perceptron_v_xor_1&quot; /&gt;
출처 : NeoWizard&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;하지만 비선형적 곡선을 나타내기어렵다.&lt;/li&gt;
  &lt;li&gt;우리는 여러개의 퍼셉트론을 엮어서 비선형처럼 만들수있다. (여러개의 직선)&lt;/li&gt;
  &lt;li&gt;이 아이디어가 다층 퍼셉트론으로 단일 퍼셉트론의 한계를 극복했다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/Perceptron/perceptron_v_xor_2.png&quot; alt=&quot;perceptron_v_xor_2&quot; /&gt;
출처 : Wikidocs&lt;/p&gt;

&lt;h4 id=&quot;xor-문제의-솔루션&quot;&gt;XOR 문제의 솔루션&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;단일 퍼셉트론 OR, NAND, AND를 결합하여 XOR을 만들었다.&lt;/li&gt;
  &lt;li&gt;입력 x1, x2를 각각(OR, NAND)의 gate에 입력으로 사용하고 그 출력을 다시 AND의 입력으로 사용한다.&lt;/li&gt;
  &lt;li&gt;이렇게 함으로써 단일 직선으로 해결하기 어려웠던 문제를 해결 할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/Perceptron/perceptron_xor_solution.png&quot; alt=&quot;perceptron_xor_solution&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;xor-논리게이트의-상세-diagram&quot;&gt;XOR 논리게이트의 상세 Diagram&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;XOR의 논리게이트로 표현된 위의 그림을 우리가 학습했던 linear regression과 classification으로 표현하면 다음과 같다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/Perceptron/perceptron_xor_detail_solution.png&quot; alt=&quot;perceptron_xor_detail_solution&quot; /&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Logistic Regression</title>
   <link href="http://localhost:4000/logistic-regression/"/>
   <updated>2022-04-19T00:00:00+00:00</updated>
   <id>http://localhost:4000/02-logistic-regression</id>
   <content type="html">&lt;h2 id=&quot;분류-classification&quot;&gt;분류 (classification)&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Training data 특성과 관계 등을 파악 한 후에, 미지의 입력 데이터에 대해서 결과가 어떤 종류의 값으로 분류 될 수 있는지를 예측하는것.
 예) 스팸문자 분류&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/classification/classification.png&quot; alt=&quot;classification_image&quot; width=&quot;100%&quot; /&gt;
출처 : NeoWizard&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;즉, Logistic Regression 알고리즘은,
    &lt;ol&gt;
      &lt;li&gt;Training data 특성과 분포를 나타내는 최적의 직선을 찾고(linear regression)&lt;/li&gt;
      &lt;li&gt;그 직선을 기준으로 데이터를 위(1) 또는 아래(0) 등으로 분류(Classification) 해주는 알고리즘&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;이러한 Logistic Regression은 Classification 알고리즘 중에서도 정확도가 높은 알고리즘으로 알려져 있어서 Deep learning에서 기본 component로 사용되고 있다고함.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/classification/classification_simple_flow.png&quot; alt=&quot;classification_simple&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;
출처 : NeoWizard&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;출력 값 y가 0 또는 1 만을 가져야하는 분류 시스템에서, 분류 함수로 sigmoid함수를 사용하여 0/1 값을 갖게 할 수 있음&lt;/li&gt;
  &lt;li&gt;즉, linear regression 출력 Wx + b 가 어떤 값을 갖더라도, 출력 함수로 sigmoid 를 사용해서
    &lt;ol&gt;
      &lt;li&gt;sigmoid 계산 값이 0.5보다 크면 결과로 1이 나올 확률이 높다는 것이기 때문에 출력 값 y는 1을 정의&lt;/li&gt;
      &lt;li&gt;sigmoid 계산 값이 0.5 미만이면 결과로 0 이 나올 확률이 높다는 것이므로 출력 값 y는 0을 정의&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;0 또는 1을 분류하는 classification system 구현 할 수 있음.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Sigmoid 식 : 
 \(y\ =\ sigmoid\left(z\right)\ =\ \phi \left(z\right)\ =\ \frac{1}{1\ +\ {e}^{-z}}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/classification/classification_sigmoid_graph.png&quot; alt=&quot;classification_sigmoid&quot; /&gt;
 출처 : NeoWizard&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;분류 시스템 최종 출력 값 y는 sigmoid 함수에 의해 논리적으로 1 또는 0 값을 가지기 때문에, 연속 값을 갖는 선형회귀 때와는 다른 손실함수 필요함.&lt;/li&gt;
  &lt;li&gt;손실함수 (cross-entropy) 식 :&lt;/li&gt;
&lt;/ul&gt;

\[y\ =\ \frac{1}{1\ +\ {e}^{-1\left(Wx\ +b\right)}}\ ,\ t_i\ =\ 0\ or\ 1\]

\[E\left(W,\ b\right)\ =\ -\ \sum _{i\ =\ 1}^n\left\{t_i\log {y_i\ }+\ \left(1-t_i\right)\log {\left(1\ -\ y_i\right)}\right\}\]

&lt;ul&gt;
  &lt;li&gt;classification 최종 출력 값 y 는 sigmoid 함수 에 의해 0 ~ 1 사이의 값을 갖는 확률적인 분류 모델이므로, 다음과 같이 확률변수 C 를 이용해 출력 값을 나타낼 수 있음.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;TBD: 이해하고 다시 작성&lt;/strong&gt;&lt;/p&gt;
&lt;font color=&quot;red&quot;&gt;★ NeoWizard ppt 62page 참조&lt;/font&gt;

&lt;h3 id=&quot;logistic-regression-flow&quot;&gt;logistic regression flow&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/classification/logistic_regression_flow.png&quot; alt=&quot;classification_flow&quot; /&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>경사하강법 (Gradient decent algorithm)</title>
   <link href="http://localhost:4000/gradient-decent/"/>
   <updated>2022-04-19T00:00:00+00:00</updated>
   <id>http://localhost:4000/01-경사하강법</id>
   <content type="html">&lt;h2 id=&quot;손실함수&quot;&gt;손실함수&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;손실함수는 오차의 평균값을 나타내기 때문에, 손실함수가 최소값을 갖는다는 것은 실제 정답과 계산 값의 차이인 오차가 최소가 되어, 미지의 데이터에 대해서 결과를 잘 예측 할 수 있다는 것을 의미함.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;손실함수는 W, b 에 영향을 받기 때문에, &lt;strong&gt;손실함수가 최소가 되는 가중치 W와 바이어스 b를 찾는 것&lt;/strong&gt;이 regression을 구현하는 최종 목표.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;손실함수-식&quot;&gt;손실함수 식&lt;/h4&gt;

\[loss\ function= E(W,\ b) = \frac{\left({t}_1\ -\ y_1\right)^2\ +\left({t}_2\ -\ y_2\right)^2\ +\ \left({t}_3\ -\ y_3\right)^2\ +\ ...\ \ +\ \left({t}_n\ -\ y_n\right)^2\ \ }{n}\]

\[=\frac{\left[{t}_1\ -\ \left(Wx_1\ +\ b\right)\right]^2\ +\ \left[{t}_2\ -\ \left(Wx_2\ +\ b\right)\right]^2\ +\ ...\ +\ \left[{t}_n\ -\ \left(Wx_n\ +\ b\right)\right]^2\ \ }{n}\]

\[=\frac{1}{n}\sum _{i=1}^n\left[t_i\ -\ \left(Wx_i\ +\ b\right)\right]^2\]

&lt;h3 id=&quot;손실함수-최적화-기술&quot;&gt;손실함수 최적화 기술&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;경사하강법&lt;/li&gt;
  &lt;li&gt;배치 경사하강법&lt;/li&gt;
  &lt;li&gt;확률적 경사하강법&lt;/li&gt;
  &lt;li&gt;미니 배치 경사하강법&lt;/li&gt;
  &lt;li&gt;모멘텀&lt;/li&gt;
  &lt;li&gt;아담&lt;/li&gt;
  &lt;li&gt;참고 : &lt;a href=&quot;https://truman.tistory.com/164&quot;&gt;https://truman.tistory.com/164&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;경사하강법-이해하기&quot;&gt;경사하강법 이해하기&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;가정&lt;/strong&gt; &lt;br /&gt;
 E(W, b)에서 바이어스 b = 0으로 가정 (계산을 쉽게하고 손실함수의 모양을 파악하기 위해)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;다음과 같은 training data set가 있을때, &lt;br /&gt;
&lt;img src=&quot;/assets/images/blog_images/Gradient/gradient_training_data.png&quot; alt=&quot;gradient_training_data&quot; /&gt;
출처 : NeoWizard&lt;/li&gt;
  &lt;li&gt;W 값에대한 손실함수 E(W, b) 계산 결과는 다음과 같다. &lt;br /&gt;
&lt;img src=&quot;/assets/images/blog_images/Gradient/gradient_loss_functiono_result.png&quot; alt=&quot;gradient_loss_function_result&quot; /&gt;
출처 : NeoWizard&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;loss function의 계산 결과를 그래프로 나타냈을때 다음과 같다. &lt;br /&gt;
&lt;img src=&quot;/assets/images/blog_images/Gradient/gradient_data_of_loss_fuction_result.png&quot; alt=&quot;gradient_data_of_loss_result&quot; /&gt;
출처 : NeoWizard
&lt;img src=&quot;/assets/images/blog_images/Gradient/gradient_graph.png&quot; alt=&quot;gradient_graph&quot; /&gt;
출처 : NeoWizard&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;그러면, 우리가 어떤 가중치를 임의로 설정했을때 최소값을 찾아가는 방법은 뭘까?&lt;/li&gt;
  &lt;li&gt;아래와같은 방법을 생각해 볼 수 있다. &lt;br /&gt;
 &lt;img src=&quot;/assets/images/blog_images/Gradient/gradient_decent.png&quot; alt=&quot;gradient_graph&quot; /&gt;
 출처 : NeoWizard&lt;/li&gt;
  &lt;li&gt;방법은 아래와 같이 기울기가 &lt;strong&gt;작아지는쪽&lt;/strong&gt;으로 이동 시키면된다. (가장낮은쪽으로 이동)   &lt;br /&gt;
 W 에서의 편미분 𝜕E(W)/𝜕W 가 해당 W 에서 기울기를 나타냄 &lt;br /&gt;
 𝜕E(W)/𝜕W 양수값을 갖는다면 W는 왼쪽으로 이동 &lt;br /&gt;
 𝜕E(W)/𝜕W 음수값을 갖는다면 W는 오른쪽으로 이동&lt;/li&gt;
  &lt;li&gt;식,&lt;br /&gt;
 W(weight) 가중치 : 
 \(w\ =\ w\ -\ \alpha \ \cdot \ \frac{\partial E\left(W,\ b\right)}{\partial w}\)
, b(bias) 바이어스 : 
\(b\ =\ b\ -\ \alpha \ \cdot \ \frac{\partial E\left(W,\ b\right)}{\partial b}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(※ \(\alpha\) 는 학습율(learning rate)라고 부르며, W 값의 감소 또는 증가 되는 비율을 나타냄) &lt;br /&gt;
 &lt;img src=&quot;/assets/images/blog_images/Gradient/gradient_decent2.png&quot; alt=&quot;gradient_graph&quot; /&gt;
 출처 : NeoWizard&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;이처럼, W에서의 직선의 기울기인 미분 값을 이용하여, 그 값이 작아지는 방향으로 진행하여 손실함수 최소값을 찾는 방법을 &lt;font color=&quot;red&quot;&gt; 경사하강법 (gradient decent algorithm) &lt;/font&gt; 이라고 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;전체-학습-flow-linear-regression&quot;&gt;전체 학습 Flow (linear regression)&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/Gradient/Gradient_decent_flow(linear_regression).png&quot; alt=&quot;gradient_flow&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>회귀와 손실함수 (Regression And loss-function)</title>
   <link href="http://localhost:4000/Regression-and-lossfunction/"/>
   <updated>2022-04-18T00:00:00+00:00</updated>
   <id>http://localhost:4000/04-회귀와 손실함수</id>
   <content type="html">&lt;ol&gt;
  &lt;li&gt;linear Regression&lt;/li&gt;
  &lt;li&gt;손실함수&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;회귀-regression&quot;&gt;회귀 (Regression)&lt;/h2&gt;
&lt;p&gt;※ 용어 :  회귀(Regression)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;원래 뜻은 예전으로 되돌아간다는 의미입니다. 영국의 유전학자 프랜시스 골턴이 부모와 자녀들 키의 연관 관계를 연구하다 보니, 개개인의 키는 결국 전체 키의 평균으로 수렴하는 경향이 있다는 걸 발견하고는 자신의 방법론에 ‘회귀 분석’이란 이름을 붙였다고 합니다.&lt;/li&gt;
  &lt;li&gt;‘회귀’라는 용어는 1885년 영국의 과학자 갈톤(F. Galton)이 발표한 ‘유전에 의하여 보통사람의 신장으로 회귀(Regression toward Meiocrity in Hereditary Stature’라는 논문에서 비롯되었다. 그는 아들의 키와 부모의 평균 키와의 관계를 분석하였는데, 부모의 키가 매우 클 때(또는 작을 때) 아들의 키는 일반적으로 평균키보다는 크지만(작지만) 그들의 부모만큼 크(작)지는 않다는 결론이다. 즉 부모의 키가 크(작)더라도 그 자식들은 결국 보통키로 회귀(돌아간다)한다는 뜻이다.
[출처] 회귀분석의 유래 : 대체 왜 Regression(회귀)이라고 불릴까?|작성자 바른인간&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;선형-회귀-linear-regression&quot;&gt;선형 회귀 (Linear Regression)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Trainig Data를 이용하여 데이터의 특성과 상관관계 등을 파악하고, 그 결과를 바탕으로 Training Data에 없는 미지의 데이터가 주어졌을 경우에, 그 결과를 연속적인 (숫자) 값으로 예측 하는것&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(예) 공부시간과 시험성적 관계, 집 평수와 집 가격 관계 등&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/Regression/linearRegression_01.PNG&quot; alt=&quot;linearRegression_01&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;
출처 : NeoWizard&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;학습 데이터는 입력(x)인 공부시간에 비례해서 출력(y)인 시험성적도 증가하는 경향이 있음
즉, 입력(x)과 출력(y)은 &lt;strong&gt;y = Wx + b&lt;/strong&gt; 형태로 나타낼 수 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/Regression/linearRegression_02.PNG&quot; alt=&quot;linearRegression_01&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;
출처 : NeoWizard&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;y = Wx + b&lt;/strong&gt;를 만족하는 다양한 &lt;strong&gt;1, 2, 3&lt;/strong&gt;과 같은 직선중, Training data의 특성을 가장 잘 표현할 수 있는 가중치 &lt;strong&gt;W(기울기)&lt;/strong&gt;, &lt;strong&gt;바이어스 b(y 절편 or 편차)&lt;/strong&gt;를 찾는 것이 학습(Learning) 개념임
※ 용어 정리 : 머신러닝에서는 W는 가중치(weight), b는 바이어스(bias) 라고 함.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;최적의-wweight와-bbias를-찾는방법&quot;&gt;최적의 W(weight)와 b(bias)를 찾는방법&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/Regression/linearRegression_03.PNG&quot; alt=&quot;linearRegression_01&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;
출처 : NeoWizard&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Training data의 정답(t)과 직선 y = Wx + b 값의 차이인 &lt;font color=&quot;red&quot;&gt; 오차(error) = t - y = t - (Wx + b)&lt;/font&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;오차가 크다면, 우리가 임의로 설정한 직선의 가중치와 바이어스 값이 잘못된 것이고, 오차가 작다면 직선의 가중치와 바이어스 값이 잘 된 것이기 때문에 미래 값 예측도 정확할 수 있다고 예상할 수 있음.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;즉, 머신러닝의 Regression 시스템은, &lt;strong&gt;모든 데이터&lt;/strong&gt;의 &lt;font color=&quot;red&quot;&gt; 오차(error) = t - y = t - (Wx + b)&lt;/font&gt;의 합이 최소가 되서, 미래 값을 잘 예측할 수 있는 가중치 W와 바이어스 b값을 찾아야 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;전체-학습-flow-linear-regression&quot;&gt;전체 학습 Flow (linear regression)&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/Gradient/Gradient_decent_flow(linear_regression).png&quot; alt=&quot;gradient_flow&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;손실함수-loss-function&quot;&gt;손실함수 (loss function)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;위의 &lt;strong&gt;최적의 W(weight)와 b(bias)를 찾는방법&lt;/strong&gt;에서 보았듯이 최적의 W(가중치)와 b(바이어스)를 찾는 것은 모든 데이터의 오차(error)가 작은 값을 찾으면된다.&lt;/li&gt;
  &lt;li&gt;즉, &lt;font color=&quot;red&quot;&gt;손실함수 (loss function)&lt;/font&gt;는, training data의 정답(t)과 입력(x)에 대한 계산 값 y의 차이(error)를 모두 더해 수식으로 나타낸 것.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/Regression/linearRegression_03.PNG&quot; alt=&quot;linearRegression_01&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;
출처 : NeoWizard&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;주의 : 각각의 오차를 모두 더해서 손실함수(loss function)을 구하면 각각의 오차가 (+), (-) 등이 동시에 존재하기 때문에 오차의 합이 0이 나올 수도 있음.&lt;/li&gt;
  &lt;li&gt;즉, 0 이라는 것이 최소 오차 값인지 아닌지를 판별하는 것이 어려움.&lt;/li&gt;
  &lt;li&gt;그래서, &lt;font color=&quot;red&quot;&gt;손실함수에서 오차(error)를 계산할 때는 양변에 제곱을 사용하여 계산함.&lt;/font&gt;&lt;/li&gt;
  &lt;li&gt;
\[{\left(t-y\right)}^2\ =\ {\left(t\ -\ {\left[Wx\ +\ b\right]}\right)}^2\]
  &lt;/li&gt;
  &lt;li&gt;즉, 오차는 언제나 양수이며, 제곱을 하기때문에 정답과 계산값 차이가 크다면, &lt;strong&gt;제곱에 의해 오차는 더 큰 값을 가지게 되어 머신러닝 학습에 있어 장점을 가짐&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;손실함수-식&quot;&gt;손실함수 식&lt;/h4&gt;

\[loss\ function= E(W,\ b) = \frac{\left({t}_1\ -\ y_1\right)^2\ +\left({t}_2\ -\ y_2\right)^2\ +\ \left({t}_3\ -\ y_3\right)^2\ +\ ...\ \ +\ \left({t}_n\ -\ y_n\right)^2\ \ }{n}\]

\[=\frac{\left[{t}_1\ -\ \left(Wx_1\ +\ b\right)\right]^2\ +\ \left[{t}_2\ -\ \left(Wx_2\ +\ b\right)\right]^2\ +\ ...\ +\ \left[{t}_n\ -\ \left(Wx_n\ +\ b\right)\right]^2\ \ }{n}\]

\[=\frac{1}{n}\sum _{i=1}^n\left[t_i\ -\ \left(Wx_i\ +\ b\right)\right]^2\]

&lt;ul&gt;
  &lt;li&gt;x 와 t 는 training data 에서 주어지는 값이므로, 손실함수(loss function)인 E(W, b)는 결국 W와 b에 영향을 받는 함수임.&lt;/li&gt;
  &lt;li&gt;E(W, b) 값이 작다는것은 정답(t, target)과 y = Wx + b에 의해 계산된 값의 평균 오차가 작다는 의미&lt;/li&gt;
  &lt;li&gt;평균 오차가 작다는 것은 임의의 데이터 x 가 주어질 경우, 확률적으로 미래의 결과값도 오차가 작을 것이라고 추측할 수 있음.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;즉, training data를 바탕으로 손실 함수 E(W, b)가 최소값을 갖도록 (W, b)를 구하는 것이 (linear) regression의 최종 목적임.&lt;/strong&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>머신러닝 구분</title>
   <link href="http://localhost:4000/Which-is-the-MachineLearning/"/>
   <updated>2022-04-18T00:00:00+00:00</updated>
   <id>http://localhost:4000/03-머신러닝구분</id>
   <content type="html">&lt;h2 id=&quot;구분&quot;&gt;구분&lt;/h2&gt;
&lt;p&gt;학습 방법에 따라 다음과 같이 나뉨&lt;/p&gt;

&lt;h3 id=&quot;지도학습-supervised&quot;&gt;지도학습 (Supervised)&lt;/h3&gt;
&lt;p&gt;: 입력 값(x)과 정답(t, label)을 포함하는 Training Data를 이용하여 학습하고, 그 학습된 결과를 바탕으로 미지의 데이터(Test Data)에 대해 미래 값을 예측(Predict)하는 방법 =&amp;gt; 대부분 머신러닝 문제는 지도학습에 해당됨.&lt;/p&gt;

&lt;p&gt;ex)&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;시험공부 시간(입력)과 Pass/Fail(정답)을 이용하여 당락 여부 예측&lt;/li&gt;
  &lt;li&gt;집 평수(입력)와 가격 데이터(정답) 이요하여 임의의 평수 가격 예측&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;*지도학습은 학습결과를 바탕으로, 미래의 무엇을 예측하느냐에 따라 회귀, 분류등으로 구분할 수 있음&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;회귀-regression&quot;&gt;회귀 (Regression)&lt;/h4&gt;
&lt;p&gt;: Training Data를 이용하여 &lt;strong&gt;연속적인 (숫자) 값을 예측&lt;/strong&gt;하는 것을 말하며, 집평수와 가격 관계, 공부시간과 시험성적 등의 관계임.&lt;/p&gt;
&lt;h4 id=&quot;분류-classification&quot;&gt;분류 (Classification)&lt;/h4&gt;
&lt;p&gt;: Training Data를 이용하여 주어진 입력값이 &lt;strong&gt;어떤 종류&lt;/strong&gt;의 값인지 구별하는 것을 지칭함&lt;/p&gt;

&lt;h3 id=&quot;비지도학습-unsupervised&quot;&gt;비지도학습 (Unsupervised)&lt;/h3&gt;
&lt;p&gt;: Training Data에 정답은 없고 입력 데이터만 있기 때문에, 입력에 대한 정답을 찾는 것이 아닌 입력데이터의 패턴, 특성 등을 학습을 통해 발견하는 방법을 말함. &lt;br /&gt;
ex)&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;군집화(Clustering) 알고리즘을 이용한 뉴스 그룹핑, 백화점의 상품 추천시스템 등&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;군집화-clustering&quot;&gt;군집화 (Clustering)&lt;/h4&gt;

</content>
 </entry>
 
 <entry>
   <title>수학개념 (Prerequisite for Machine Learning)</title>
   <link href="http://localhost:4000/Prerequisite-of-Math/"/>
   <updated>2022-04-18T00:00:00+00:00</updated>
   <id>http://localhost:4000/02-수학개념(PreRequisiteForMachineLearning)</id>
   <content type="html">&lt;h2 id=&quot;수치미분&quot;&gt;수치미분&lt;/h2&gt;

&lt;h3 id=&quot;미분으로-얻을-수-있는-insight&quot;&gt;미분으로 얻을 수 있는 Insight&lt;/h3&gt;
&lt;p&gt;-&amp;gt; 입력 변수 x가 미세하게 변할때, 함수 f(x)가 얼마나 변하는지 알 수 있는 식을 구해라. &lt;br /&gt;
 -&amp;gt; 한수 f(x)는 입력 x의 미세한 변화에 얼마나 민감하게 반응하는지 알 수 있는 식을 구해라.      &lt;br /&gt;
&lt;strong&gt;Insight&lt;/strong&gt; &lt;br /&gt;
 -&amp;gt; 입력 x 를 현재 값에서 아주 조금 변화시키면, 함수 f(x)는 얼마나 변하는가? &lt;br /&gt;
 -&amp;gt; 함수 f(x)는 입력 x의 미세한 변화에 얼마나 민감하게 반응하는가?&lt;/p&gt;

&lt;font color=&quot;red&quot;&gt; 어떤 값 x가 아주조금 변할때 y값의 미세한 변화량을 나타내는 기울기 혹은 크기 &lt;/font&gt;

&lt;h3 id=&quot;기본-미분-공식&quot;&gt;기본 미분 공식&lt;/h3&gt;
&lt;p&gt;\(f&apos;\left(x\right)\ =\ \frac{df\left(x\right)}{dx}\ =\lim _{\Delta x \to 0}{\frac{f\left(\left(x\ +\ \Delta x\right)\ -\ f\left(x\right)\right)}{\Delta x}}\)&lt;/p&gt;

&lt;h3 id=&quot;중앙차미분&quot;&gt;중앙차미분&lt;/h3&gt;
&lt;p&gt;\(f&apos;\left(x\right)\ =\ \frac{df\left(x\right)}{dx}\ =\lim _{\Delta x\to 0}^{ }\frac{f\left(\left(x\ +\ \Delta x\right)\ -\ f\left(x\ -\ \Delta x\right)\right)}{2\Delta x}\)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/NumericalDifferentiation/Numerical_Differentiation.jpg&quot; alt=&quot;Numerical_Differentiation&quot; title=&quot;Numerical Differentiation&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;참조 : &lt;a href=&quot;https://blog.naver.com/PostView.naver?blogId=mykepzzang&amp;amp;logNo=220072089756&amp;amp;parentCategoryNo=&amp;amp;categoryNo=16&amp;amp;viewDate=&amp;amp;isShowPopularPosts=false&amp;amp;from=postView&quot;&gt;https://blog.naver.com/PostView.naver?blogId=mykepzzang&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;편미분&quot;&gt;편미분&lt;/h3&gt;

&lt;p&gt;예 ) \(f\left(x,\ y\right)\ =\ 2x\ +\ 3xy\ +\ y3\) 에 대해,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;변수 x에 대하여 편미분 &lt;br /&gt;
\(\frac{\partial f\left(x,y\right)}{\partial x}\ =\ \frac{\partial \left(2x\ +\ 3xy\ +{y}^3\right)}{\partial x}\ =\ 2\ +\ 3y\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;변수 y에 대하여 편미분 &lt;br /&gt;
\(\frac{\partial f\left(x,y\right)}{\partial x}\ =\ \frac{\partial \left(2x\ +\ 3xy\ +{y}^3\right)}{\partial x}\ =\ 3x\ +\ 3y^2\)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;chain-rule&quot;&gt;Chain Rule&lt;/h3&gt;
&lt;p&gt;합성함수란 여러 함수로 구성된 함수로서, 이러한 합성함수를 미분하려면 ‘합성함수를 구성하는 각 함수의 미분의 곱’으로 나타내는 chain rule(연쇄 법칙) 이용
Ref. NeoWizard PPT 5 page (개인용)&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>4차 산업혁명의 키워드 인공지능</title>
   <link href="http://localhost:4000/what-is-the-machinlearning/"/>
   <updated>2022-04-18T00:00:00+00:00</updated>
   <id>http://localhost:4000/01-인공지능이란</id>
   <content type="html">&lt;h2 id=&quot;4차-산업-혁명&quot;&gt;4차 산업 혁명&lt;/h2&gt;
&lt;p&gt;2016년 스위스 다보스에서 개최된 세계경제 포럼에서 처음 언급됨&lt;/p&gt;

&lt;p&gt;학자에 따라 정의는 조금씩 다르지만, 대체로 &lt;strong&gt;4차 산업혁명&lt;/strong&gt;은 모든 것이 &lt;font color=&quot;red&quot;&gt;연결(Connectivity)&lt;/font&gt;되어 있는 환경에서 &lt;font color=&quot;red&quot;&gt;인공지능(Artificial Intelligence)&lt;/font&gt;에 의해 더운 편리하고 지능적인 사회로의 혁신적 변화를 지칭함.&lt;/p&gt;

&lt;h2 id=&quot;인공지능artificial-intelligence란&quot;&gt;인공지능(Artificial Intelligence)란&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images//WhatIstheAI/WhatIsthe_AI.drawio.png&quot; alt=&quot;WhatIsTheAI&quot; title=&quot;What is the AI&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;regression-classification&quot;&gt;Regression, Classification&lt;/h2&gt;
&lt;p&gt;Classification : 분류 &lt;br /&gt;
Regression : 회귀&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images//WhatIstheAI/Regression_Classification.png&quot; alt=&quot;WhatIsTheAI&quot; title=&quot;Regression &amp;amp; Classification&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;딥러닝&quot;&gt;딥러닝&lt;/h2&gt;
&lt;p&gt;뉴런 - 상호작용  (퍼셉트론 설명)&lt;/p&gt;

&lt;h2 id=&quot;학습을-위한-전제조건prerequisite&quot;&gt;학습을 위한 전제조건(Prerequisite)&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;프로그래밍 개념&lt;/li&gt;
  &lt;li&gt;기본 수학 개념&lt;/li&gt;
  &lt;li&gt;행렬(Matrix) 연산&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>First post (글)</title>
   <link href="http://localhost:4000/first-post-content/"/>
   <updated>2022-04-16T00:00:00+00:00</updated>
   <id>http://localhost:4000/first-post-content</id>
   <content type="html">&lt;p&gt;Github page로 만들어본 블로그&lt;/p&gt;

&lt;p&gt;오늘은 무슨 기능이있는지 알아보자.&lt;/p&gt;

&lt;p&gt;*기본적으로 markdown kramdown을 사용하여 그린다.&lt;/p&gt;

&lt;p&gt;Python&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Python syntax highlighting&quot;&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;C언어&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;&lt;span class=&quot;cp&quot;&gt;#include&lt;/span&gt; &lt;span class=&quot;cpf&quot;&gt;&amp;lt;stdio.h&amp;gt;&lt;/span&gt;&lt;span class=&quot;cp&quot;&gt;
&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(){&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;printf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;hello world&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;JAVA&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;&lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;HelloWorld&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nc&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Hello, World!&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h1 id=&quot;샾-강조-1&quot;&gt;샾 강조 1&lt;/h1&gt;
&lt;h2 id=&quot;샾샾-강조-2&quot;&gt;샾샾 강조 2&lt;/h2&gt;
&lt;h3 id=&quot;샾샾샾-강조-3&quot;&gt;샾샾샾 강조 3&lt;/h3&gt;
&lt;h4 id=&quot;샾샾샾샾-강조-4&quot;&gt;샾샾샾샾 강조 4&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://pandoly2.github.io&quot;&gt;링크&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;나는 &lt;strong&gt;Pandoly2&lt;/strong&gt; 입니다.
나는 &lt;em&gt;Pandoly2&lt;/em&gt; 입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog_images/panda.jpg&quot; alt=&quot;Panda Image&quot; title=&quot;Panda&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;항목 1&lt;/li&gt;
  &lt;li&gt;항목 2&lt;/li&gt;
  &lt;li&gt;항목 3&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;첫번째&lt;/li&gt;
  &lt;li&gt;두번째&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;참조 : &lt;a href=&quot;https://sukwonyun.github.io/jekyll/Jekyll-%ED%85%8C%EB%A7%88%EC%97%90%EC%84%9C-Latex-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0/&quot;&gt;마크다운 수식 넣는 법 링크&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;끝.&lt;/p&gt;
</content>
 </entry>
 

</feed>
