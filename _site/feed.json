{
    "version": "https://jsonfeed.org/version/1",
    "title": "Pandoly2 - 생각나는대로",
    "home_page_url": "http://localhost:4000/",
    "feed_url": "http://localhost:4000/feed.json",
    "description": "생각나는대로 적는 공간",
    "icon": "http://localhost:4000/assets/images/apple-touch-icon.png",
    "favicon": "http://localhost:4000/assets/images/favicon.png",
    "expired": false,
    
    "author":  {
        "name": "pandoly2",
        "url": null,
        "avatar": null
    },
    
"items": [
    
        {
            "id": "http://localhost:4000/optimization-parameter-5/",
            "title": "심층 신경망(딥러닝) 학습 관련 기술(optimization) - Hyperparameter",
            "summary": null,
            "content_text": "5. 적절한 하이퍼파라미터 값 찾기",
            "content_html": "<h2 id=\"5-적절한-하이퍼파라미터-값-찾기\">5. 적절한 하이퍼파라미터 값 찾기</h2>",
            "url": "http://localhost:4000/optimization-parameter-5/",
            
            
            
            "tags": ["pandoly2","machine learning","AI"],
            
            "date_published": "2022-05-03T00:00:00+00:00",
            "date_modified": "2022-05-03T00:00:00+00:00",
            
                "author": "pandoly2"
            
        },
    
        {
            "id": "http://localhost:4000/optimization-parameter-4/",
            "title": "심층 신경망(딥러닝) 학습 관련 기술(optimization) - 바른 학습을 위해",
            "summary": null,
            "content_text": "4. 바른 학습을 위해오버피팅  학습 데이터에 너무 최적화 되어 weight 값이 잡히고, 이후 학습 데이터가 아닌 새로운 데이터에는 올바른 값을 내보내지 못하는 현상. 매개변수에 비해 상대적으로 훈련 데이터 수가 적을 때 일어난다.Regularization  데이터에 너무 과적합되어 모델이 피팅되었으니, 이를 좀 덜 적합하게 하고, 이후 새로운 데이터에도 일반적으로 들어맞는 모델을 만들어야 한다. 이 때, 과적합이 아닌 일반성을 띄게 해주는 기법가중치 감소  weight decay 기하학적 설명    드롭아웃    학습시 마다 뉴런을 랜덤으로 비활성화 시킨다.  비율은 Hyperparameter로 초기 학습시 정한다.  놀랍게도 overfitting을 피하기위해서 많이 쓰는 기법이다.  두가지 해석이있다. (증명이아닌듯)          앙상블 기법      은행원      ",
            "content_html": "<h2 id=\"4-바른-학습을-위해\">4. 바른 학습을 위해</h2><h3 id=\"오버피팅\">오버피팅</h3><ul>  <li>학습 데이터에 너무 최적화 되어 weight 값이 잡히고, 이후 학습 데이터가 아닌 새로운 데이터에는 올바른 값을 내보내지 못하는 현상. 매개변수에 비해 상대적으로 훈련 데이터 수가 적을 때 일어난다.</li></ul><p><img src=\"/assets/images/blog_images/DeepNeuralNetwork4/overfitting.PNG\" alt=\"nn_overfitting_png\" /></p><h3 id=\"regularization\">Regularization</h3><ul>  <li>데이터에 너무 과적합되어 모델이 피팅되었으니, 이를 좀 덜 적합하게 하고, 이후 새로운 데이터에도 일반적으로 들어맞는 모델을 만들어야 한다. 이 때, 과적합이 아닌 일반성을 띄게 해주는 기법</li></ul><h4 id=\"가중치-감소\">가중치 감소</h4><p><img src=\"/assets/images/blog_images/DeepNeuralNetwork4/weight_decay.PNG\" alt=\"nn_weight_decay_png\" /></p><ul>  <li>weight decay 기하학적 설명<img src=\"/assets/images/blog_images/DeepNeuralNetwork4/weight_decay_desc.PNG\" alt=\"nn_weight_decay_desc\" />    <h4 id=\"드롭아웃\">드롭아웃</h4>  </li>  <li>학습시 마다 뉴런을 랜덤으로 비활성화 시킨다.</li>  <li>비율은 Hyperparameter로 초기 학습시 정한다.</li>  <li>놀랍게도 overfitting을 피하기위해서 많이 쓰는 기법이다.</li>  <li>두가지 해석이있다. (증명이아닌듯)    <ol>      <li>앙상블 기법</li>      <li>은행원</li>    </ol>  </li></ul><p><img src=\"/assets/images/blog_images/DeepNeuralNetwork4/dropout.PNG\" alt=\"nn_dropout_png\" /></p>",
            "url": "http://localhost:4000/optimization-parameter-4/",
            
            
            
            "tags": ["pandoly2","machine learning","AI"],
            
            "date_published": "2022-05-03T00:00:00+00:00",
            "date_modified": "2022-05-03T00:00:00+00:00",
            
                "author": "pandoly2"
            
        },
    
        {
            "id": "http://localhost:4000/optimization-parameter-3/",
            "title": "심층 신경망(딥러닝) 학습 관련 기술(optimization) - Batch Normalization",
            "summary": null,
            "content_text": "3. 배치 정규화  학습을 시작하기전에 가중치의 분포, 특히 표준편차를 잘 잡아서 은닉층에 나타나는 데이터의 분포가 중앙에 쏠리거나 양 사이드에 나타나지않게 하므로써 학습이 잘되게 하는게 목표였다.  affine층과 활성화 층 사이에 Batch Normalization층을 끼워 넣는다.  데이터를 분산시키기위해 정규분포의 특성으로 만든다. (평균 0, 표준편차 1)          입력값이 다음과 같이 주어질때,   \\(x\\ =\\ \\begin{pmatrix}x_1\\\\x_2\\\\x_3\\\\...\\\\x_n\\end{pmatrix}\\)      각 열에대한 평균, \\(\\mu \\ =\\ \\frac{1}{n}\\sum _{i=1}^nx_i\\)      각 열의 평균을 0으로 만듬 : \\(x_c\\ =\\ x\\ -\\ \\mu\\)      x의 각 열의 분산 \\(\\sigma ^2\\ =\\ \\frac{1}{N}\\sum _{i=1}^n\\left(x_i\\ -\\ \\mu \\right)^2\\)      각 열을 normalize함, \\(x_n\\ =\\ \\frac{x_c}{\\sigma }\\) , (평균을 뺀 각열을 표준편차로 나눔)      \\(x_n\\) 은 평균이 0, 표준 편차가 1인 분포를 따름.      여기에 데이터를 “잘”분산시키기위해 λ, β를 적용, \\(\\lambda \\ \\cdot \\ x_n\\ +\\ \\beta\\)      결과값이 최종 y 값. (batch normalization 된 값)      ",
            "content_html": "<h2 id=\"3-배치-정규화\">3. 배치 정규화</h2><ul>  <li>학습을 시작하기전에 가중치의 분포, 특히 표준편차를 잘 잡아서 은닉층에 나타나는 데이터의 분포가 중앙에 쏠리거나 양 사이드에 나타나지않게 하므로써 학습이 잘되게 하는게 목표였다.</li>  <li>affine층과 활성화 층 사이에 Batch Normalization층을 끼워 넣는다.</li></ul><p><img src=\"/assets/images/blog_images/DeepNeuralNetwork3/ML.drawio.png\" alt=\"opt3_batch_normal_desc\" /></p><ul>  <li>데이터를 분산시키기위해 정규분포의 특성으로 만든다. (평균 0, 표준편차 1)    <ol>      <li>입력값이 다음과 같이 주어질때,  <br /> \\(x\\ =\\ \\begin{pmatrix}x_1\\\\x_2\\\\x_3\\\\...\\\\x_n\\end{pmatrix}\\)</li>      <li>각 열에대한 평균, \\(\\mu \\ =\\ \\frac{1}{n}\\sum _{i=1}^nx_i\\)</li>      <li>각 열의 평균을 0으로 만듬 : \\(x_c\\ =\\ x\\ -\\ \\mu\\)</li>      <li>x의 각 열의 분산 \\(\\sigma ^2\\ =\\ \\frac{1}{N}\\sum _{i=1}^n\\left(x_i\\ -\\ \\mu \\right)^2\\)</li>      <li>각 열을 normalize함, \\(x_n\\ =\\ \\frac{x_c}{\\sigma }\\) , (평균을 뺀 각열을 표준편차로 나눔)</li>      <li>\\(x_n\\) 은 평균이 0, 표준 편차가 1인 분포를 따름.</li>      <li>여기에 데이터를 “잘”분산시키기위해 λ, β를 적용, \\(\\lambda \\ \\cdot \\ x_n\\ +\\ \\beta\\)</li>      <li>결과값이 최종 y 값. (batch normalization 된 값)</li>    </ol>  </li></ul>",
            "url": "http://localhost:4000/optimization-parameter-3/",
            
            
            
            "tags": ["pandoly2","machine learning","AI"],
            
            "date_published": "2022-05-03T00:00:00+00:00",
            "date_modified": "2022-05-03T00:00:00+00:00",
            
                "author": "pandoly2"
            
        },
    
        {
            "id": "http://localhost:4000/optimization-parameter-2/",
            "title": "심층 신경망(딥러닝) 학습 관련 기술(optimization) - 가중치 초깃값",
            "summary": null,
            "content_text": "2. 가중치의 초깃값  학습이 이루어지지않은 초기에 W(weight)와 b(bias)를 어떻게 설정하는게 좋을까.  초기값 설정의 예          가중치(Weight)를 평균이 0, 표준편차가 1인 정규분포로 초기화할때의 각층의 데이터 활성화값 분포  문제 : Vanishing Gradient Problem      가중치(Weight)를 평균이 0, 표준편차가 0.01인 정규분포로 초기화할때의 각층의 데이터 확성화값 분포  문제 : 표현력 제한      표현력 제한  첫번째 가중치 행렬의 각 행이 동일하고 두번째부터는 각 단계마다 가중치가 동일한 신경망을 가정해보면, 이 신경망은 layer 1부터 뉴런의 개수가 모두 1개인 신경망과 본질적으로 동일하다.  즉, 정보를 담을 수 있는 노드(or 뉴런)의 가중치가 제한된다.  표준편차를 너무작게 잡게되면 정보를 담을 수 있는 노드가 부족해지는것을 주의해야 한다.Vanishing gradient problem  sigmoid 미분값을 전파함.  데이터 셋이 y값 1/0부분에 군집되어있음.  sigmoid 미분값을 계속 곱하다보면 0에 수렴하는 값이됨.  층이 뒤로갈수록 gradient값이 작아서 학습이 이루어 지지않음.  인공신경망의 2번째 겨울에 하나의 주된 원인 (꽤 오랫동안 문제였다고함)LeCun 초기값 / Xavier 초기값  활성화 함수 Sigmoid일때,      평균이 0인 정규분포, 입력 뉴런과 출력 뉴런의 평균의 역수 : 분산     가중치 초가값으로 LeCun/Xavier 초기값을 이용했을때 각층의 데이터 활성화값 분포He 초기값  활성화 함수 ReLU 일때,    Lecun 초기값을 사용했을때,   He 초기값을 사용했을때,  3. 배치 정규화4. 바른 학습을 위해오버피팅가중치 감소드롭아웃5. 적절한 하이퍼파라미터 값 찾기",
            "content_html": "<h2 id=\"2-가중치의-초깃값\">2. 가중치의 초깃값</h2><ul>  <li>학습이 이루어지지않은 초기에 W(weight)와 b(bias)를 어떻게 설정하는게 좋을까.</li>  <li>초기값 설정의 예    <ol>      <li>가중치(Weight)를 평균이 0, 표준편차가 1인 정규분포로 초기화할때의 각층의 데이터 활성화값 분포<img src=\"/assets/images/blog_images/DeepNeuralNetwork2/fig%206-10.png\" alt=\"dl2_initial_param_ex\" /> <br /> 문제 : <strong>Vanishing Gradient Problem</strong></li>      <li>가중치(Weight)를 평균이 0, 표준편차가 0.01인 정규분포로 초기화할때의 각층의 데이터 확성화값 분포<img src=\"/assets/images/blog_images/DeepNeuralNetwork2/fig%206-11.png\" alt=\"dl2_initial_param_ex\" /> <br /> 문제 : <strong>표현력 제한</strong></li>    </ol>  </li></ul><h3 id=\"표현력-제한\">표현력 제한</h3><ul>  <li>첫번째 가중치 행렬의 각 행이 동일하고 두번째부터는 각 단계마다 가중치가 동일한 신경망을 가정해보면, 이 신경망은 layer 1부터 뉴런의 개수가 모두 1개인 신경망과 본질적으로 동일하다.</li>  <li>즉, 정보를 담을 수 있는 노드(or 뉴런)의 가중치가 제한된다.</li>  <li>표준편차를 너무작게 잡게되면 정보를 담을 수 있는 노드가 부족해지는것을 주의해야 한다.</li></ul><p><img src=\"/assets/images/blog_images/DeepNeuralNetwork2/%ED%91%9C%ED%98%84%EB%A0%A5%EC%A0%9C%ED%95%9C.png\" alt=\"dl2_initial_param_limit_expression\" /></p><h3 id=\"vanishing-gradient-problem\">Vanishing gradient problem</h3><ul>  <li>sigmoid 미분값을 전파함.</li>  <li>데이터 셋이 y값 1/0부분에 군집되어있음.</li>  <li>sigmoid 미분값을 계속 곱하다보면 0에 수렴하는 값이됨.</li>  <li>층이 뒤로갈수록 gradient값이 작아서 학습이 이루어 지지않음.</li>  <li>인공신경망의 2번째 겨울에 하나의 주된 원인 (꽤 오랫동안 문제였다고함)<img src=\"/assets/images/blog_images/DeepNeuralNetwork2/vanishing_gradient.PNG\" alt=\"dl2_vanish_gradient\" /></li></ul><h3 id=\"lecun-초기값--xavier-초기값\">LeCun 초기값 / Xavier 초기값</h3><ul>  <li>활성화 함수 Sigmoid일때,</li>  <li>    <p>평균이 0인 정규분포, 입력 뉴런과 출력 뉴런의 평균의 역수 : 분산 <br /><img src=\"/assets/images/blog_images/DeepNeuralNetwork2/Lecun_Xavier.PNG\" alt=\"dl_lecun_xavier\" /></p>  </li>  <li>가중치 초가값으로 LeCun/Xavier 초기값을 이용했을때 각층의 데이터 활성화값 분포<img src=\"/assets/images/blog_images/DeepNeuralNetwork2/Xavier_n.PNG\" alt=\"dl_xavier_n\" /></li></ul><h3 id=\"he-초기값\">He 초기값</h3><ul>  <li>활성화 함수 ReLU 일때,  <br /><img src=\"/assets/images/blog_images/DeepNeuralNetwork2/he_initial.PNG\" alt=\"dl_he_initial\" /></li>  <li>Lecun 초기값을 사용했을때, <br /><img src=\"/assets/images/blog_images/DeepNeuralNetwork2/relu_lecun.PNG\" alt=\"dl_ReLU_lecun\" /></li>  <li>He 초기값을 사용했을때,  <br /><img src=\"/assets/images/blog_images/DeepNeuralNetwork2/relu_he.PNG\" alt=\"dl_ReLU_He\" /></li></ul><h2 id=\"3-배치-정규화\">3. 배치 정규화</h2><h2 id=\"4-바른-학습을-위해\">4. 바른 학습을 위해</h2><h3 id=\"오버피팅\">오버피팅</h3><h3 id=\"가중치-감소\">가중치 감소</h3><h3 id=\"드롭아웃\">드롭아웃</h3><h2 id=\"5-적절한-하이퍼파라미터-값-찾기\">5. 적절한 하이퍼파라미터 값 찾기</h2>",
            "url": "http://localhost:4000/optimization-parameter-2/",
            
            
            
            "tags": ["pandoly2","machine learning","AI"],
            
            "date_published": "2022-04-24T00:00:00+00:00",
            "date_modified": "2022-04-24T00:00:00+00:00",
            
                "author": "pandoly2"
            
        },
    
        {
            "id": "http://localhost:4000/optimization-parameter/",
            "title": "심층 신경망(딥러닝) 학습 관련 기술(optimization) - 매개변수 갱신",
            "summary": null,
            "content_text": "  심층 신경망에서의 학습은 실제값과 우리모델이 예측한 예측값의 차이를 줄여주는것에 목적이 있다.  우리는 이미 linear regression이나 logistic regression에서 학습할때 손실함수(loss fuction)을 이용하여 예측값 y와 손실함수의 결과값의 차이가 작아 질 수 있도록 W(weight: 가중치)와 b(bias : 바이어스)를 업데이트하면서 성공률을 높이는 방법을 공부했다.  우리가 linear regression에서 사용한 손실함수(loss function)를 줄이는 기술은 경사 하강법 (Gradient decent)이다.  우리는 심층 신경망에서 보다 더 정확도가 높고, 안정적이며, 학습 속도가 빠른 학습방법 및 기술이 어떤것이 있는지 공부해볼 필요가 있다.1. 매개변수 갱신경사하강법경사하강법 (Gradient Decent)  평균제곱 오차 [https://pandoly2.github.io/Regression-and-lossfunction/]배치 경사 하강법 (Batch Gradient Decent)  경사 하강법의 손실 함수의 기울기 계산에 배치를 전체 학습 데이터셋의 크기와 동일하게 잡는 방법이다.  즉, 경사 하강법에 사용되는 전체 데이터의 크기가 배치 크기와 동일하다는 것이다.  장점 : 모든데이터가 대상이므로 안정적으로 수렴한다.       수렴(최소값)까지 발생하는 가중치 및 바이어스 업데이트 수가 매우 적다.  단점 : 데이터셋 전체를 대상으로 하다 보니 가중치 및 바이어스가 변경될때마다, 계산해야 할 값이 많으므로, 계산 시간도 길어지고, 소모되는 메모리도 많다.  지역 최소해(local Minimum)에 빠지면 빠저나오기가 힘들다.  학습 데이서텟이 커지면 커질수록 시간과 리소스 소모가 크다.출처 : 만년필잉크의 데이터 분석 지식 저장소확률적 경사 하강법 (Stochastic Gradient Descent, SDG)  학습 데이터셋에서 무작위로 한 개의 샘플 데이터 셋을 추출하고, 그 샘플에 대해서만 기울기를 계산 하느것      장점 : 샘플 데이터 셋에 대해서만 기울기를 계산하므로 계산해야할 데이터 수가 적다.  큰 데이터셋이라도 하나의 샘플씩 계산하기때문에 메모리 소모량이 매우 낮다.  무작위 샘플 계산으로 불안정하지만 지역 최소값에서 빠져나올 가능성이 BGD보다 높다.    단점 : 무작위 추출로 데이터를 계산하기때문에 불안정하게 움직이며 수렴한다.   무작위 샘플링이기때문에 최적해에 도달하지 못할 수도 있다.출처 : 만년필잉크의 데이터 분석 지식 저장소미니 배치 경사 하강법 (mini-Batch Gradient Descent)  BGD의 배치크기를 줄이고 SGD를 활용하는 방법 (BGD, SGD 둘다 사용)  예) 1000개의 전체 학습데이터가 있다면, 100개씩 10개의 mini batch로 나누어 SGD를 진행한다.  일반적으로 확률적 경사 하강법(SGD)은 실제로 미니 배치 경사 하강법(mini-BGD)이다. 통상 SDG == mini-BDG 이다.  장점 : mini batch 크기에대해 SDG를 진행할때, 한 mini batch의 평균에대한 경사도로 하강을 진행하기 때문에 SDG(mini batch없는)보다 안정적으로 최적값에 수렴한다.   그로인해 안정적으로 최적값에 수렴할수있으나, 지역 최소값(local minimum) 현상이 발생할 수는 있다.  팁 : 배치 크기는 총 학습 데이터셋의 크기를 배치 크기로 나눴을 때, 딱 떨어지는 크기로 하는것이 좋다.   만약 1050개면 뒤에 50개는 버리도록 랜덤으로 데이터셋에서 버리도록 한다.참고 : https://gooopy.tistory.com/70?category=824281SGD(확률적 경사 하강법)은 왜 지그재그로 움직이는가  집합 : \\(\\left\\{x\\ \\in \\ {R}^n\\ :\\ f\\left(x\\right)\\ =\\ k\\right\\}\\)을 함수 f의 k-등위선이라고 부른다.  점 x를 지나는 등위선과 gradient Df(x)는 항상 수직이다.   등위선에 수직으로 점을 움직여 나가면 지그재그로 움직이게 된다.따라서, Gradient Descent는 진동 현상을 겪으며 매우 비효율적으로 움직인다.참고 : https://www.youtube.com/watch?v=5fwD1p9ymx8모멘텀(Momentum)  모멘텀은 ‘운동량’을 뜻하는 단어로, 물리와 관계가 있다.  물리계에서는 공이 굴러가는 방향은 중력뿐 아니라 기존의 관성에도 영향을 받는다.  참고 : 밑바닥부터 시작하는 딥러닝  Momentum은 Gradient Descent에 현재의 관성을 추가한다.  Momentum의 수식 :  \\(V_n\\ =\\ \\alpha V_{n-1}\\ -\\eta \\ \\nabla f\\left(X_n\\right),\\ \\left(v_{-1}\\ =\\ 0\\right)\\) \\(X_{n\\ +\\ 1}\\ =\\ X_n\\ +\\ V_n\\)  V_n은 속도 V_n-1은 관성, α는 관성계수  관성계수 α가 클수록 속도가 관성에 더 많은 영향을 받는다.  델 \\(\\nabla\\) 연산자 : 참고 https://daewonjang.gitbooks.io/vector-calculus/content/chapter2.html 참고 : 밑바닥부터 시작하는 딥러닝SDG vs Momentum 이동 곡선 비교NAG (Nesterov Accelated Gradient) - (모멘텀(Momentum)의 변형)  Momentum은 현재위치에서 관성과 gradient의 반대방향을 합한다.  NAG는 Momentum을 공격적인 방식으로 변형한다.  현재 위치에서의 관성과 관성방향으로 움직인 후의 위치에서의 gradient의 반대방향을 합한다.  (개인적으로 이해한것 정리) 미리 이동할 곳을가서 그때의 경사에따라서 지금의 위치에서 내가 이동할 방향을 판단한다.  점화식 :  \\(V_n\\ =\\ \\alpha V_{n-1}\\ -\\ \\eta \\ \\nabla f\\left(x_n\\ +\\ \\alpha V_{n-1}\\right)\\ ,\\ \\ v_{-1}\\ =\\ 0\\) \\(X_{n+1}\\ =\\ X_n\\ +\\ V_n\\)  즉, 현재위치 f(X_n)에서 관성만큼 α*V_n-1 이동후, 그 위치의 gradient값을 기준으로  gradient반대방향으로 이동하겠다는 이론.Bengio의 근사적 접근  X_n이 아닌 다른 점(X의 다음 이동점)의 gradient를 구하기때문에 신경망에서 구현하기 적합하지않다. (현재 위치가 아니기때문에)  Xn 에서 momentum step후의 위치를 X’n이라 하자.  점화식 :   풀이 : 참고 : https://www.youtube.com/watch?v=TQmCLe1BCJk&amp;list=PLBiQZMT3oSxXNGcmAwI7vzh2LzwcwJpxU&amp;index=2AdaGrad (Adaptive Gradient)  아이디어 :   일정한 learning rate를 사용하지 않고 변수마다 그리고, 스텝마다 learning rate가 바뀐다.  즉, 목표(최소값)에 많이 다다른쪽은 적게 곱하고, 목표점에 가까워지면 가까워질수록 step의 크기를 줄이겠다는 아이디어.  시간이 지날수록 learning rate는 줄어드는데 큰 변화를 겪은 변수의 learning rate는 대폭 작아지고 작은 변화를 겪은 변수의 learning rate는 소폭으로 작아진다.  큰 변화를 겪은 변수는 이미 최적에 가까워졌고 작은 변화를 겪은 변수는 최적에 아직 멀다고 생각하기 때문  점화식 :  \\(h_n\\ =\\ h_{n-1}\\ +\\ \\nabla f\\left(x_n\\right)\\ \\odot \\ \\nabla f\\left(x_n\\right),\\ \\ h_{-1}\\ =\\ 0\\) \\(x_{n+1}\\ =\\ x_n\\ -\\ \\eta \\ \\frac{1}{\\sqrt{h_n}}\\ \\odot \\ \\nabla f\\left(x_n\\right)\\)  벡터 h_n에는 gradient의 좌표별 제곱이 누적되어 있다. 즉,\\(h_n\\ =\\ \\sum _{k=0}^n\\nabla f\\left(x_k\\right)\\ \\odot \\ \\nabla \\ f\\left(x_k\\right)\\)  연산자 \\(\\odot\\) :  참고 : (https://en.wikipedia.org/wiki/Hadamard_product_(matrices)출처 : 밑바닥부터 시작하는 딥러닝RMSProp  AdaGrad는 과거의 기울기를 제곱하여 계속 더한다. 그래서 학습을 진행할수록 step이 줄어든다.  만약 무한대로 계속 학습한다면, step이 0에 수렴할것이고 그때부터는 갱신되지 않을것이다.  즉 오래 학습을 한다고 의미 있는 값이 나오는것이 아니다.  이문제를 개선한 기법이 RMSProp이다.  이전 누적치와 현재 gradient의 좌표별 제곱의 가중치 평균을 생각한다.  점화식 :  \\(h_n\\ =\\ \\gamma h_{n-1}\\ +\\ \\left(1\\ -\\ \\gamma \\right)\\nabla f\\left(x_n\\right)\\ \\odot \\ \\nabla f\\left(x_n\\right),\\ \\ h_{-1}\\ =\\ 0\\) \\(x_{n+1}\\ =\\ x_n\\ -\\ \\eta \\ \\frac{1}{\\sqrt{h_n}}\\odot \\ \\nabla f\\left(x_n\\right)\\)  forgetting factor (decay rate) γ가 클수록 과거가 중요하고 작을수록 현재가 중요하다.  내분을 하면 H_n이 한없이 커지지않는다.(공부할것)Adam  Momentum과 RMSProp 두가지 방식을 합친것  복잡하다. 하지만 현재 가장 많이 쓰이는 Optimizer다. (전혀 아담하지않다(?))  가중치 β_1으로 Momentum을 변형하여 점화식 : \\(m_n\\ =\\ \\beta _1m_{n-1}\\ +\\ \\left(1-\\beta _1\\right)\\nabla f\\left(x_n\\right),\\ m_{-1}\\ =\\ 0\\)  을 생각하고 가중치 β_2로 AdaGrad를 변형하여 점화식 : \\(v_n\\ =\\ \\beta _2v_{n-1}\\ +\\ \\left(1-\\beta _2\\right)\\nabla f\\left(x_n\\right)\\odot \\nabla f\\left(x_n\\right),\\ \\ v_{-1}\\ =\\ 0\\)  을 생각한다.  Adam은 2015년에 제안된 새로운 방법이다.※ 코드 테스트 해볼것 chapter06 - test출처 : 밑바닥부터 시작하는 딥러닝최적화 기법 비교 : SGD, Momentum, AdaGrad, Adam출처 : 밑바닥부터 시작하는 딥러닝",
            "content_html": "<ul>  <li>심층 신경망에서의 학습은 실제값과 우리모델이 예측한 예측값의 차이를 줄여주는것에 목적이 있다.</li>  <li>우리는 이미 linear regression이나 logistic regression에서 학습할때 손실함수(loss fuction)을 이용하여 예측값 y와 손실함수의 결과값의 차이가 작아 질 수 있도록 W(weight: 가중치)와 b(bias : 바이어스)를 업데이트하면서 성공률을 높이는 방법을 공부했다.</li>  <li>우리가 linear regression에서 사용한 손실함수(loss function)를 줄이는 기술은 <strong>경사 하강법 (Gradient decent)</strong>이다.</li>  <li>우리는 심층 신경망에서 보다 더 정확도가 높고, 안정적이며, 학습 속도가 빠른 학습방법 및 기술이 어떤것이 있는지 공부해볼 필요가 있다.</li></ul><h2 id=\"1-매개변수-갱신\">1. 매개변수 갱신</h2><h3 id=\"경사하강법\">경사하강법</h3><h4 id=\"경사하강법-gradient-decent\">경사하강법 (Gradient Decent)</h4><ul>  <li>평균제곱 오차 <a href=\"https://pandoly2.github.io/Regression-and-lossfunction/\">[https://pandoly2.github.io/Regression-and-lossfunction/]</a></li></ul><h4 id=\"배치-경사-하강법-batch-gradient-decent\">배치 경사 하강법 (Batch Gradient Decent)</h4><ul>  <li>경사 하강법의 손실 함수의 기울기 계산에 배치를 전체 학습 데이터셋의 크기와 동일하게 잡는 방법이다.</li>  <li>즉, 경사 하강법에 사용되는 전체 데이터의 크기가 배치 크기와 동일하다는 것이다.</li>  <li><strong>장점</strong> : 모든데이터가 대상이므로 안정적으로 수렴한다. <br />      수렴(최소값)까지 발생하는 가중치 및 바이어스 업데이트 수가 매우 적다.</li>  <li><strong>단점</strong> : 데이터셋 전체를 대상으로 하다 보니 가중치 및 바이어스가 변경될때마다, 계산해야 할 값이 많으므로, 계산 시간도 길어지고, 소모되는 메모리도 많다. <br /> 지역 최소해(local Minimum)에 빠지면 빠저나오기가 힘들다. <br /> 학습 데이서텟이 커지면 커질수록 시간과 리소스 소모가 크다.</li></ul><p><img src=\"/assets/images/blog_images/DeepNeuralNetwork/gd_batch.png\" alt=\"Deep_batch_gradient\" />출처 : 만년필잉크의 데이터 분석 지식 저장소</p><h4 id=\"확률적-경사-하강법-stochastic-gradient-descent-sdg\">확률적 경사 하강법 (Stochastic Gradient Descent, SDG)</h4><ul>  <li>학습 데이터셋에서 무작위로 한 개의 샘플 데이터 셋을 추출하고, 그 <strong>샘플에 대해서만 기울기를 계산</strong> 하느것</li>  <li>    <p><strong>장점</strong> : 샘플 데이터 셋에 대해서만 기울기를 계산하므로 계산해야할 데이터 수가 적다. <br /> 큰 데이터셋이라도 하나의 샘플씩 계산하기때문에 메모리 소모량이 매우 낮다. <br /> 무작위 샘플 계산으로 불안정하지만 지역 최소값에서 빠져나올 가능성이 BGD보다 높다.</p>  </li>  <li><strong>단점</strong> : 무작위 추출로 데이터를 계산하기때문에 불안정하게 움직이며 수렴한다.  <br /> 무작위 샘플링이기때문에 최적해에 도달하지 못할 수도 있다.</li></ul><p><img src=\"/assets/images/blog_images/DeepNeuralNetwork/gd_sto.png\" alt=\"Deep_stochastic_gradient\" />출처 : 만년필잉크의 데이터 분석 지식 저장소</p><h4 id=\"미니-배치-경사-하강법-mini-batch-gradient-descent\">미니 배치 경사 하강법 (mini-Batch Gradient Descent)</h4><ul>  <li>BGD의 배치크기를 줄이고 SGD를 활용하는 방법 (BGD, SGD 둘다 사용)</li>  <li>예) 1000개의 전체 학습데이터가 있다면, 100개씩 10개의 mini batch로 나누어 SGD를 진행한다.</li>  <li>일반적으로 확률적 경사 하강법(SGD)은 실제로 미니 배치 경사 하강법(mini-BGD)이다. 통상 SDG == mini-BDG 이다.</li>  <li><strong>장점</strong> : mini batch 크기에대해 SDG를 진행할때, 한 mini batch의 평균에대한 경사도로 하강을 진행하기 때문에 SDG(mini batch없는)보다 안정적으로 최적값에 수렴한다.  <br /> 그로인해 안정적으로 최적값에 수렴할수있으나, 지역 최소값(local minimum) 현상이 발생할 수는 있다.</li>  <li><strong>팁</strong> : 배치 크기는 총 학습 데이터셋의 크기를 배치 크기로 나눴을 때, 딱 떨어지는 크기로 하는것이 좋다.  <br /> 만약 1050개면 뒤에 50개는 버리도록 랜덤으로 데이터셋에서 버리도록 한다.</li></ul><p><img src=\"/assets/images/blog_images/DeepNeuralNetwork/gd_mini.png\" alt=\"Deep_mini_gradient\" /></p><p>참고 : <a href=\"https://gooopy.tistory.com/70?category=824281\">https://gooopy.tistory.com/70?category=824281</a></p><!--4월 21일--><h4 id=\"sgd확률적-경사-하강법은-왜-지그재그로-움직이는가\">SGD(확률적 경사 하강법)은 왜 지그재그로 움직이는가</h4><ul>  <li>집합 : \\(\\left\\{x\\ \\in \\ {R}^n\\ :\\ f\\left(x\\right)\\ =\\ k\\right\\}\\)을 함수 f의 k-등위선이라고 부른다.</li>  <li>점 x를 지나는 <strong>등위선과 gradient Df(x)는 항상 수직</strong>이다. <br /><img src=\"/assets/images/blog_images/DeepNeuralNetwork/optimizer_gradient_vertical.png\" alt=\"optimizer_gradient_vertical\" /></li>  <li>등위선에 수직으로 점을 움직여 나가면 지그재그로 움직이게 된다.따라서, Gradient Descent는 진동 현상을 겪으며 매우 비효율적으로 움직인다.</li></ul><p>참고 : <a href=\"https://www.youtube.com/watch?v=5fwD1p9ymx8\">https://www.youtube.com/watch?v=5fwD1p9ymx8</a></p><h3 id=\"모멘텀momentum\">모멘텀(Momentum)</h3><ul>  <li>모멘텀은 ‘운동량’을 뜻하는 단어로, 물리와 관계가 있다.</li>  <li>물리계에서는 공이 굴러가는 방향은 중력뿐 아니라 기존의 관성에도 영향을 받는다. <img src=\"/assets/images/blog_images/DeepNeuralNetwork/fig%206-4.png\" alt=\"dl_momentum_ball\" /> <br />참고 : 밑바닥부터 시작하는 딥러닝</li>  <li>Momentum은 Gradient Descent에 현재의 관성을 추가한다.</li>  <li>Momentum의 수식 :  <br />\\(V_n\\ =\\ \\alpha V_{n-1}\\ -\\eta \\ \\nabla f\\left(X_n\\right),\\ \\left(v_{-1}\\ =\\ 0\\right)\\) <br />\\(X_{n\\ +\\ 1}\\ =\\ X_n\\ +\\ V_n\\)</li>  <li>V_n은 속도 V_n-1은 관성, α는 관성계수</li>  <li>관성계수 α가 클수록 속도가 관성에 더 많은 영향을 받는다.</li>  <li>델 \\(\\nabla\\) 연산자 : 참고 <a href=\"https://daewonjang.gitbooks.io/vector-calculus/content/chapter2.html\">https://daewonjang.gitbooks.io/vector-calculus/content/chapter2.html</a></li></ul><!--4월 22일--><p><img src=\"/assets/images/blog_images/DeepNeuralNetwork/fig%206-5.png\" alt=\"dl_momentum_graph\" /> <br />참고 : 밑바닥부터 시작하는 딥러닝</p><h4 id=\"sdg-vs-momentum-이동-곡선-비교\">SDG vs Momentum 이동 곡선 비교</h4><p><img src=\"/assets/images/blog_images/DeepNeuralNetwork/dp_sdg_momentum_compare.png\" alt=\"dl_compare_sgd_momentum\" /></p><h4 id=\"nag-nesterov-accelated-gradient---모멘텀momentum의-변형\">NAG (Nesterov Accelated Gradient) - (모멘텀(Momentum)의 변형)</h4><ul>  <li>Momentum은 현재위치에서 관성과 gradient의 반대방향을 합한다.</li>  <li>NAG는 Momentum을 <strong>공격적인 방식</strong>으로 변형한다.</li>  <li>현재 위치에서의 관성과 관성방향으로 움직인 후의 위치에서의 gradient의 반대방향을 합한다.</li>  <li>(개인적으로 이해한것 정리) <strong>미리 이동할 곳을가서 그때의 경사에따라서 지금의 위치에서 내가 이동할 방향을 판단한다.</strong></li></ul><p><img src=\"/assets/images/blog_images/DeepNeuralNetwork/dl_momentum_compare_nag.jpg\" alt=\"dl_momentum_nag_compare\" /></p><ul>  <li>점화식 :  <br />\\(V_n\\ =\\ \\alpha V_{n-1}\\ -\\ \\eta \\ \\nabla f\\left(x_n\\ +\\ \\alpha V_{n-1}\\right)\\ ,\\ \\ v_{-1}\\ =\\ 0\\) <br />\\(X_{n+1}\\ =\\ X_n\\ +\\ V_n\\)</li>  <li>즉, 현재위치 f(X_n)에서 관성만큼 α*V_n-1 이동후, 그 위치의 gradient값을 기준으로  gradient반대방향으로 이동하겠다는 이론.</li></ul><h4 id=\"bengio의-근사적-접근\">Bengio의 근사적 접근</h4><ul>  <li>X_n이 아닌 다른 점(X의 다음 이동점)의 gradient를 구하기때문에 신경망에서 구현하기 적합하지않다. (현재 위치가 아니기때문에)</li>  <li>Xn 에서 momentum step후의 위치를 X’n이라 하자.</li>  <li>점화식 : <br /><img src=\"/assets/images/blog_images/DeepNeuralNetwork/dl_bengio_fx.png\" alt=\"dl_bengio_fx\" /></li>  <li>풀이 : <br /><img src=\"/assets/images/blog_images/DeepNeuralNetwork/dl_bengio_fx_sol.png\" alt=\"dl_bengio_fx_sol\" /></li></ul><p>참고 : <a href=\"https://www.youtube.com/watch?v=TQmCLe1BCJk&amp;list=PLBiQZMT3oSxXNGcmAwI7vzh2LzwcwJpxU&amp;index=2\">https://www.youtube.com/watch?v=TQmCLe1BCJk&amp;list=PLBiQZMT3oSxXNGcmAwI7vzh2LzwcwJpxU&amp;index=2</a></p><h3 id=\"adagrad-adaptive-gradient\">AdaGrad (Adaptive Gradient)</h3><ul>  <li>아이디어 : <br /><img src=\"/assets/images/blog_images/DeepNeuralNetwork/dl_adagrad_picture_01.png\" alt=\"dl_adaptive_gradient_adia\" /></li>  <li>일정한 learning rate를 사용하지 않고 변수마다 그리고, 스텝마다 learning rate가 바뀐다.</li>  <li>즉, <strong>목표(최소값)에 많이 다다른쪽은 적게 곱하고, 목표점에 가까워지면 가까워질수록 step의 크기를 줄이겠다는 아이디어.</strong></li>  <li>시간이 지날수록 learning rate는 줄어드는데 큰 변화를 겪은 변수의 learning rate는 대폭 작아지고 작은 변화를 겪은 변수의 learning rate는 소폭으로 작아진다.</li>  <li>큰 변화를 겪은 변수는 이미 최적에 가까워졌고 작은 변화를 겪은 변수는 최적에 아직 멀다고 생각하기 때문</li>  <li>점화식 :  <br />\\(h_n\\ =\\ h_{n-1}\\ +\\ \\nabla f\\left(x_n\\right)\\ \\odot \\ \\nabla f\\left(x_n\\right),\\ \\ h_{-1}\\ =\\ 0\\) <br />\\(x_{n+1}\\ =\\ x_n\\ -\\ \\eta \\ \\frac{1}{\\sqrt{h_n}}\\ \\odot \\ \\nabla f\\left(x_n\\right)\\)</li>  <li>벡터 h_n에는 gradient의 좌표별 제곱이 누적되어 있다. 즉,\\(h_n\\ =\\ \\sum _{k=0}^n\\nabla f\\left(x_k\\right)\\ \\odot \\ \\nabla \\ f\\left(x_k\\right)\\)</li>  <li>연산자 \\(\\odot\\) :  <br />참고 : <a href=\"https://en.wikipedia.org/wiki/Hadamard_product_(matrices)\">(https://en.wikipedia.org/wiki/Hadamard_product_(matrices)</a></li></ul><p><img src=\"/assets/images/blog_images/DeepNeuralNetwork/fig%206-6.png\" alt=\"dl_adaptive_gradient_graph\" />출처 : 밑바닥부터 시작하는 딥러닝</p><h3 id=\"rmsprop\">RMSProp</h3><ul>  <li>AdaGrad는 과거의 기울기를 제곱하여 계속 더한다. 그래서 학습을 진행할수록 step이 줄어든다.</li>  <li>만약 무한대로 계속 학습한다면, step이 0에 수렴할것이고 그때부터는 갱신되지 않을것이다.</li>  <li>즉 오래 학습을 한다고 의미 있는 값이 나오는것이 아니다.</li>  <li>이문제를 개선한 기법이 <strong>RMSProp</strong>이다.</li>  <li>이전 누적치와 현재 gradient의 좌표별 제곱의 가중치 평균을 생각한다.</li>  <li>점화식 :  <br />\\(h_n\\ =\\ \\gamma h_{n-1}\\ +\\ \\left(1\\ -\\ \\gamma \\right)\\nabla f\\left(x_n\\right)\\ \\odot \\ \\nabla f\\left(x_n\\right),\\ \\ h_{-1}\\ =\\ 0\\) <br />\\(x_{n+1}\\ =\\ x_n\\ -\\ \\eta \\ \\frac{1}{\\sqrt{h_n}}\\odot \\ \\nabla f\\left(x_n\\right)\\)</li>  <li>forgetting factor (decay rate) γ가 클수록 과거가 중요하고 작을수록 현재가 중요하다.</li>  <li>내분을 하면 H_n이 한없이 커지지않는다.<strong>(공부할것)</strong></li></ul><h3 id=\"adam\">Adam</h3><ul>  <li>Momentum과 RMSProp 두가지 방식을 합친것</li>  <li>복잡하다. 하지만 현재 가장 많이 쓰이는 Optimizer다. (전혀 아담하지않다(?))</li>  <li>가중치 β_1으로 Momentum을 변형하여 점화식 : \\(m_n\\ =\\ \\beta _1m_{n-1}\\ +\\ \\left(1-\\beta _1\\right)\\nabla f\\left(x_n\\right),\\ m_{-1}\\ =\\ 0\\)</li>  <li>을 생각하고 가중치 β_2로 AdaGrad를 변형하여 점화식 : \\(v_n\\ =\\ \\beta _2v_{n-1}\\ +\\ \\left(1-\\beta _2\\right)\\nabla f\\left(x_n\\right)\\odot \\nabla f\\left(x_n\\right),\\ \\ v_{-1}\\ =\\ 0\\)</li>  <li>을 생각한다.</li>  <li>Adam은 2015년에 제안된 새로운 방법이다.</li></ul><p>※ 코드 테스트 해볼것 chapter06 - test</p><p><img src=\"/assets/images/blog_images/DeepNeuralNetwork/fig%206-7.png\" alt=\"dl_adam_graph\" />출처 : 밑바닥부터 시작하는 딥러닝</p><h3 id=\"최적화-기법-비교--sgd-momentum-adagrad-adam\">최적화 기법 비교 : SGD, Momentum, AdaGrad, Adam</h3><p><img src=\"/assets/images/blog_images/DeepNeuralNetwork/fig%206-8.png\" alt=\"dl_compare_minimun\" />출처 : 밑바닥부터 시작하는 딥러닝</p>",
            "url": "http://localhost:4000/optimization-parameter/",
            
            
            
            "tags": ["pandoly2","machine learning","AI"],
            
            "date_published": "2022-04-23T00:00:00+00:00",
            "date_modified": "2022-04-23T00:00:00+00:00",
            
                "author": "pandoly2"
            
        },
    
        {
            "id": "http://localhost:4000/back-propagation/",
            "title": "Feed Forward and Back Propagation",
            "summary": null,
            "content_text": "TBD : PageFeed ForwardBack Propagation (오차역전법)",
            "content_html": "<p>TBD : Page</p><h2 id=\"feed-forward\">Feed Forward</h2><h2 id=\"back-propagation-오차역전법\">Back Propagation (오차역전법)</h2>",
            "url": "http://localhost:4000/back-propagation/",
            
            
            
            "tags": ["pandoly2","machine learning","AI"],
            
            "date_published": "2022-04-20T00:00:00+00:00",
            "date_modified": "2022-04-20T00:00:00+00:00",
            
                "author": "pandoly2"
            
        },
    
        {
            "id": "http://localhost:4000/neural-network/",
            "title": "신경망 (Neural Network)",
            "summary": null,
            "content_text": "신경망의 구조  퍼셉트론이 여러층 연결되어 구성되듯이 인간의 신경계도 여러개의 뉴런의 신호전달로 이루어진다.      인공신경망도 신경계와같이 여러개의 노드들간의 입력신호와 출력신호, 가중치, 바이어스에따라서 신호 전달이 이루어진다. ※ 인공 신경망과 퍼셉트론으 가장 큰 차이는, 퍼셉트론은 가중치를 결정할때 사람이 직접 관여를 해야하지만, 인공 신경망은 데이터를 통해 스스로 학습하여 가중치를 결정한다는 점이다.        다층 퍼셉트론         신경망     인공 신경망의 구조   인공 신경망은 입력층, 은닉층, 출력층으로 구성된다.  각각의 원은 뉴런 또는 노드라는 용어로 사용된다인공 신경망 구성  퍼셉트론의 flow를 생각해보면, linear regression의 출력값을 classification 함수를 사용하여 분류를 했다.  신경망의 뉴런또한 어떠한 신호가 들어왔을때, 특정한 임계치를 넘지않으면 그다음 뉴런으로 신호가 발생 되지않는다.  이것과 마찬가지로 인공 신경망에서도 활성화 함수라고 명칭하는 분류기 혹은 해당신호를 다음 노드의 입력으로 출력할 지를 결정하는 함수가 존재한다.  그림은 다음과 같다.출처 : 밑바닥부터 시작하는 딥러닝  입력신호와 가중치 그리고 바이어스를 조합한 결과가 a노드가 되고 a노드는 h()라는 활성화 함수를 통과하여 y노드로 변환된다.  즉 활성화 함수를 통해 신호가 1/0으로 구분된다.활성화 함수의 종류  Sigmoid  ReLU  Leaky ReLU  tanh  활성화 함수에대해서 좀더 자세히 알고싶다면 다음 블로그를 참고하면 된다.  활성화 함수 종류",
            "content_html": "<h3 id=\"신경망의-구조\">신경망의 구조</h3><ul>  <li>퍼셉트론이 여러층 연결되어 구성되듯이 인간의 신경계도 여러개의 뉴런의 신호전달로 이루어진다.</li>  <li>    <p>인공신경망도 신경계와같이 여러개의 노드들간의 입력신호와 출력신호, 가중치, 바이어스에따라서 신호 전달이 이루어진다. <br /><strong>※ 인공 신경망과 퍼셉트론으 가장 큰 차이는, 퍼셉트론은 가중치를 결정할때 사람이 직접 관여를 해야하지만, 인공 신경망은 데이터를 통해 스스로 학습하여 가중치를 결정한다는 점이다.</strong></p>  </li>  <li>    <p>다층 퍼셉트론 <br /><img src=\"/assets/images/blog_images/Perceptron/perceptron_xor_detail_solution.png\" alt=\"perceptron_xor_detail_solution\" /></p>  </li>  <li>    <p>신경망 <br /><img src=\"/assets/images/blog_images/NeuralNetwork/nn_natural.png\" alt=\"nn_natural\" /></p>  </li>  <li>인공 신경망의 구조 <br /><img src=\"/assets/images/blog_images/NeuralNetwork/nn_artificial.png\" alt=\"nn_artificial\" /></li>  <li>인공 신경망은 입력층, 은닉층, 출력층으로 구성된다.</li>  <li>각각의 원은 뉴런 또는 노드라는 용어로 사용된다</li></ul><h3 id=\"인공-신경망-구성\">인공 신경망 구성</h3><ul>  <li>퍼셉트론의 flow를 생각해보면, linear regression의 출력값을 classification 함수를 사용하여 분류를 했다.</li>  <li>신경망의 뉴런또한 어떠한 신호가 들어왔을때, 특정한 임계치를 넘지않으면 그다음 뉴런으로 신호가 발생 되지않는다.</li>  <li>이것과 마찬가지로 인공 신경망에서도 <strong>활성화 함수</strong>라고 명칭하는 분류기 혹은 해당신호를 다음 노드의 입력으로 출력할 지를 결정하는 함수가 존재한다.</li>  <li>그림은 다음과 같다.</li></ul><p><img src=\"/assets/images/blog_images/NeuralNetwork/nn_activation_struct.png\" alt=\"nn_activation_struct\" />출처 : 밑바닥부터 시작하는 딥러닝</p><ul>  <li>입력신호와 가중치 그리고 바이어스를 조합한 결과가 a노드가 되고 a노드는 h()라는 활성화 함수를 통과하여 y노드로 변환된다.</li>  <li>즉 활성화 함수를 통해 신호가 1/0으로 구분된다.</li></ul><h3 id=\"활성화-함수의-종류\">활성화 함수의 종류</h3><ul>  <li>Sigmoid</li>  <li>ReLU</li>  <li>Leaky ReLU</li>  <li>tanh</li>  <li>활성화 함수에대해서 좀더 자세히 알고싶다면 다음 블로그를 참고하면 된다.  <a href=\"https://wooono.tistory.com/209\">활성화 함수 종류</a></li></ul>",
            "url": "http://localhost:4000/neural-network/",
            
            
            
            "tags": ["pandoly2","machine learning","AI"],
            
            "date_published": "2022-04-20T00:00:00+00:00",
            "date_modified": "2022-04-20T00:00:00+00:00",
            
                "author": "pandoly2"
            
        },
    
        {
            "id": "http://localhost:4000/perceptron/",
            "title": "퍼셉트론 (perceptron)",
            "summary": null,
            "content_text": "퍼셉트론 아이디어  지금까지 정리한 linear regression , logistic regression과 같은 개념으로 여러개의 입력데이터를 받아 하나의 출력을 갖는 구조를 퍼셉트론이라고 부름.  linear regression으로 최적의 구분선을 찾고 classification으로 데이터를 분류하는것이 개념적으로 동일해 보임.퍼셉트론 탄생  퍼셉트론은 프랑크 로젠블라트가 1957년에 고안한 알고리즘입니다. (신경망(딥러닝)의 기원이 되는 알고리즘)  퍼셉트론의 구조를 배우는 것은 신경망과 딥러닝으로 나아가는데 중요한 아이디어를 배우는 일.퍼셉트론이란?출처 : WikiDocs  퍼셉트론은 다수의 신호를 입력으로 받아 하나의 신호를 출력 (신호 : 전류나 강물처럼 흐름이 있는 것)  퍼셉트론의 신호는 ‘흐른다/안 흐른다(1이나 0)’의 두 가지 값을 가질수 있음.출처 : NeoWizard  x1과 x2는 입력신호, y는 출력 신호, w1과 w2는 가중치를 의미  원을 뉴런 또는 노드라고 부른다  입력 신호가 뉴런에 보내질 때는 각각 고유한 가중치가 곱해진다.      뉴런에서 전달 받은 신호의 총합이 임계값 θ를 넘을때만 1을 출력한다 (활성화 함수 or classification 함수)        수식 :  \\(y\\ =\\ \\begin{cases}0\\ \\left({w}_1{x}_1\\ +\\ {w}_2{x}_2\\ \\le \\ \\theta \\right)\\\\1\\ \\left({w}_1x_1\\ +\\ {w}_2x_2\\ &gt;\\ \\theta \\right)\\end{cases}\\)    퍼셉트론은 복수의 입력 신호 각각에 가중치를 부여한다. 가중치는 각 신호가 결과에 주는 영향력을 조절하는 요소로 작용하며, 가중치가 클수록 해당 신호가 그만큼 더 중요함을 뜻한다.퍼셉트론의 대표적인 예 단순논리회로 - AND, OR, NAD, XOR 문제  우리는 다수의 입력에대하여 하나의 출력을 나타내는 단순한 논리회로를 퍼셉트론으로 나타낼수 있다.  (명칭은 퍼셉트론이라고 하지만 개념적으로는 적절한 선을 찾고(linear regression)그에 따른 분류를하는 것(classification))AND Gate출처 : NeoWizard입력 x1, x2에대해 아래의의 수식에 대입해보면 만족하는 w1, w2, θ값을 여러개 찾을 수 있다. 그중 하나의 예는 (0.5, 0.5, 0,7)      수식 :  \\(y\\ =\\ \\begin{cases}0\\ \\left({w}_1{x}_1\\ +\\ {w}_2{x}_2\\ \\le \\ \\theta \\right)\\\\1\\ \\left({w}_1x_1\\ +\\ {w}_2x_2\\ &gt;\\ \\theta \\right)\\end{cases}\\)        이를 시각화하면 다음과 같은 그림이다.  출처 : WikiDocsNAND Gate출처 : NeoWizard입력 x1, x2에대해 아래의의 수식에 대입해보면 만족하는 w1, w2, θ값을 여러개 찾을 수 있다. 그중 하나의 예는 (-0.5, -0.5, -0,7)  시각화 출처 : WikiDocsOR Gate출처 : NeoWizard입력 x1, x2에대해 아래의의 수식에 대입해보면 만족하는 w1, w2, θ값을 여러개 찾을 수 있다. 그중 하나의 예는 (0.5, 0.5, 0,2)  시각화 출처 : WikiDocs가중치와 편향 수식  위의 식에서 θ를 b(bias)로 변경하여 식을 나타내면 다음과같다.  즉, regression 에서도 확인했듯이 최적의 가중치와 바이어스를 찾는 문제와 같다. 찾은 가중치와 바이어스에 의한 결과값이 0보다 큰지 작은지에 대한 분류 문제인것이다.\\[y\\ =\\ \\begin{cases}0\\ \\left({w}_1{x}_1\\ +\\ {w}_2{x}_2\\ +\\ b\\le \\ 0\\right)\\\\1\\ \\left({w}_1x_1\\ +\\ {w}_2x_2\\ \\ +\\ b&gt;\\ 0\\right)\\end{cases}\\]단일 퍼셉트론의 문제 - 논리회로 XOR  다음은 XOR의 진리표이다.  배타적 논리합이라는 논리 회로로 x1, x2 한쪽이 1일때만 1을 출력한다.출처 : NeoWizard      시각화 출처 : NeoWizard    XOR은 위의 그림과같이 하나의 직선으로 두영역을 나눌수없다.  만약 직선 즉, 선형이라는 제약이 없다면 비선형으로 다음과 같이 나눌수 있을것이다.출처 : NeoWizard  하지만 비선형적 곡선을 나타내기어렵다.  우리는 여러개의 퍼셉트론을 엮어서 비선형처럼 만들수있다. (여러개의 직선)  이 아이디어가 다층 퍼셉트론으로 단일 퍼셉트론의 한계를 극복했다.출처 : WikidocsXOR 문제의 솔루션  단일 퍼셉트론 OR, NAND, AND를 결합하여 XOR을 만들었다.  입력 x1, x2를 각각(OR, NAND)의 gate에 입력으로 사용하고 그 출력을 다시 AND의 입력으로 사용한다.  이렇게 함으로써 단일 직선으로 해결하기 어려웠던 문제를 해결 할 수 있다.XOR 논리게이트의 상세 Diagram  XOR의 논리게이트로 표현된 위의 그림을 우리가 학습했던 linear regression과 classification으로 표현하면 다음과 같다.",
            "content_html": "<h3 id=\"퍼셉트론-아이디어\">퍼셉트론 아이디어</h3><ul>  <li>지금까지 정리한 linear regression , logistic regression과 같은 개념으로 여러개의 입력데이터를 받아 하나의 출력을 갖는 구조를 퍼셉트론이라고 부름.</li>  <li>linear regression으로 최적의 구분선을 찾고 classification으로 데이터를 분류하는것이 개념적으로 동일해 보임.</li></ul><h3 id=\"퍼셉트론-탄생\">퍼셉트론 탄생</h3><ul>  <li>퍼셉트론은 프랑크 로젠블라트가 1957년에 고안한 알고리즘입니다. (신경망(딥러닝)의 기원이 되는 알고리즘)</li>  <li>퍼셉트론의 구조를 배우는 것은 신경망과 딥러닝으로 나아가는데 중요한 아이디어를 배우는 일.</li></ul><h3 id=\"퍼셉트론이란\">퍼셉트론이란?</h3><p><img src=\"/assets/images/blog_images/Perceptron/perceptron_detail.png\" alt=\"perceptron_basic_0\" />출처 : WikiDocs</p><ul>  <li>퍼셉트론은 다수의 신호를 입력으로 받아 하나의 신호를 출력 (신호 : 전류나 강물처럼 흐름이 있는 것)</li>  <li>퍼셉트론의 신호는 ‘흐른다/안 흐른다(1이나 0)’의 두 가지 값을 가질수 있음.</li></ul><p><img src=\"/assets/images/blog_images/Perceptron/perceptron.png\" alt=\"perceptron_basic\" />출처 : NeoWizard</p><ul>  <li>x1과 x2는 입력신호, y는 출력 신호, w1과 w2는 가중치를 의미</li>  <li>원을 <strong>뉴런</strong> 또는 <strong>노드</strong>라고 부른다</li>  <li>입력 신호가 뉴런에 보내질 때는 각각 고유한 가중치가 곱해진다.</li>  <li>    <p>뉴런에서 전달 받은 신호의 총합이 임계값 θ를 넘을때만 1을 출력한다 (활성화 함수 or classification 함수)</p>  </li>  <li>    <p>수식 :  \\(y\\ =\\ \\begin{cases}0\\ \\left({w}_1{x}_1\\ +\\ {w}_2{x}_2\\ \\le \\ \\theta \\right)\\\\1\\ \\left({w}_1x_1\\ +\\ {w}_2x_2\\ &gt;\\ \\theta \\right)\\end{cases}\\)</p>  </li>  <li>퍼셉트론은 복수의 입력 신호 각각에 가중치를 부여한다. 가중치는 각 신호가 결과에 주는 영향력을 조절하는 요소로 작용하며, 가중치가 클수록 해당 신호가 그만큼 더 중요함을 뜻한다.</li></ul><h3 id=\"퍼셉트론의-대표적인-예-단순논리회로---and-or-nad-xor-문제\">퍼셉트론의 대표적인 예 단순논리회로 - AND, OR, NAD, XOR 문제</h3><ul>  <li>우리는 다수의 입력에대하여 하나의 출력을 나타내는 단순한 논리회로를 퍼셉트론으로 나타낼수 있다.  (명칭은 퍼셉트론이라고 하지만 개념적으로는 적절한 선을 찾고(linear regression)그에 따른 분류를하는 것(classification))</li></ul><h4 id=\"and-gate\">AND Gate</h4><p><img src=\"/assets/images/blog_images/Perceptron/perceptron_and.png\" alt=\"perceptron_and_gate\" />출처 : NeoWizard</p><p>입력 x1, x2에대해 아래의의 수식에 대입해보면 만족하는 w1, w2, θ값을 여러개 찾을 수 있다. <br />그중 하나의 예는 (0.5, 0.5, 0,7)</p><ul>  <li>    <p>수식 :  \\(y\\ =\\ \\begin{cases}0\\ \\left({w}_1{x}_1\\ +\\ {w}_2{x}_2\\ \\le \\ \\theta \\right)\\\\1\\ \\left({w}_1x_1\\ +\\ {w}_2x_2\\ &gt;\\ \\theta \\right)\\end{cases}\\)</p>  </li>  <li>    <p>이를 시각화하면 다음과 같은 그림이다.</p>  </li></ul><p><img src=\"/assets/images/blog_images/Perceptron/perceptron_v_and.png\" alt=\"perceptron_v_and\" />출처 : WikiDocs</p><h4 id=\"nand-gate\">NAND Gate</h4><p><img src=\"/assets/images/blog_images/Perceptron/perceptron_nand.png\" alt=\"perceptron_nand_gate\" />출처 : NeoWizard</p><p>입력 x1, x2에대해 아래의의 수식에 대입해보면 만족하는 w1, w2, θ값을 여러개 찾을 수 있다. <br />그중 하나의 예는 (-0.5, -0.5, -0,7)</p><ul>  <li>시각화 <br /><img src=\"/assets/images/blog_images/Perceptron/perceptron_v_nand.png\" alt=\"perceptron_v_nand\" />출처 : WikiDocs</li></ul><h4 id=\"or-gate\">OR Gate</h4><p><img src=\"/assets/images/blog_images/Perceptron/perceptron_or.png\" alt=\"perceptron_or_gate\" />출처 : NeoWizard</p><p>입력 x1, x2에대해 아래의의 수식에 대입해보면 만족하는 w1, w2, θ값을 여러개 찾을 수 있다. <br />그중 하나의 예는 (0.5, 0.5, 0,2)</p><ul>  <li>시각화 <br /><img src=\"/assets/images/blog_images/Perceptron/perceptron_v_or.png\" alt=\"perceptron_v_or\" />출처 : WikiDocs</li></ul><h4 id=\"가중치와-편향-수식\">가중치와 편향 수식</h4><ul>  <li>위의 식에서 θ를 b(bias)로 변경하여 식을 나타내면 다음과같다.  즉, regression 에서도 확인했듯이 최적의 가중치와 바이어스를 찾는 문제와 같다. 찾은 가중치와 바이어스에 의한 결과값이 0보다 큰지 작은지에 대한 분류 문제인것이다.</li></ul>\\[y\\ =\\ \\begin{cases}0\\ \\left({w}_1{x}_1\\ +\\ {w}_2{x}_2\\ +\\ b\\le \\ 0\\right)\\\\1\\ \\left({w}_1x_1\\ +\\ {w}_2x_2\\ \\ +\\ b&gt;\\ 0\\right)\\end{cases}\\]<h3 id=\"단일-퍼셉트론의-문제---논리회로-xor\">단일 퍼셉트론의 문제 - 논리회로 XOR</h3><ul>  <li>다음은 XOR의 진리표이다.</li>  <li>배타적 논리합이라는 논리 회로로 x1, x2 한쪽이 1일때만 1을 출력한다.</li></ul><p><img src=\"/assets/images/blog_images/Perceptron/perceptron_xor.png\" alt=\"perceptron_xor\" />출처 : NeoWizard</p><ul>  <li>    <p>시각화 <br /><img src=\"/assets/images/blog_images/Perceptron/perceptron_v_xor_0.png\" alt=\"perceptron_v_xor_0\" />출처 : NeoWizard</p>  </li>  <li>XOR은 위의 그림과같이 하나의 직선으로 두영역을 나눌수없다.</li>  <li>만약 직선 즉, 선형이라는 제약이 없다면 비선형으로 다음과 같이 나눌수 있을것이다.</li></ul><p><img src=\"/assets/images/blog_images/Perceptron/perceptron_v_xor_1.png\" alt=\"perceptron_v_xor_1\" />출처 : NeoWizard</p><ul>  <li>하지만 비선형적 곡선을 나타내기어렵다.</li>  <li>우리는 여러개의 퍼셉트론을 엮어서 비선형처럼 만들수있다. (여러개의 직선)</li>  <li>이 아이디어가 다층 퍼셉트론으로 단일 퍼셉트론의 한계를 극복했다.</li></ul><p><img src=\"/assets/images/blog_images/Perceptron/perceptron_v_xor_2.png\" alt=\"perceptron_v_xor_2\" />출처 : Wikidocs</p><h4 id=\"xor-문제의-솔루션\">XOR 문제의 솔루션</h4><ul>  <li>단일 퍼셉트론 OR, NAND, AND를 결합하여 XOR을 만들었다.</li>  <li>입력 x1, x2를 각각(OR, NAND)의 gate에 입력으로 사용하고 그 출력을 다시 AND의 입력으로 사용한다.</li>  <li>이렇게 함으로써 단일 직선으로 해결하기 어려웠던 문제를 해결 할 수 있다.</li></ul><p><img src=\"/assets/images/blog_images/Perceptron/perceptron_xor_solution.png\" alt=\"perceptron_xor_solution\" /></p><h4 id=\"xor-논리게이트의-상세-diagram\">XOR 논리게이트의 상세 Diagram</h4><ul>  <li>XOR의 논리게이트로 표현된 위의 그림을 우리가 학습했던 linear regression과 classification으로 표현하면 다음과 같다.</li></ul><p><img src=\"/assets/images/blog_images/Perceptron/perceptron_xor_detail_solution.png\" alt=\"perceptron_xor_detail_solution\" /></p>",
            "url": "http://localhost:4000/perceptron/",
            
            
            
            "tags": ["pandoly2","machine learning","AI"],
            
            "date_published": "2022-04-20T00:00:00+00:00",
            "date_modified": "2022-04-20T00:00:00+00:00",
            
                "author": "pandoly2"
            
        },
    
        {
            "id": "http://localhost:4000/logistic-regression/",
            "title": "Logistic Regression",
            "summary": null,
            "content_text": "분류 (classification)  Training data 특성과 관계 등을 파악 한 후에, 미지의 입력 데이터에 대해서 결과가 어떤 종류의 값으로 분류 될 수 있는지를 예측하는것. 예) 스팸문자 분류출처 : NeoWizard  즉, Logistic Regression 알고리즘은,          Training data 특성과 분포를 나타내는 최적의 직선을 찾고(linear regression)      그 직선을 기준으로 데이터를 위(1) 또는 아래(0) 등으로 분류(Classification) 해주는 알고리즘        이러한 Logistic Regression은 Classification 알고리즘 중에서도 정확도가 높은 알고리즘으로 알려져 있어서 Deep learning에서 기본 component로 사용되고 있다고함.출처 : NeoWizard  출력 값 y가 0 또는 1 만을 가져야하는 분류 시스템에서, 분류 함수로 sigmoid함수를 사용하여 0/1 값을 갖게 할 수 있음  즉, linear regression 출력 Wx + b 가 어떤 값을 갖더라도, 출력 함수로 sigmoid 를 사용해서          sigmoid 계산 값이 0.5보다 크면 결과로 1이 나올 확률이 높다는 것이기 때문에 출력 값 y는 1을 정의      sigmoid 계산 값이 0.5 미만이면 결과로 0 이 나올 확률이 높다는 것이므로 출력 값 y는 0을 정의            0 또는 1을 분류하는 classification system 구현 할 수 있음.    Sigmoid 식 :  \\(y\\ =\\ sigmoid\\left(z\\right)\\ =\\ \\phi \\left(z\\right)\\ =\\ \\frac{1}{1\\ +\\ {e}^{-z}}\\) 출처 : NeoWizard  분류 시스템 최종 출력 값 y는 sigmoid 함수에 의해 논리적으로 1 또는 0 값을 가지기 때문에, 연속 값을 갖는 선형회귀 때와는 다른 손실함수 필요함.  손실함수 (cross-entropy) 식 :\\[y\\ =\\ \\frac{1}{1\\ +\\ {e}^{-1\\left(Wx\\ +b\\right)}}\\ ,\\ t_i\\ =\\ 0\\ or\\ 1\\]\\[E\\left(W,\\ b\\right)\\ =\\ -\\ \\sum _{i\\ =\\ 1}^n\\left\\{t_i\\log {y_i\\ }+\\ \\left(1-t_i\\right)\\log {\\left(1\\ -\\ y_i\\right)}\\right\\}\\]  classification 최종 출력 값 y 는 sigmoid 함수 에 의해 0 ~ 1 사이의 값을 갖는 확률적인 분류 모델이므로, 다음과 같이 확률변수 C 를 이용해 출력 값을 나타낼 수 있음.TBD: 이해하고 다시 작성★ NeoWizard ppt 62page 참조logistic regression flow",
            "content_html": "<h2 id=\"분류-classification\">분류 (classification)</h2><ul>  <li>Training data 특성과 관계 등을 파악 한 후에, 미지의 입력 데이터에 대해서 결과가 어떤 종류의 값으로 분류 될 수 있는지를 예측하는것. 예) 스팸문자 분류</li></ul><p><img src=\"/assets/images/blog_images/classification/classification.png\" alt=\"classification_image\" width=\"100%\" />출처 : NeoWizard</p><ul>  <li>즉, Logistic Regression 알고리즘은,    <ol>      <li>Training data 특성과 분포를 나타내는 최적의 직선을 찾고(linear regression)</li>      <li>그 직선을 기준으로 데이터를 위(1) 또는 아래(0) 등으로 분류(Classification) 해주는 알고리즘</li>    </ol>  </li>  <li>이러한 Logistic Regression은 Classification 알고리즘 중에서도 정확도가 높은 알고리즘으로 알려져 있어서 Deep learning에서 기본 component로 사용되고 있다고함.</li></ul><p><img src=\"/assets/images/blog_images/classification/classification_simple_flow.png\" alt=\"classification_simple\" width=\"100%\" height=\"100%\" />출처 : NeoWizard</p><ul>  <li>출력 값 y가 0 또는 1 만을 가져야하는 분류 시스템에서, 분류 함수로 sigmoid함수를 사용하여 0/1 값을 갖게 할 수 있음</li>  <li>즉, linear regression 출력 Wx + b 가 어떤 값을 갖더라도, 출력 함수로 sigmoid 를 사용해서    <ol>      <li>sigmoid 계산 값이 0.5보다 크면 결과로 1이 나올 확률이 높다는 것이기 때문에 출력 값 y는 1을 정의</li>      <li>sigmoid 계산 값이 0.5 미만이면 결과로 0 이 나올 확률이 높다는 것이므로 출력 값 y는 0을 정의</li>    </ol>  </li>  <li>    <p>0 또는 1을 분류하는 classification system 구현 할 수 있음.</p>  </li>  <li>Sigmoid 식 :  \\(y\\ =\\ sigmoid\\left(z\\right)\\ =\\ \\phi \\left(z\\right)\\ =\\ \\frac{1}{1\\ +\\ {e}^{-z}}\\)</li></ul><p><img src=\"/assets/images/blog_images/classification/classification_sigmoid_graph.png\" alt=\"classification_sigmoid\" /> 출처 : NeoWizard</p><ul>  <li>분류 시스템 최종 출력 값 y는 sigmoid 함수에 의해 논리적으로 1 또는 0 값을 가지기 때문에, 연속 값을 갖는 선형회귀 때와는 다른 손실함수 필요함.</li>  <li>손실함수 (cross-entropy) 식 :</li></ul>\\[y\\ =\\ \\frac{1}{1\\ +\\ {e}^{-1\\left(Wx\\ +b\\right)}}\\ ,\\ t_i\\ =\\ 0\\ or\\ 1\\]\\[E\\left(W,\\ b\\right)\\ =\\ -\\ \\sum _{i\\ =\\ 1}^n\\left\\{t_i\\log {y_i\\ }+\\ \\left(1-t_i\\right)\\log {\\left(1\\ -\\ y_i\\right)}\\right\\}\\]<ul>  <li>classification 최종 출력 값 y 는 sigmoid 함수 에 의해 0 ~ 1 사이의 값을 갖는 확률적인 분류 모델이므로, 다음과 같이 확률변수 C 를 이용해 출력 값을 나타낼 수 있음.</li></ul><p><strong>TBD: 이해하고 다시 작성</strong></p><font color=\"red\">★ NeoWizard ppt 62page 참조</font><h3 id=\"logistic-regression-flow\">logistic regression flow</h3><p><img src=\"/assets/images/blog_images/classification/logistic_regression_flow.png\" alt=\"classification_flow\" /></p>",
            "url": "http://localhost:4000/logistic-regression/",
            
            
            
            "tags": ["pandoly2","machine learning","AI"],
            
            "date_published": "2022-04-19T00:00:00+00:00",
            "date_modified": "2022-04-19T00:00:00+00:00",
            
                "author": "pandoly2"
            
        },
    
        {
            "id": "http://localhost:4000/gradient-decent/",
            "title": "경사하강법 (Gradient decent algorithm)",
            "summary": null,
            "content_text": "손실함수      손실함수는 오차의 평균값을 나타내기 때문에, 손실함수가 최소값을 갖는다는 것은 실제 정답과 계산 값의 차이인 오차가 최소가 되어, 미지의 데이터에 대해서 결과를 잘 예측 할 수 있다는 것을 의미함.        손실함수는 W, b 에 영향을 받기 때문에, 손실함수가 최소가 되는 가중치 W와 바이어스 b를 찾는 것이 regression을 구현하는 최종 목표.  손실함수 식\\[loss\\ function= E(W,\\ b) = \\frac{\\left({t}_1\\ -\\ y_1\\right)^2\\ +\\left({t}_2\\ -\\ y_2\\right)^2\\ +\\ \\left({t}_3\\ -\\ y_3\\right)^2\\ +\\ ...\\ \\ +\\ \\left({t}_n\\ -\\ y_n\\right)^2\\ \\ }{n}\\]\\[=\\frac{\\left[{t}_1\\ -\\ \\left(Wx_1\\ +\\ b\\right)\\right]^2\\ +\\ \\left[{t}_2\\ -\\ \\left(Wx_2\\ +\\ b\\right)\\right]^2\\ +\\ ...\\ +\\ \\left[{t}_n\\ -\\ \\left(Wx_n\\ +\\ b\\right)\\right]^2\\ \\ }{n}\\]\\[=\\frac{1}{n}\\sum _{i=1}^n\\left[t_i\\ -\\ \\left(Wx_i\\ +\\ b\\right)\\right]^2\\]손실함수 최적화 기술  경사하강법  배치 경사하강법  확률적 경사하강법  미니 배치 경사하강법  모멘텀  아담  참고 : https://truman.tistory.com/164경사하강법 이해하기가정  E(W, b)에서 바이어스 b = 0으로 가정 (계산을 쉽게하고 손실함수의 모양을 파악하기 위해)  다음과 같은 training data set가 있을때, 출처 : NeoWizard  W 값에대한 손실함수 E(W, b) 계산 결과는 다음과 같다. 출처 : NeoWizard      loss function의 계산 결과를 그래프로 나타냈을때 다음과 같다. 출처 : NeoWizard출처 : NeoWizard    그러면, 우리가 어떤 가중치를 임의로 설정했을때 최소값을 찾아가는 방법은 뭘까?  아래와같은 방법을 생각해 볼 수 있다.   출처 : NeoWizard  방법은 아래와 같이 기울기가 작아지는쪽으로 이동 시키면된다. (가장낮은쪽으로 이동)    W 에서의 편미분 𝜕E(W)/𝜕W 가 해당 W 에서 기울기를 나타냄  𝜕E(W)/𝜕W 양수값을 갖는다면 W는 왼쪽으로 이동  𝜕E(W)/𝜕W 음수값을 갖는다면 W는 오른쪽으로 이동  식, W(weight) 가중치 :  \\(w\\ =\\ w\\ -\\ \\alpha \\ \\cdot \\ \\frac{\\partial E\\left(W,\\ b\\right)}{\\partial w}\\), b(bias) 바이어스 : \\(b\\ =\\ b\\ -\\ \\alpha \\ \\cdot \\ \\frac{\\partial E\\left(W,\\ b\\right)}{\\partial b}\\)(※ \\(\\alpha\\) 는 학습율(learning rate)라고 부르며, W 값의 감소 또는 증가 되는 비율을 나타냄)   출처 : NeoWizard  이처럼, W에서의 직선의 기울기인 미분 값을 이용하여, 그 값이 작아지는 방향으로 진행하여 손실함수 최소값을 찾는 방법을  경사하강법 (gradient decent algorithm)  이라고 한다.전체 학습 Flow (linear regression)",
            "content_html": "<h2 id=\"손실함수\">손실함수</h2><ul>  <li>    <p>손실함수는 오차의 평균값을 나타내기 때문에, 손실함수가 최소값을 갖는다는 것은 실제 정답과 계산 값의 차이인 오차가 최소가 되어, 미지의 데이터에 대해서 결과를 잘 예측 할 수 있다는 것을 의미함.</p>  </li>  <li>    <p>손실함수는 W, b 에 영향을 받기 때문에, <strong>손실함수가 최소가 되는 가중치 W와 바이어스 b를 찾는 것</strong>이 regression을 구현하는 최종 목표.</p>  </li></ul><h4 id=\"손실함수-식\">손실함수 식</h4>\\[loss\\ function= E(W,\\ b) = \\frac{\\left({t}_1\\ -\\ y_1\\right)^2\\ +\\left({t}_2\\ -\\ y_2\\right)^2\\ +\\ \\left({t}_3\\ -\\ y_3\\right)^2\\ +\\ ...\\ \\ +\\ \\left({t}_n\\ -\\ y_n\\right)^2\\ \\ }{n}\\]\\[=\\frac{\\left[{t}_1\\ -\\ \\left(Wx_1\\ +\\ b\\right)\\right]^2\\ +\\ \\left[{t}_2\\ -\\ \\left(Wx_2\\ +\\ b\\right)\\right]^2\\ +\\ ...\\ +\\ \\left[{t}_n\\ -\\ \\left(Wx_n\\ +\\ b\\right)\\right]^2\\ \\ }{n}\\]\\[=\\frac{1}{n}\\sum _{i=1}^n\\left[t_i\\ -\\ \\left(Wx_i\\ +\\ b\\right)\\right]^2\\]<h3 id=\"손실함수-최적화-기술\">손실함수 최적화 기술</h3><ul>  <li>경사하강법</li>  <li>배치 경사하강법</li>  <li>확률적 경사하강법</li>  <li>미니 배치 경사하강법</li>  <li>모멘텀</li>  <li>아담</li>  <li>참고 : <a href=\"https://truman.tistory.com/164\">https://truman.tistory.com/164</a></li></ul><h2 id=\"경사하강법-이해하기\">경사하강법 이해하기</h2><p><strong>가정</strong> <br /> E(W, b)에서 바이어스 b = 0으로 가정 (계산을 쉽게하고 손실함수의 모양을 파악하기 위해)</p><ul>  <li>다음과 같은 training data set가 있을때, <br /><img src=\"/assets/images/blog_images/Gradient/gradient_training_data.png\" alt=\"gradient_training_data\" />출처 : NeoWizard</li>  <li>W 값에대한 손실함수 E(W, b) 계산 결과는 다음과 같다. <br /><img src=\"/assets/images/blog_images/Gradient/gradient_loss_functiono_result.png\" alt=\"gradient_loss_function_result\" />출처 : NeoWizard</li>  <li>    <p>loss function의 계산 결과를 그래프로 나타냈을때 다음과 같다. <br /><img src=\"/assets/images/blog_images/Gradient/gradient_data_of_loss_fuction_result.png\" alt=\"gradient_data_of_loss_result\" />출처 : NeoWizard<img src=\"/assets/images/blog_images/Gradient/gradient_graph.png\" alt=\"gradient_graph\" />출처 : NeoWizard</p>  </li>  <li>그러면, 우리가 어떤 가중치를 임의로 설정했을때 최소값을 찾아가는 방법은 뭘까?</li>  <li>아래와같은 방법을 생각해 볼 수 있다. <br /> <img src=\"/assets/images/blog_images/Gradient/gradient_decent.png\" alt=\"gradient_graph\" /> 출처 : NeoWizard</li>  <li>방법은 아래와 같이 기울기가 <strong>작아지는쪽</strong>으로 이동 시키면된다. (가장낮은쪽으로 이동)   <br /> W 에서의 편미분 𝜕E(W)/𝜕W 가 해당 W 에서 기울기를 나타냄 <br /> 𝜕E(W)/𝜕W 양수값을 갖는다면 W는 왼쪽으로 이동 <br /> 𝜕E(W)/𝜕W 음수값을 갖는다면 W는 오른쪽으로 이동</li>  <li>식,<br /> W(weight) 가중치 :  \\(w\\ =\\ w\\ -\\ \\alpha \\ \\cdot \\ \\frac{\\partial E\\left(W,\\ b\\right)}{\\partial w}\\), b(bias) 바이어스 : \\(b\\ =\\ b\\ -\\ \\alpha \\ \\cdot \\ \\frac{\\partial E\\left(W,\\ b\\right)}{\\partial b}\\)</li></ul><p>(※ \\(\\alpha\\) 는 학습율(learning rate)라고 부르며, W 값의 감소 또는 증가 되는 비율을 나타냄) <br /> <img src=\"/assets/images/blog_images/Gradient/gradient_decent2.png\" alt=\"gradient_graph\" /> 출처 : NeoWizard</p><ul>  <li>이처럼, W에서의 직선의 기울기인 미분 값을 이용하여, 그 값이 작아지는 방향으로 진행하여 손실함수 최소값을 찾는 방법을 <font color=\"red\"> 경사하강법 (gradient decent algorithm) </font> 이라고 한다.</li></ul><h3 id=\"전체-학습-flow-linear-regression\">전체 학습 Flow (linear regression)</h3><p><img src=\"/assets/images/blog_images/Gradient/Gradient_decent_flow(linear_regression).png\" alt=\"gradient_flow\" width=\"100%\" height=\"100%\" /></p>",
            "url": "http://localhost:4000/gradient-decent/",
            
            
            
            "tags": ["pandoly2","machine learning","AI"],
            
            "date_published": "2022-04-19T00:00:00+00:00",
            "date_modified": "2022-04-19T00:00:00+00:00",
            
                "author": "pandoly2"
            
        },
    
        {
            "id": "http://localhost:4000/Regression-and-lossfunction/",
            "title": "회귀와 손실함수 (Regression And loss-function)",
            "summary": null,
            "content_text": "  linear Regression  손실함수회귀 (Regression)※ 용어 :  회귀(Regression)  원래 뜻은 예전으로 되돌아간다는 의미입니다. 영국의 유전학자 프랜시스 골턴이 부모와 자녀들 키의 연관 관계를 연구하다 보니, 개개인의 키는 결국 전체 키의 평균으로 수렴하는 경향이 있다는 걸 발견하고는 자신의 방법론에 ‘회귀 분석’이란 이름을 붙였다고 합니다.  ‘회귀’라는 용어는 1885년 영국의 과학자 갈톤(F. Galton)이 발표한 ‘유전에 의하여 보통사람의 신장으로 회귀(Regression toward Meiocrity in Hereditary Stature’라는 논문에서 비롯되었다. 그는 아들의 키와 부모의 평균 키와의 관계를 분석하였는데, 부모의 키가 매우 클 때(또는 작을 때) 아들의 키는 일반적으로 평균키보다는 크지만(작지만) 그들의 부모만큼 크(작)지는 않다는 결론이다. 즉 부모의 키가 크(작)더라도 그 자식들은 결국 보통키로 회귀(돌아간다)한다는 뜻이다.[출처] 회귀분석의 유래 : 대체 왜 Regression(회귀)이라고 불릴까?|작성자 바른인간선형 회귀 (Linear Regression)  Trainig Data를 이용하여 데이터의 특성과 상관관계 등을 파악하고, 그 결과를 바탕으로 Training Data에 없는 미지의 데이터가 주어졌을 경우에, 그 결과를 연속적인 (숫자) 값으로 예측 하는것(예) 공부시간과 시험성적 관계, 집 평수와 집 가격 관계 등출처 : NeoWizard  학습 데이터는 입력(x)인 공부시간에 비례해서 출력(y)인 시험성적도 증가하는 경향이 있음즉, 입력(x)과 출력(y)은 y = Wx + b 형태로 나타낼 수 있음출처 : NeoWizard  y = Wx + b를 만족하는 다양한 1, 2, 3과 같은 직선중, Training data의 특성을 가장 잘 표현할 수 있는 가중치 W(기울기), 바이어스 b(y 절편 or 편차)를 찾는 것이 학습(Learning) 개념임※ 용어 정리 : 머신러닝에서는 W는 가중치(weight), b는 바이어스(bias) 라고 함.최적의 W(weight)와 b(bias)를 찾는방법출처 : NeoWizard  Training data의 정답(t)과 직선 y = Wx + b 값의 차이인  오차(error) = t - y = t - (Wx + b)      오차가 크다면, 우리가 임의로 설정한 직선의 가중치와 바이어스 값이 잘못된 것이고, 오차가 작다면 직선의 가중치와 바이어스 값이 잘 된 것이기 때문에 미래 값 예측도 정확할 수 있다고 예상할 수 있음.    즉, 머신러닝의 Regression 시스템은, 모든 데이터의  오차(error) = t - y = t - (Wx + b)의 합이 최소가 되서, 미래 값을 잘 예측할 수 있는 가중치 W와 바이어스 b값을 찾아야 한다.전체 학습 Flow (linear regression)손실함수 (loss function)  위의 최적의 W(weight)와 b(bias)를 찾는방법에서 보았듯이 최적의 W(가중치)와 b(바이어스)를 찾는 것은 모든 데이터의 오차(error)가 작은 값을 찾으면된다.  즉, 손실함수 (loss function)는, training data의 정답(t)과 입력(x)에 대한 계산 값 y의 차이(error)를 모두 더해 수식으로 나타낸 것.출처 : NeoWizard  주의 : 각각의 오차를 모두 더해서 손실함수(loss function)을 구하면 각각의 오차가 (+), (-) 등이 동시에 존재하기 때문에 오차의 합이 0이 나올 수도 있음.  즉, 0 이라는 것이 최소 오차 값인지 아닌지를 판별하는 것이 어려움.  그래서, 손실함수에서 오차(error)를 계산할 때는 양변에 제곱을 사용하여 계산함.  \\[{\\left(t-y\\right)}^2\\ =\\ {\\left(t\\ -\\ {\\left[Wx\\ +\\ b\\right]}\\right)}^2\\]    즉, 오차는 언제나 양수이며, 제곱을 하기때문에 정답과 계산값 차이가 크다면, 제곱에 의해 오차는 더 큰 값을 가지게 되어 머신러닝 학습에 있어 장점을 가짐.손실함수 식\\[loss\\ function= E(W,\\ b) = \\frac{\\left({t}_1\\ -\\ y_1\\right)^2\\ +\\left({t}_2\\ -\\ y_2\\right)^2\\ +\\ \\left({t}_3\\ -\\ y_3\\right)^2\\ +\\ ...\\ \\ +\\ \\left({t}_n\\ -\\ y_n\\right)^2\\ \\ }{n}\\]\\[=\\frac{\\left[{t}_1\\ -\\ \\left(Wx_1\\ +\\ b\\right)\\right]^2\\ +\\ \\left[{t}_2\\ -\\ \\left(Wx_2\\ +\\ b\\right)\\right]^2\\ +\\ ...\\ +\\ \\left[{t}_n\\ -\\ \\left(Wx_n\\ +\\ b\\right)\\right]^2\\ \\ }{n}\\]\\[=\\frac{1}{n}\\sum _{i=1}^n\\left[t_i\\ -\\ \\left(Wx_i\\ +\\ b\\right)\\right]^2\\]  x 와 t 는 training data 에서 주어지는 값이므로, 손실함수(loss function)인 E(W, b)는 결국 W와 b에 영향을 받는 함수임.  E(W, b) 값이 작다는것은 정답(t, target)과 y = Wx + b에 의해 계산된 값의 평균 오차가 작다는 의미  평균 오차가 작다는 것은 임의의 데이터 x 가 주어질 경우, 확률적으로 미래의 결과값도 오차가 작을 것이라고 추측할 수 있음.즉, training data를 바탕으로 손실 함수 E(W, b)가 최소값을 갖도록 (W, b)를 구하는 것이 (linear) regression의 최종 목적임.",
            "content_html": "<ol>  <li>linear Regression</li>  <li>손실함수</li></ol><h2 id=\"회귀-regression\">회귀 (Regression)</h2><p>※ 용어 :  회귀(Regression)</p><ul>  <li>원래 뜻은 예전으로 되돌아간다는 의미입니다. 영국의 유전학자 프랜시스 골턴이 부모와 자녀들 키의 연관 관계를 연구하다 보니, 개개인의 키는 결국 전체 키의 평균으로 수렴하는 경향이 있다는 걸 발견하고는 자신의 방법론에 ‘회귀 분석’이란 이름을 붙였다고 합니다.</li>  <li>‘회귀’라는 용어는 1885년 영국의 과학자 갈톤(F. Galton)이 발표한 ‘유전에 의하여 보통사람의 신장으로 회귀(Regression toward Meiocrity in Hereditary Stature’라는 논문에서 비롯되었다. 그는 아들의 키와 부모의 평균 키와의 관계를 분석하였는데, 부모의 키가 매우 클 때(또는 작을 때) 아들의 키는 일반적으로 평균키보다는 크지만(작지만) 그들의 부모만큼 크(작)지는 않다는 결론이다. 즉 부모의 키가 크(작)더라도 그 자식들은 결국 보통키로 회귀(돌아간다)한다는 뜻이다.[출처] 회귀분석의 유래 : 대체 왜 Regression(회귀)이라고 불릴까?|작성자 바른인간</li></ul><h3 id=\"선형-회귀-linear-regression\">선형 회귀 (Linear Regression)</h3><ul>  <li>Trainig Data를 이용하여 데이터의 특성과 상관관계 등을 파악하고, 그 결과를 바탕으로 Training Data에 없는 미지의 데이터가 주어졌을 경우에, 그 결과를 연속적인 (숫자) 값으로 예측 하는것</li></ul><p>(예) 공부시간과 시험성적 관계, 집 평수와 집 가격 관계 등</p><p><img src=\"/assets/images/blog_images/Regression/linearRegression_01.PNG\" alt=\"linearRegression_01\" width=\"100%\" height=\"100%\" />출처 : NeoWizard</p><ul>  <li>학습 데이터는 입력(x)인 공부시간에 비례해서 출력(y)인 시험성적도 증가하는 경향이 있음즉, 입력(x)과 출력(y)은 <strong>y = Wx + b</strong> 형태로 나타낼 수 있음</li></ul><p><img src=\"/assets/images/blog_images/Regression/linearRegression_02.PNG\" alt=\"linearRegression_01\" width=\"100%\" height=\"100%\" />출처 : NeoWizard</p><ul>  <li><strong>y = Wx + b</strong>를 만족하는 다양한 <strong>1, 2, 3</strong>과 같은 직선중, Training data의 특성을 가장 잘 표현할 수 있는 가중치 <strong>W(기울기)</strong>, <strong>바이어스 b(y 절편 or 편차)</strong>를 찾는 것이 학습(Learning) 개념임※ 용어 정리 : 머신러닝에서는 W는 가중치(weight), b는 바이어스(bias) 라고 함.</li></ul><h4 id=\"최적의-wweight와-bbias를-찾는방법\">최적의 W(weight)와 b(bias)를 찾는방법</h4><p><img src=\"/assets/images/blog_images/Regression/linearRegression_03.PNG\" alt=\"linearRegression_01\" width=\"100%\" height=\"100%\" />출처 : NeoWizard</p><ul>  <li>Training data의 정답(t)과 직선 y = Wx + b 값의 차이인 <font color=\"red\"> 오차(error) = t - y = t - (Wx + b)</font></li>  <li>    <p>오차가 크다면, 우리가 임의로 설정한 직선의 가중치와 바이어스 값이 잘못된 것이고, 오차가 작다면 직선의 가중치와 바이어스 값이 잘 된 것이기 때문에 미래 값 예측도 정확할 수 있다고 예상할 수 있음.</p>  </li>  <li>즉, 머신러닝의 Regression 시스템은, <strong>모든 데이터</strong>의 <font color=\"red\"> 오차(error) = t - y = t - (Wx + b)</font>의 합이 최소가 되서, 미래 값을 잘 예측할 수 있는 가중치 W와 바이어스 b값을 찾아야 한다.</li></ul><h3 id=\"전체-학습-flow-linear-regression\">전체 학습 Flow (linear regression)</h3><p><img src=\"/assets/images/blog_images/Gradient/Gradient_decent_flow(linear_regression).png\" alt=\"gradient_flow\" width=\"100%\" height=\"100%\" /></p><h3 id=\"손실함수-loss-function\">손실함수 (loss function)</h3><ul>  <li>위의 <strong>최적의 W(weight)와 b(bias)를 찾는방법</strong>에서 보았듯이 최적의 W(가중치)와 b(바이어스)를 찾는 것은 모든 데이터의 오차(error)가 작은 값을 찾으면된다.</li>  <li>즉, <font color=\"red\">손실함수 (loss function)</font>는, training data의 정답(t)과 입력(x)에 대한 계산 값 y의 차이(error)를 모두 더해 수식으로 나타낸 것.</li></ul><p><img src=\"/assets/images/blog_images/Regression/linearRegression_03.PNG\" alt=\"linearRegression_01\" width=\"100%\" height=\"100%\" />출처 : NeoWizard</p><ul>  <li>주의 : 각각의 오차를 모두 더해서 손실함수(loss function)을 구하면 각각의 오차가 (+), (-) 등이 동시에 존재하기 때문에 오차의 합이 0이 나올 수도 있음.</li>  <li>즉, 0 이라는 것이 최소 오차 값인지 아닌지를 판별하는 것이 어려움.</li>  <li>그래서, <font color=\"red\">손실함수에서 오차(error)를 계산할 때는 양변에 제곱을 사용하여 계산함.</font></li>  <li>\\[{\\left(t-y\\right)}^2\\ =\\ {\\left(t\\ -\\ {\\left[Wx\\ +\\ b\\right]}\\right)}^2\\]  </li>  <li>즉, 오차는 언제나 양수이며, 제곱을 하기때문에 정답과 계산값 차이가 크다면, <strong>제곱에 의해 오차는 더 큰 값을 가지게 되어 머신러닝 학습에 있어 장점을 가짐</strong>.</li></ul><h4 id=\"손실함수-식\">손실함수 식</h4>\\[loss\\ function= E(W,\\ b) = \\frac{\\left({t}_1\\ -\\ y_1\\right)^2\\ +\\left({t}_2\\ -\\ y_2\\right)^2\\ +\\ \\left({t}_3\\ -\\ y_3\\right)^2\\ +\\ ...\\ \\ +\\ \\left({t}_n\\ -\\ y_n\\right)^2\\ \\ }{n}\\]\\[=\\frac{\\left[{t}_1\\ -\\ \\left(Wx_1\\ +\\ b\\right)\\right]^2\\ +\\ \\left[{t}_2\\ -\\ \\left(Wx_2\\ +\\ b\\right)\\right]^2\\ +\\ ...\\ +\\ \\left[{t}_n\\ -\\ \\left(Wx_n\\ +\\ b\\right)\\right]^2\\ \\ }{n}\\]\\[=\\frac{1}{n}\\sum _{i=1}^n\\left[t_i\\ -\\ \\left(Wx_i\\ +\\ b\\right)\\right]^2\\]<ul>  <li>x 와 t 는 training data 에서 주어지는 값이므로, 손실함수(loss function)인 E(W, b)는 결국 W와 b에 영향을 받는 함수임.</li>  <li>E(W, b) 값이 작다는것은 정답(t, target)과 y = Wx + b에 의해 계산된 값의 평균 오차가 작다는 의미</li>  <li>평균 오차가 작다는 것은 임의의 데이터 x 가 주어질 경우, 확률적으로 미래의 결과값도 오차가 작을 것이라고 추측할 수 있음.</li></ul><p><strong>즉, training data를 바탕으로 손실 함수 E(W, b)가 최소값을 갖도록 (W, b)를 구하는 것이 (linear) regression의 최종 목적임.</strong></p>",
            "url": "http://localhost:4000/Regression-and-lossfunction/",
            
            
            
            "tags": ["pandoly2","machine learning","AI"],
            
            "date_published": "2022-04-18T00:00:00+00:00",
            "date_modified": "2022-04-18T00:00:00+00:00",
            
                "author": "pandoly2"
            
        },
    
        {
            "id": "http://localhost:4000/Which-is-the-MachineLearning/",
            "title": "머신러닝 구분",
            "summary": null,
            "content_text": "구분학습 방법에 따라 다음과 같이 나뉨지도학습 (Supervised): 입력 값(x)과 정답(t, label)을 포함하는 Training Data를 이용하여 학습하고, 그 학습된 결과를 바탕으로 미지의 데이터(Test Data)에 대해 미래 값을 예측(Predict)하는 방법 =&gt; 대부분 머신러닝 문제는 지도학습에 해당됨.ex)  시험공부 시간(입력)과 Pass/Fail(정답)을 이용하여 당락 여부 예측  집 평수(입력)와 가격 데이터(정답) 이요하여 임의의 평수 가격 예측*지도학습은 학습결과를 바탕으로, 미래의 무엇을 예측하느냐에 따라 회귀, 분류등으로 구분할 수 있음회귀 (Regression): Training Data를 이용하여 연속적인 (숫자) 값을 예측하는 것을 말하며, 집평수와 가격 관계, 공부시간과 시험성적 등의 관계임.분류 (Classification): Training Data를 이용하여 주어진 입력값이 어떤 종류의 값인지 구별하는 것을 지칭함비지도학습 (Unsupervised): Training Data에 정답은 없고 입력 데이터만 있기 때문에, 입력에 대한 정답을 찾는 것이 아닌 입력데이터의 패턴, 특성 등을 학습을 통해 발견하는 방법을 말함. ex)  군집화(Clustering) 알고리즘을 이용한 뉴스 그룹핑, 백화점의 상품 추천시스템 등군집화 (Clustering)",
            "content_html": "<h2 id=\"구분\">구분</h2><p>학습 방법에 따라 다음과 같이 나뉨</p><h3 id=\"지도학습-supervised\">지도학습 (Supervised)</h3><p>: 입력 값(x)과 정답(t, label)을 포함하는 Training Data를 이용하여 학습하고, 그 학습된 결과를 바탕으로 미지의 데이터(Test Data)에 대해 미래 값을 예측(Predict)하는 방법 =&gt; 대부분 머신러닝 문제는 지도학습에 해당됨.</p><p>ex)</p><ol>  <li>시험공부 시간(입력)과 Pass/Fail(정답)을 이용하여 당락 여부 예측</li>  <li>집 평수(입력)와 가격 데이터(정답) 이요하여 임의의 평수 가격 예측</li></ol><p><strong>*지도학습은 학습결과를 바탕으로, 미래의 무엇을 예측하느냐에 따라 회귀, 분류등으로 구분할 수 있음</strong></p><h4 id=\"회귀-regression\">회귀 (Regression)</h4><p>: Training Data를 이용하여 <strong>연속적인 (숫자) 값을 예측</strong>하는 것을 말하며, 집평수와 가격 관계, 공부시간과 시험성적 등의 관계임.</p><h4 id=\"분류-classification\">분류 (Classification)</h4><p>: Training Data를 이용하여 주어진 입력값이 <strong>어떤 종류</strong>의 값인지 구별하는 것을 지칭함</p><h3 id=\"비지도학습-unsupervised\">비지도학습 (Unsupervised)</h3><p>: Training Data에 정답은 없고 입력 데이터만 있기 때문에, 입력에 대한 정답을 찾는 것이 아닌 입력데이터의 패턴, 특성 등을 학습을 통해 발견하는 방법을 말함. <br />ex)</p><ol>  <li>군집화(Clustering) 알고리즘을 이용한 뉴스 그룹핑, 백화점의 상품 추천시스템 등</li></ol><h4 id=\"군집화-clustering\">군집화 (Clustering)</h4>",
            "url": "http://localhost:4000/Which-is-the-MachineLearning/",
            
            
            
            "tags": ["pandoly2","machine learning","AI"],
            
            "date_published": "2022-04-18T00:00:00+00:00",
            "date_modified": "2022-04-18T00:00:00+00:00",
            
                "author": "pandoly2"
            
        },
    
        {
            "id": "http://localhost:4000/Prerequisite-of-Math/",
            "title": "수학개념 (Prerequisite for Machine Learning)",
            "summary": null,
            "content_text": "수치미분미분으로 얻을 수 있는 Insight-&gt; 입력 변수 x가 미세하게 변할때, 함수 f(x)가 얼마나 변하는지 알 수 있는 식을 구해라.  -&gt; 한수 f(x)는 입력 x의 미세한 변화에 얼마나 민감하게 반응하는지 알 수 있는 식을 구해라.      Insight  -&gt; 입력 x 를 현재 값에서 아주 조금 변화시키면, 함수 f(x)는 얼마나 변하는가?  -&gt; 함수 f(x)는 입력 x의 미세한 변화에 얼마나 민감하게 반응하는가? 어떤 값 x가 아주조금 변할때 y값의 미세한 변화량을 나타내는 기울기 혹은 크기 기본 미분 공식\\(f'\\left(x\\right)\\ =\\ \\frac{df\\left(x\\right)}{dx}\\ =\\lim _{\\Delta x \\to 0}{\\frac{f\\left(\\left(x\\ +\\ \\Delta x\\right)\\ -\\ f\\left(x\\right)\\right)}{\\Delta x}}\\)중앙차미분\\(f'\\left(x\\right)\\ =\\ \\frac{df\\left(x\\right)}{dx}\\ =\\lim _{\\Delta x\\to 0}^{ }\\frac{f\\left(\\left(x\\ +\\ \\Delta x\\right)\\ -\\ f\\left(x\\ -\\ \\Delta x\\right)\\right)}{2\\Delta x}\\)참조 : https://blog.naver.com/PostView.naver?blogId=mykepzzang편미분예 ) \\(f\\left(x,\\ y\\right)\\ =\\ 2x\\ +\\ 3xy\\ +\\ y3\\) 에 대해,      변수 x에 대하여 편미분 \\(\\frac{\\partial f\\left(x,y\\right)}{\\partial x}\\ =\\ \\frac{\\partial \\left(2x\\ +\\ 3xy\\ +{y}^3\\right)}{\\partial x}\\ =\\ 2\\ +\\ 3y\\)        변수 y에 대하여 편미분 \\(\\frac{\\partial f\\left(x,y\\right)}{\\partial x}\\ =\\ \\frac{\\partial \\left(2x\\ +\\ 3xy\\ +{y}^3\\right)}{\\partial x}\\ =\\ 3x\\ +\\ 3y^2\\)  Chain Rule합성함수란 여러 함수로 구성된 함수로서, 이러한 합성함수를 미분하려면 ‘합성함수를 구성하는 각 함수의 미분의 곱’으로 나타내는 chain rule(연쇄 법칙) 이용Ref. NeoWizard PPT 5 page (개인용)",
            "content_html": "<h2 id=\"수치미분\">수치미분</h2><h3 id=\"미분으로-얻을-수-있는-insight\">미분으로 얻을 수 있는 Insight</h3><p>-&gt; 입력 변수 x가 미세하게 변할때, 함수 f(x)가 얼마나 변하는지 알 수 있는 식을 구해라. <br /> -&gt; 한수 f(x)는 입력 x의 미세한 변화에 얼마나 민감하게 반응하는지 알 수 있는 식을 구해라.      <br /><strong>Insight</strong> <br /> -&gt; 입력 x 를 현재 값에서 아주 조금 변화시키면, 함수 f(x)는 얼마나 변하는가? <br /> -&gt; 함수 f(x)는 입력 x의 미세한 변화에 얼마나 민감하게 반응하는가?</p><font color=\"red\"> 어떤 값 x가 아주조금 변할때 y값의 미세한 변화량을 나타내는 기울기 혹은 크기 </font><h3 id=\"기본-미분-공식\">기본 미분 공식</h3><p>\\(f'\\left(x\\right)\\ =\\ \\frac{df\\left(x\\right)}{dx}\\ =\\lim _{\\Delta x \\to 0}{\\frac{f\\left(\\left(x\\ +\\ \\Delta x\\right)\\ -\\ f\\left(x\\right)\\right)}{\\Delta x}}\\)</p><h3 id=\"중앙차미분\">중앙차미분</h3><p>\\(f'\\left(x\\right)\\ =\\ \\frac{df\\left(x\\right)}{dx}\\ =\\lim _{\\Delta x\\to 0}^{ }\\frac{f\\left(\\left(x\\ +\\ \\Delta x\\right)\\ -\\ f\\left(x\\ -\\ \\Delta x\\right)\\right)}{2\\Delta x}\\)</p><p><img src=\"/assets/images/blog_images/NumericalDifferentiation/Numerical_Differentiation.jpg\" alt=\"Numerical_Differentiation\" title=\"Numerical Differentiation\" width=\"100%\" height=\"100%\" /></p><p>참조 : <a href=\"https://blog.naver.com/PostView.naver?blogId=mykepzzang&amp;logNo=220072089756&amp;parentCategoryNo=&amp;categoryNo=16&amp;viewDate=&amp;isShowPopularPosts=false&amp;from=postView\">https://blog.naver.com/PostView.naver?blogId=mykepzzang</a></p><h3 id=\"편미분\">편미분</h3><p>예 ) \\(f\\left(x,\\ y\\right)\\ =\\ 2x\\ +\\ 3xy\\ +\\ y3\\) 에 대해,</p><ol>  <li>    <p>변수 x에 대하여 편미분 <br />\\(\\frac{\\partial f\\left(x,y\\right)}{\\partial x}\\ =\\ \\frac{\\partial \\left(2x\\ +\\ 3xy\\ +{y}^3\\right)}{\\partial x}\\ =\\ 2\\ +\\ 3y\\)</p>  </li>  <li>    <p>변수 y에 대하여 편미분 <br />\\(\\frac{\\partial f\\left(x,y\\right)}{\\partial x}\\ =\\ \\frac{\\partial \\left(2x\\ +\\ 3xy\\ +{y}^3\\right)}{\\partial x}\\ =\\ 3x\\ +\\ 3y^2\\)</p>  </li></ol><h3 id=\"chain-rule\">Chain Rule</h3><p>합성함수란 여러 함수로 구성된 함수로서, 이러한 합성함수를 미분하려면 ‘합성함수를 구성하는 각 함수의 미분의 곱’으로 나타내는 chain rule(연쇄 법칙) 이용Ref. NeoWizard PPT 5 page (개인용)</p>",
            "url": "http://localhost:4000/Prerequisite-of-Math/",
            
            
            
            "tags": ["pandoly2","machine learning","AI","Math"],
            
            "date_published": "2022-04-18T00:00:00+00:00",
            "date_modified": "2022-04-18T00:00:00+00:00",
            
                "author": "pandoly2"
            
        },
    
        {
            "id": "http://localhost:4000/what-is-the-machinlearning/",
            "title": "4차 산업혁명의 키워드 인공지능",
            "summary": null,
            "content_text": "4차 산업 혁명2016년 스위스 다보스에서 개최된 세계경제 포럼에서 처음 언급됨학자에 따라 정의는 조금씩 다르지만, 대체로 4차 산업혁명은 모든 것이 연결(Connectivity)되어 있는 환경에서 인공지능(Artificial Intelligence)에 의해 더운 편리하고 지능적인 사회로의 혁신적 변화를 지칭함.인공지능(Artificial Intelligence)란Regression, ClassificationClassification : 분류 Regression : 회귀딥러닝뉴런 - 상호작용  (퍼셉트론 설명)학습을 위한 전제조건(Prerequisite)  프로그래밍 개념  기본 수학 개념  행렬(Matrix) 연산",
            "content_html": "<h2 id=\"4차-산업-혁명\">4차 산업 혁명</h2><p>2016년 스위스 다보스에서 개최된 세계경제 포럼에서 처음 언급됨</p><p>학자에 따라 정의는 조금씩 다르지만, 대체로 <strong>4차 산업혁명</strong>은 모든 것이 <font color=\"red\">연결(Connectivity)</font>되어 있는 환경에서 <font color=\"red\">인공지능(Artificial Intelligence)</font>에 의해 더운 편리하고 지능적인 사회로의 혁신적 변화를 지칭함.</p><h2 id=\"인공지능artificial-intelligence란\">인공지능(Artificial Intelligence)란</h2><p><img src=\"/assets/images/blog_images//WhatIstheAI/WhatIsthe_AI.drawio.png\" alt=\"WhatIsTheAI\" title=\"What is the AI\" /></p><h2 id=\"regression-classification\">Regression, Classification</h2><p>Classification : 분류 <br />Regression : 회귀</p><p><img src=\"/assets/images/blog_images//WhatIstheAI/Regression_Classification.png\" alt=\"WhatIsTheAI\" title=\"Regression &amp; Classification\" width=\"100%\" height=\"100%\" /></p><h2 id=\"딥러닝\">딥러닝</h2><p>뉴런 - 상호작용  (퍼셉트론 설명)</p><h2 id=\"학습을-위한-전제조건prerequisite\">학습을 위한 전제조건(Prerequisite)</h2><ul>  <li>프로그래밍 개념</li>  <li>기본 수학 개념</li>  <li>행렬(Matrix) 연산</li></ul>",
            "url": "http://localhost:4000/what-is-the-machinlearning/",
            
            
            
            "tags": ["pandoly2","machine learning","AI"],
            
            "date_published": "2022-04-18T00:00:00+00:00",
            "date_modified": "2022-04-18T00:00:00+00:00",
            
                "author": "pandoly2"
            
        },
    
        {
            "id": "http://localhost:4000/first-post-content/",
            "title": "First post (글)",
            "summary": null,
            "content_text": "Github page로 만들어본 블로그오늘은 무슨 기능이있는지 알아보자.*기본적으로 markdown kramdown을 사용하여 그린다.Pythons = \"Python syntax highlighting\"print sC언어#include &lt;stdio.h&gt;int main(){\tprintf(\"hello world\");}JAVAclass HelloWorld {    public static void main(String[] args) {        System.out.println(\"Hello, World!\");     }}샾 강조 1샾샾 강조 2샾샾샾 강조 3샾샾샾샾 강조 4링크나는 Pandoly2 입니다.나는 Pandoly2 입니다.  항목 1  항목 2  항목 3  첫번째  두번째참조 : 마크다운 수식 넣는 법 링크끝.",
            "content_html": "<p>Github page로 만들어본 블로그</p><p>오늘은 무슨 기능이있는지 알아보자.</p><p>*기본적으로 markdown kramdown을 사용하여 그린다.</p><p>Python</p><figure class=\"highlight\"><pre><code class=\"language-python\" data-lang=\"python\"><span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"s\">\"Python syntax highlighting\"</span><span class=\"k\">print</span> <span class=\"n\">s</span></code></pre></figure><p>C언어</p><figure class=\"highlight\"><pre><code class=\"language-c\" data-lang=\"c\"><span class=\"cp\">#include</span> <span class=\"cpf\">&lt;stdio.h&gt;</span><span class=\"cp\"></span><span class=\"kt\">int</span> <span class=\"nf\">main</span><span class=\"p\">(){</span>\t<span class=\"n\">printf</span><span class=\"p\">(</span><span class=\"s\">\"hello world\"</span><span class=\"p\">);</span><span class=\"p\">}</span></code></pre></figure><p>JAVA</p><figure class=\"highlight\"><pre><code class=\"language-java\" data-lang=\"java\"><span class=\"kd\">class</span> <span class=\"nc\">HelloWorld</span> <span class=\"o\">{</span>    <span class=\"kd\">public</span> <span class=\"kd\">static</span> <span class=\"kt\">void</span> <span class=\"nf\">main</span><span class=\"o\">(</span><span class=\"nc\">String</span><span class=\"o\">[]</span> <span class=\"n\">args</span><span class=\"o\">)</span> <span class=\"o\">{</span>        <span class=\"nc\">System</span><span class=\"o\">.</span><span class=\"na\">out</span><span class=\"o\">.</span><span class=\"na\">println</span><span class=\"o\">(</span><span class=\"s\">\"Hello, World!\"</span><span class=\"o\">);</span>     <span class=\"o\">}</span><span class=\"o\">}</span></code></pre></figure><h1 id=\"샾-강조-1\">샾 강조 1</h1><h2 id=\"샾샾-강조-2\">샾샾 강조 2</h2><h3 id=\"샾샾샾-강조-3\">샾샾샾 강조 3</h3><h4 id=\"샾샾샾샾-강조-4\">샾샾샾샾 강조 4</h4><p><a href=\"https://pandoly2.github.io\">링크</a></p><p>나는 <strong>Pandoly2</strong> 입니다.나는 <em>Pandoly2</em> 입니다.</p><p><img src=\"/assets/images/blog_images/panda.jpg\" alt=\"Panda Image\" title=\"Panda\" /></p><ul>  <li>항목 1</li>  <li>항목 2</li>  <li>항목 3</li></ul><ol>  <li>첫번째</li>  <li>두번째</li></ol><p>참조 : <a href=\"https://sukwonyun.github.io/jekyll/Jekyll-%ED%85%8C%EB%A7%88%EC%97%90%EC%84%9C-Latex-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0/\">마크다운 수식 넣는 법 링크</a></p><p>끝.</p>",
            "url": "http://localhost:4000/first-post-content/",
            
            
            
            "tags": ["pandoly2","jekyll"],
            
            "date_published": "2022-04-16T00:00:00+00:00",
            "date_modified": "2022-04-16T00:00:00+00:00",
            
                "author": "pandoly2"
            
        }
    
    ]
}