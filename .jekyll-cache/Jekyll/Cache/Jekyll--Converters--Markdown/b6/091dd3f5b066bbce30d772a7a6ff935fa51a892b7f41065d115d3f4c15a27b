I"<ul>
  <li>심층 신경망에서의 학습은 실제값과 우리모델이 예측한 예측값의 차이를 줄여주는것에 목적이 있다.</li>
  <li>우리는 이미 linear regression이나 logistic regression에서 학습할때 손실함수(loss fuction)을 이용하여 예측값 y와 손실함수의 결과값의 차이가 작아 질 수 있도록 W(weight: 가중치)와 b(bias : 바이어스)를 업데이트하면서 성공률을 높이는 방법을 공부했다.</li>
  <li>우리가 linear regression에서 사용한 손실함수(loss function)를 줄이는 기술은 <strong>경사 하강법 (Gradient decent)</strong>이다.</li>
  <li>우리는 심층 신경망에서 보다 더 정확도가 높고, 안정적이며, 학습 속도가 빠른 학습방법 및 기술이 어떤것이 있는지 공부해볼 필요가 있다.</li>
</ul>

<h2 id="1-매개변수-갱신">1. 매개변수 갱신</h2>
<h3 id="경사하강법">경사하강법</h3>
<h4 id="경사하강법-gradient-decent">경사하강법 (Gradient Decent)</h4>
<ul>
  <li>평균제곱 오차 <a href="https://pandoly2.github.io/Regression-and-lossfunction/">[https://pandoly2.github.io/Regression-and-lossfunction/]</a></li>
</ul>

<h4 id="배치-경사-하강법-batch-gradient-decent">배치 경사 하강법 (Batch Gradient Decent)</h4>
<ul>
  <li>경사 하강법의 손실 함수의 기울기 계산에 배치를 전체 학습 데이터셋의 크기와 동일하게 잡는 방법이다.</li>
  <li>즉, 경사 하강법에 사용되는 전체 데이터의 크기가 배치 크기와 동일하다는 것이다.</li>
  <li><strong>장점</strong> : 모든데이터가 대상이므로 안정적으로 수렴한다. <br />
      수렴(최소값)까지 발생하는 가중치 및 바이어스 업데이트 수가 매우 적다.</li>
  <li><strong>단점</strong> : 데이터셋 전체를 대상으로 하다 보니 가중치 및 바이어스가 변경될때마다, 계산해야 할 값이 많으므로, 계산 시간도 길어지고, 소모되는 메모리도 많다. <br />
 지역 최소해(local Minimum)에 빠지면 빠저나오기가 힘들다. <br />
 학습 데이서텟이 커지면 커질수록 시간과 리소스 소모가 크다.</li>
</ul>

<p><img src="/assets/images/blog_images/DeepNeuralNetwork/gd_batch.png" alt="Deep_batch_gradient" />
출처 : 만년필잉크의 데이터 분석 지식 저장소</p>

<h4 id="확률적-경사-하강법-stochastic-gradient-descent-sdg">확률적 경사 하강법 (Stochastic Gradient Descent, SDG)</h4>
<ul>
  <li>학습 데이터셋에서 무작위로 한 개의 샘플 데이터 셋을 추출하고, 그 <strong>샘플에 대해서만 기울기를 계산</strong> 하느것</li>
  <li>
    <p><strong>장점</strong> : 샘플 데이터 셋에 대해서만 기울기를 계산하므로 계산해야할 데이터 수가 적다. <br />
 큰 데이터셋이라도 하나의 샘플씩 계산하기때문에 메모리 소모량이 매우 낮다. <br />
 무작위 샘플 계산으로 불안정하지만 지역 최소값에서 빠져나올 가능성이 BGD보다 높다.</p>
  </li>
  <li><strong>단점</strong> : 무작위 추출로 데이터를 계산하기때문에 불안정하게 움직이며 수렴한다.  <br />
 무작위 샘플링이기때문에 최적해에 도달하지 못할 수도 있다.</li>
</ul>

<p><img src="/assets/images/blog_images/DeepNeuralNetwork/gd_sto.png" alt="Deep_stochastic_gradient" />
출처 : 만년필잉크의 데이터 분석 지식 저장소</p>

<h4 id="미니-배치-경사-하강법-mini-batch-gradient-descent">미니 배치 경사 하강법 (mini-Batch Gradient Descent)</h4>
<ul>
  <li>BGD의 배치크기를 줄이고 SGD를 활용하는 방법 (BGD, SGD 둘다 사용)</li>
  <li>예) 1000개의 전체 학습데이터가 있다면, 100개씩 10개의 mini batch로 나누어 SGD를 진행한다.</li>
  <li>일반적으로 확률적 경사 하강법(SGD)은 실제로 미니 배치 경사 하강법(mini-BGD)이다. 통상 SDG == mini-BDG 이다.</li>
  <li><strong>장점</strong> : mini batch 크기에대해 SDG를 진행할때, 한 mini batch의 평균에대한 경사도로 하강을 진행하기 때문에 SDG(mini batch없는)보다 안정적으로 최적값에 수렴한다.  <br />
 그로인해 안정적으로 최적값에 수렴할수있으나, 지역 최소값(local minimum) 현상이 발생할 수는 있다.</li>
  <li><strong>팁</strong> : 배치 크기는 총 학습 데이터셋의 크기를 배치 크기로 나눴을 때, 딱 떨어지는 크기로 하는것이 좋다.  <br />
 만약 1050개면 뒤에 50개는 버리도록 랜덤으로 데이터셋에서 버리도록 한다.</li>
</ul>

<p><img src="/assets/images/blog_images/DeepNeuralNetwork/gd_mini.png" alt="Deep_mini_gradient" /></p>

<p>참고 : <a href="https://gooopy.tistory.com/70?category=824281">https://gooopy.tistory.com/70?category=824281</a></p>

<h4 id="sgd확률적-경사-하강법은-왜-지그재그로-움직이는가">SGD(확률적 경사 하강법)은 왜 지그재그로 움직이는가</h4>
<ul>
  <li>집합 : 
\(\left\{x\ \in \ {R}^n\ :\ f\left(x\right)\ =\ k\right\}\)
을 함수 f의 k-등위선이라고 부른다.</li>
  <li>점 x를 지나는 <strong>등위선과 gradient Df(x)는 항상 수직</strong>이다. <br />
<img src="/assets/images/blog_images/DeepNeuralNetwork/optimizer_gradient_vertical.png" alt="optimizer_gradient_vertical" /></li>
  <li>등위선에 수직으로 점을 움직여 나가면 지그재그로 움직이게 된다.
따라서, Gradient Descent는 진동 현상을 겪으며 매우 비효율적으로 움직인다.</li>
</ul>

<p>참고 : <a href="https://www.youtube.com/watch?v=5fwD1p9ymx8">https://www.youtube.com/watch?v=5fwD1p9ymx8</a></p>

<h3 id="모멘텀momentum">모멘텀(Momentum)</h3>
<ul>
  <li>모멘텀은 ‘운동량’을 뜻하는 단어로, 물리와 관계가 있다.</li>
  <li>물리계에서는 공이 굴러가는 방향은 중력뿐 아니라 기존의 관성에도 영향을 받는다.</li>
  <li>Momentum은 Gradient Descent에 현재의 관성을 추가한다.</li>
  <li>Momentum의 수식 :  <br />
\(V_n\ =\ \alpha V_{n-1}\ -\eta \ \cdot \ f\left(x\right)',\ \left(v_{-1}\ =\ 0\right)\) <br />
\(V_n\ =\ \alpha V_{n-1}\ -\ \eta \ \cdot \ \frac{\partial E\left(W,\ b\right)}{\partial w},\ \left(v_{-1}\ =\ 0\right)\) <br />
\(X_{n\ +\ 1}\ =\ X_n\ +\ V_n\)</li>
  <li>V_n은 속도 V_n-1은 관성, α는 관성계수</li>
  <li>관성계수 α가 클수록 속도가 관성에 더 많은 영향을 받는다.</li>
</ul>

<h3 id="adagrad">AdaGrad</h3>

<h3 id="adam">Adam</h3>

<h2 id="2-가중치의-초깃값">2. 가중치의 초깃값</h2>

<h2 id="3-배치-정규화">3. 배치 정규화</h2>

<h2 id="4-바른-학습을-위해">4. 바른 학습을 위해</h2>
<h3 id="오버피팅">오버피팅</h3>

<h3 id="가중치-감소">가중치 감소</h3>

<h3 id="드롭아웃">드롭아웃</h3>

<h2 id="5-적절한-하이퍼파라미터-값-찾기">5. 적절한 하이퍼파라미터 값 찾기</h2>

:ET