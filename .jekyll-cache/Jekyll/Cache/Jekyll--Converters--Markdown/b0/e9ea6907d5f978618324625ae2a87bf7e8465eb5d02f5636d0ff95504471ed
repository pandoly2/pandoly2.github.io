I")<h2 id="손실함수">손실함수</h2>
<ul>
  <li>
    <p>손실함수는 오차의 평균값을 나타내기 때문에, 손실함수가 최소값을 갖는다는 것은 실제 정답과 계산 값의 차이인 오차가 최소가 되어, 미지의 데이터에 대해서 결과를 잘 예측 할 수 있다는 것을 의미함.</p>
  </li>
  <li>
    <p>손실함수는 W, b 에 영향을 받기 때문에, <strong>손실함수가 최소가 되는 가중치 W와 바이어스 b를 찾는 것</strong>이 regression을 구현하는 최종 목표.</p>
  </li>
</ul>

<h4 id="손실함수-식">손실함수 식</h4>

\[loss\ function= E(W,\ b) = \frac{\left({t}_1\ -\ y_1\right)^2\ +\left({t}_2\ -\ y_2\right)^2\ +\ \left({t}_3\ -\ y_3\right)^2\ +\ ...\ \ +\ \left({t}_n\ -\ y_n\right)^2\ \ }{n}\]

\[=\frac{\left[{t}_1\ -\ \left(Wx_1\ +\ b\right)\right]^2\ +\ \left[{t}_2\ -\ \left(Wx_2\ +\ b\right)\right]^2\ +\ ...\ +\ \left[{t}_n\ -\ \left(Wx_n\ +\ b\right)\right]^2\ \ }{n}\]

\[=\frac{1}{n}\sum _{i=1}^n\left[t_i\ -\ \left(Wx_i\ +\ b\right)\right]^2\]

<h3 id="손실함수-최적화-기술">손실함수 최적화 기술</h3>
<ul>
  <li>경사하강법</li>
  <li>배치 경사하강법</li>
  <li>확률적 경사하강법</li>
  <li>미니 배치 경사하강법</li>
  <li>모멘텀</li>
  <li>아담</li>
  <li>참고 : <a href="https://truman.tistory.com/164">https://truman.tistory.com/164</a></li>
</ul>

<h2 id="경사하강법-이해하기">경사하강법 이해하기</h2>
<p><strong>가정</strong> <br />
 E(W, b)에서 바이어스 b = 0으로 가정 (계산을 쉽게하고 손실함수의 모양을 파악하기 위해)</p>

<ul>
  <li>다음과 같은 training data set가 있을때, <br />
<img src="/assets/images/blog_images/Gradient/gradient_training_data.png" alt="gradient_training_data" />
출처 : NeoWizard</li>
  <li>W 값에대한 손실함수 E(W, b) 계산 결과는 다음과 같다. <br />
<img src="/assets/images/blog_images/Gradient/gradient_loss_functiono_result.png" alt="gradient_loss_function_result" />
출처 : NeoWizard</li>
  <li>
    <p>loss function의 계산 결과를 그래프로 나타냈을때 다음과 같다. <br />
<img src="/assets/images/blog_images/Gradient/gradient_data_of_loss_fuction_result.png" alt="gradient_data_of_loss_result" />
출처 : NeoWizard
<img src="/assets/images/blog_images/Gradient/gradient_graph.png" alt="gradient_graph" />
출처 : NeoWizard</p>
  </li>
  <li>그러면, 우리가 어떤 가중치를 임의로 설정했을때 최소값을 찾아가는 방법은 뭘까?</li>
  <li>아래와같은 방법을 생각해 볼 수 있다. <br />
 <img src="/assets/images/blog_images/Gradient/gradient_decent.png" alt="gradient_graph" />
 출처 : NeoWizard</li>
  <li>방법은 아래와 같이 기울기가 <strong>작아지는쪽</strong>으로 이동 시키면된다. (가장낮은쪽으로 이동)   <br />
 W 에서의 편미분 𝜕E(W)/𝜕W 가 해당 W 에서 기울기를 나타냄 <br />
 𝜕E(W)/𝜕W 양수값을 갖는다면 W는 왼쪽으로 이동 <br />
 𝜕E(W)/𝜕W 음수값을 갖는다면 W는 오른쪽으로 이동</li>
  <li>식,<br />
 W(weight) 가중치 : 
 \(w\ =\ w\ -\ \alpha \ \cdot \ \frac{\partial E\left(W,\ b\right)}{\partial w}\)
, b(bias) 바이어스 : 
\(b\ =\ b\ -\ \alpha \ \cdot \ \frac{\partial E\left(W,\ b\right)}{\partial b}\)</li>
</ul>

<p>(※ \(\alpha\) 는 학습율(learning rate)라고 부르며, W 값의 감소 또는 증가 되는 비율을 나타냄) <br />
 <img src="/assets/images/blog_images/Gradient/gradient_decent2.png" alt="gradient_graph" />
 출처 : NeoWizard</p>
<ul>
  <li>이처럼, W에서의 직선의 기울기인 미분 값을 이용하여, 그 값이 작아지는 방향으로 진행하여 손실함수 최소값을 찾는 방법을 <font color="red"> 경사하강법 (gradient decent algorithm) </font> 이라고 한다.</li>
</ul>

<h3 id="전체-학습-flow-linear-regression">전체 학습 Flow (linear regression)</h3>
<p><img src="/assets/images/blog_images/Gradient/Gradient_decent_flow(linear_regression).png" alt="gradient_flow" width="100%" height="100%" /></p>

:ET