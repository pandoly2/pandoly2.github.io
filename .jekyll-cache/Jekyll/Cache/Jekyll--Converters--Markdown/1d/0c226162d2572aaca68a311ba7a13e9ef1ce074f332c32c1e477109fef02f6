I"<ol>
  <li>linear Regression</li>
  <li>손실함수</li>
  <li>경사하강법</li>
  <li>logistic Regression</li>
</ol>

<h2 id="회귀-regression">회귀 (Regression)</h2>
<p>※ 용어 :  회귀(Regression)</p>
<ul>
  <li>원래 뜻은 예전으로 되돌아간다는 의미입니다. 영국의 유전학자 프랜시스 골턴이 부모와 자녀들 키의 연관 관계를 연구하다 보니, 개개인의 키는 결국 전체 키의 평균으로 수렴하는 경향이 있다는 걸 발견하고는 자신의 방법론에 ‘회귀 분석’이란 이름을 붙였다고 합니다.</li>
  <li>‘회귀’라는 용어는 1885년 영국의 과학자 갈톤(F. Galton)이 발표한 ‘유전에 의하여 보통사람의 신장으로 회귀(Regression toward Meiocrity in Hereditary Stature’라는 논문에서 비롯되었다. 그는 아들의 키와 부모의 평균 키와의 관계를 분석하였는데, 부모의 키가 매우 클 때(또는 작을 때) 아들의 키는 일반적으로 평균키보다는 크지만(작지만) 그들의 부모만큼 크(작)지는 않다는 결론이다. 즉 부모의 키가 크(작)더라도 그 자식들은 결국 보통키로 회귀(돌아간다)한다는 뜻이다.
[출처] 회귀분석의 유래 : 대체 왜 Regression(회귀)이라고 불릴까?|작성자 바른인간</li>
</ul>

<h3 id="선형-회귀-linear-regression">선형 회귀 (Linear Regression)</h3>
<ul>
  <li>Trainig Data를 이용하여 데이터의 특성과 상관관계 등을 파악하고, 그 결과를 바탕으로 Training Data에 없는 미지의 데이터가 주어졌을 경우에, 그 결과를 연속적인 (숫자) 값으로 예측 하는것</li>
</ul>

<p>(예) 공부시간과 시험성적 관계, 집 평수와 집 가격 관계 등</p>

<p><img src="/assets/images/blog_images/Regression/linearRegression_01.PNG" alt="linearRegression_01" width="100%" height="100%" />
출처 : NeoWizard</p>

<ul>
  <li>학습 데이터는 입력(x)인 공부시간에 비례해서 출력(y)인 시험성적도 증가하는 경향이 있음
즉, 입력(x)과 출력(y)은 <strong>y = Wx + b</strong> 형태로 나타낼 수 있음</li>
</ul>

<p><img src="/assets/images/blog_images/Regression/linearRegression_02.PNG" alt="linearRegression_01" width="100%" height="100%" />
출처 : NeoWizard</p>

<ul>
  <li><strong>y = Wx + b</strong>를 만족하는 다양한 <strong>1, 2, 3</strong>과 같은 직선중, Training data의 특성을 가장 잘 표현할 수 있는 가중치 <strong>W(기울기)</strong>, <strong>바이어스 b(y 절편 or 편차)</strong>를 찾는 것이 학습(Learning) 개념임
※ 용어 정리 : 머신러닝에서는 W는 가중치(weight), b는 바이어스(bias) 라고 함.</li>
</ul>

<h4 id="최적의-wweight와-bbias를-찾는방법">최적의 W(weight)와 b(bias)를 찾는방법</h4>
<p><img src="/assets/images/blog_images/Regression/linearRegression_03.PNG" alt="linearRegression_01" width="100%" height="100%" />
출처 : NeoWizard</p>

<ul>
  <li>Training data의 정답(t)과 직선 y = Wx + b 값의 차이인 <font color="red"> 오차(error) = t - y = t - (Wx + b)</font></li>
  <li>
    <p>오차가 크다면, 우리가 임의로 설정한 직선의 가중치와 바이어스 값이 잘못된 것이고, 오차가 작다면 직선의 가중치와 바이어스 값이 잘 된 것이기 때문에 미래 값 예측도 정확할 수 있다고 예상할 수 있음.</p>
  </li>
  <li>즉, 머신러닝의 Regression 시스템은, <strong>모든 데이터</strong>의 <font color="red"> 오차(error) = t - y = t - (Wx + b)</font>의 합이 최소가 되서, 미래 값을 잘 예측할 수 있는 가중치 W와 바이어스 b값을 찾아야 한다.</li>
</ul>

<h3 id="손실함수-loss-function">손실함수 (loss function)</h3>
<ul>
  <li>위의 <strong>최적의 W(weight)와 b(bias)를 찾는방법</strong>에서 보았듯이 최적의 W(가중치)와 b(바이어스)를 찾는 것은 모든 데이터의 오차(error)가 작은 값을 찾으면된다.</li>
  <li>즉, <font color="red">손실함수 (loss function)</font>는, training data의 정답(t)과 입력(x)에 대한 계산 값 y의 차이(error)를 모두 더해 수식으로 나타낸 것.</li>
</ul>

<p><img src="/assets/images/blog_images/Regression/linearRegression_03.PNG" alt="linearRegression_01" width="100%" height="100%" />
출처 : NeoWizard</p>

<ul>
  <li>주의 : 각각의 오차를 모두 더해서 손실함수(loss function)을 구하면 각각의 오차가 (+), (-) 등이 동시에 존재하기 때문에 오차의 합이 0이 나올 수도 있음.</li>
  <li>즉, 0 이라는 것이 최소 오차 값인지 아닌지를 판별하는 것이 어려움.</li>
  <li>그래서, <font color="red">손실함수에서 오차(error)를 계산할 때는 양변에 제곱을 사용하여 계산함.</font></li>
  <li>
\[{\left(t-y\right)}^2\ =\ {\left(t\ -\ {\left[Wx\ +\ b\right]}\right)}^2\]
  </li>
  <li>즉, 오차는 언제나 양수이며, 제곱을 하기때문에 정답과 계산값 차이가 크다면, <strong>제곱에 의해 오차는 더 큰 값을 가지게 되어 머신러닝 학습에 있어 장점을 가짐</strong>.</li>
</ul>

<h4 id="손실함수-식">손실함수 식</h4>

\[loss\ function=\frac{\left({t}_1\ -\ y_1\right)^2\ +\left({t}_2\ -\ y_2\right)^2\ +\ \left({t}_3\ -\ y_3\right)^2\ +\ ...\ \ +\ \left({t}_n\ -\ y_n\right)^2\ \ }{n}
=\frac{\left[{t}_1\ -\ \left(Wx_1\ +\ b\right)\right]^2\ +\ \left[{t}_2\ -\ \left(Wx_2\ +\ b\right)\right]^2\ +\ ...\ +\ \left[{t}_n\ -\ \left(Wx_n\ +\ b\right)\right]^2\ \ }{n}\]

:ET