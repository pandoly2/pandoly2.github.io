I"<ul>
  <li>심층 신경망에서의 학습은 실제값과 우리모델이 예측한 예측값의 차이를 줄여주는것에 목적이 있다.</li>
  <li>우리는 이미 linear regression이나 logistic regression에서 학습할때 손실함수(loss fuction)을 이용하여 예측값 y와 손실함수의 결과값의 차이가 작아 질 수 있도록 W(weight: 가중치)와 b(bias : 바이어스)를 업데이트하면서 성공률을 높이는 방법을 공부했다.</li>
  <li>우리가 linear regression에서 사용한 손실함수(loss function)를 줄이는 기술은 <strong>경사 하강법 (Gradient decent)</strong>이다.</li>
  <li>우리는 심층 신경망에서 보다 더 정확도가 높고, 안정적이며, 학습 속도가 빠른 학습방법 및 기술이 어떤것이 있는지 공부해볼 필요가 있다.</li>
</ul>

<h2 id="1-매개변수-갱신">1. 매개변수 갱신</h2>
<h3 id="경사하강법">경사하강법</h3>
<h4 id="경사하강법-gradient-decent">경사하강법 (Gradient Decent)</h4>
<ul>
  <li>평균제곱 오차 <a href="https://pandoly2.github.io/Regression-and-lossfunction/">[https://pandoly2.github.io/Regression-and-lossfunction/]</a></li>
</ul>

<h4 id="배치-경사-하강법-batch-gradient-decent">배치 경사 하강법 (Batch Gradient Decent)</h4>
<ul>
  <li>경사 하강법의 손실 함수의 기울기 계산에 배치를 전체 학습 데이터셋의 크기와 동일하게 잡는 방법이다.</li>
  <li>즉, 경사 하강법에 사용되는 전체 데이터의 크기가 배치 크기와 동일하다는 것이다.</li>
  <li>장점 : 모든데이터가 대상이므로 안정적으로 수렴한다. <br />
      수렴(최소값)까지 발생하는 가중치 및 바이어스 업데이트 수가 매우 적다.</li>
  <li>단점 : 데이터셋 전체를 대상으로 하다 보니 가중치 및 바이어스가 변경될때마다, 계산해야 할 값이 많으므로, 계산 시간도 길어지고, 소모되는 메모리도 많다. <br />
 지역 최소해(local Minimum)에 빠지면 빠저나오기가 힘들다. <br />
 학습 데이서텟이 커지면 커질수록 시간과 리소스 소모가 크다.</li>
</ul>

<h4 id="확률적-경사-하강법-stochastic-gradient-descent-sdg">확률적 경사 하강법 (Stochastic Gradient Descent, SDG)</h4>

<h4 id="미니-배치-경사-하강법-mini-batch-gradient-descent">미니 배치 경사 하강법 (mini-Batch Gradient Descent)</h4>

<h3 id="모멘텀momentum">모멘텀(Momentum)</h3>

<h3 id="adagrad">AdaGrad</h3>

<h3 id="adam">Adam</h3>

<h2 id="2-가중치의-초깃값">2. 가중치의 초깃값</h2>

<h2 id="3-배치-정규화">3. 배치 정규화</h2>

<h2 id="4-바른-학습을-위해">4. 바른 학습을 위해</h2>
<h3 id="오버피팅">오버피팅</h3>

<h3 id="가중치-감소">가중치 감소</h3>

<h3 id="드롭아웃">드롭아웃</h3>

<h2 id="5-적절한-하이퍼파라미터-값-찾기">5. 적절한 하이퍼파라미터 값 찾기</h2>

<h2 id="back-propagation-오차역전법">Back Propagation (오차역전법)</h2>

:ET