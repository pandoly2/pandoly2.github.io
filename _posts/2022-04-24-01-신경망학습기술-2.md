---
title: "심층 신경망(딥러닝) 학습 관련 기술(optimization) - 가중치 초깃값"
date: 2022-04-24T00:00:00+00:00
author: pandoly2
layout: post
permalink: /optimization-parameter-2/
categories: Genel
tags: [pandoly2, machine learning, AI]
use_math: true
---

## 2. 가중치의 초깃값
 - 학습이 이루어지지않은 초기에 W(weight)와 b(bias)를 어떻게 설정하는게 좋을까.
 - 초기값 설정의 예
 1. 가중치(Weight)를 평균이 0, 표준편차가 1인 정규분포로 초기화할때의 각층의 데이터 활성화값 분포
![dl2_initial_param_ex](/assets/images/blog_images/DeepNeuralNetwork2/fig%206-10.png)   
 문제 : **Vanishing Gradient Problem**
 2. 가중치(Weight)를 평균이 0, 표준편차가 0.01인 정규분포로 초기화할때의 각층의 데이터 확성화값 분포
![dl2_initial_param_ex](/assets/images/blog_images/DeepNeuralNetwork2/fig%206-11.png)   
 문제 : **표현력 제한**

### 표현력 제한
 - 첫번째 가중치 행렬의 각 행이 동일하고 두번째부터는 각 단계마다 가중치가 동일한 신경망을 가정해보면, 이 신경망은 layer 1부터 뉴런의 개수가 모두 1개인 신경망과 본질적으로 동일하다.
 - 즉, 정보를 담을 수 있는 노드(or 뉴런)의 가중치가 제한된다. 
 - 표준편차를 너무작게 잡게되면 정보를 담을 수 있는 노드가 부족해지는것을 주의해야 한다. 

![dl2_initial_param_limit_expression](/assets/images/blog_images/DeepNeuralNetwork2/%ED%91%9C%ED%98%84%EB%A0%A5%EC%A0%9C%ED%95%9C.png)

### Vanishing gradient problem
 - sigmoid 미분값을 전파함.
 - 데이터 셋이 y값 1/0부분에 군집되어있음.
 - sigmoid 미분값을 계속 곱하다보면 0에 수렴하는 값이됨.
 - 층이 뒤로갈수록 gradient값이 작아서 학습이 이루어 지지않음.
 - 인공신경망의 2번째 겨울에 하나의 주된 원인 (꽤 오랫동안 문제였다고함)
![dl2_vanish_gradient](/assets/images/blog_images/DeepNeuralNetwork2/vanishing_gradient.PNG)

### LeCun 초기값 / Xavier 초기값
 - 활성화 함수 Sigmoid일때,
 - 평균이 0인 정규분포, 입력 뉴런과 출력 뉴런의 평균의 역수 : 분산   
![dl_lecun_xavier](/assets/images/blog_images/DeepNeuralNetwork2/Lecun_Xavier.PNG)

 - 가중치 초가값으로 LeCun/Xavier 초기값을 이용했을때 각층의 데이터 활성화값 분포
![dl_xavier_n](/assets/images/blog_images/DeepNeuralNetwork2/Xavier_n.PNG)   

### He 초기값
 - 활성화 함수 ReLU 일때,    
![dl_he_initial](/assets/images/blog_images/DeepNeuralNetwork2/he_initial.PNG)   
 - Lecun 초기값을 사용했을때,   
![dl_ReLU_lecun](/assets/images/blog_images/DeepNeuralNetwork2/relu_lecun.PNG)   
 - He 초기값을 사용했을때,    
![dl_ReLU_He](/assets/images/blog_images/DeepNeuralNetwork2/relu_he.PNG)   


## 3. 배치 정규화

## 4. 바른 학습을 위해
### 오버피팅

### 가중치 감소

### 드롭아웃

## 5. 적절한 하이퍼파라미터 값 찾기
