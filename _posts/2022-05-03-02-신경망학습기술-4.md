---
title: "심층 신경망(딥러닝) 학습 관련 기술(optimization) - 바른 학습을 위해"
date: 2022-05-03T00:00:00+00:00
author: pandoly2
layout: post
permalink: /optimization-parameter-4/
categories: Genel
tags: [pandoly2, machine learning, AI]
use_math: true
---

## 4. 바른 학습을 위해
### 오버피팅
 - 학습 데이터에 너무 최적화 되어 weight 값이 잡히고, 이후 학습 데이터가 아닌 새로운 데이터에는 올바른 값을 내보내지 못하는 현상. 매개변수에 비해 상대적으로 훈련 데이터 수가 적을 때 일어난다.

![nn_overfitting_png](/assets/images/blog_images/DeepNeuralNetwork4/overfitting.PNG) 

### Regularization
 - 데이터에 너무 과적합되어 모델이 피팅되었으니, 이를 좀 덜 적합하게 하고, 이후 새로운 데이터에도 일반적으로 들어맞는 모델을 만들어야 한다. 이 때, 과적합이 아닌 일반성을 띄게 해주는 기법

#### 가중치 감소
![nn_weight_decay_png](/assets/images/blog_images/DeepNeuralNetwork4/weight_decay.PNG)   
 - weight decay 기하학적 설명
![nn_weight_decay_desc](/assets/images/blog_images/DeepNeuralNetwork4/weight_decay_desc.PNG)   
#### 드롭아웃
 - 학습시 마다 뉴런을 랜덤으로 비활성화 시킨다. 
 - 비율은 Hyperparameter로 초기 학습시 정한다.
 - 놀랍게도 overfitting을 피하기위해서 많이 쓰는 기법이다. 
 - 두가지 해석이있다. (증명이아닌듯)
    1. 앙상블 기법
       한사람이 독단적으로 결정하는것이아니라, 여러 사람이 모여서 결정하는 것.   
       여러 NN을 학습시켜서 다수의 판단을 믿는 방법.
    2. 은행원
       딥러닝의 대가(힌트교수)의 해석
       한 은행원이 오래 근무를하다보면 담합을한다 (cheating)
       같은 관점에서 노드들이 담합을 하는것으로보고 그 관계를 끊는다.
       이로서 성능이 향상된다.

![nn_dropout_png](/assets/images/blog_images/DeepNeuralNetwork4/dropout.PNG)   
