---
title: "ê²½ì‚¬í•˜ê°•ë²• (Gradient decent algorithm)"
date: 2022-04-19T00:00:00+00:00
author: pandoly2
layout: post
permalink: /gradient-decent/
categories: Genel
tags: [pandoly2, machine learning, AI]
use_math: true
---

## ì†ì‹¤í•¨ìˆ˜ 
 - ì†ì‹¤í•¨ìˆ˜ëŠ” ì˜¤ì°¨ì˜ í‰ê· ê°’ì„ ë‚˜íƒ€ë‚´ê¸° ë•Œë¬¸ì—, ì†ì‹¤í•¨ìˆ˜ê°€ ìµœì†Œê°’ì„ ê°–ëŠ”ë‹¤ëŠ” ê²ƒì€ ì‹¤ì œ ì •ë‹µê³¼ ê³„ì‚° ê°’ì˜ ì°¨ì´ì¸ ì˜¤ì°¨ê°€ ìµœì†Œê°€ ë˜ì–´, ë¯¸ì§€ì˜ ë°ì´í„°ì— ëŒ€í•´ì„œ ê²°ê³¼ë¥¼ ì˜ ì˜ˆì¸¡ í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•¨.

 - ì†ì‹¤í•¨ìˆ˜ëŠ” W, b ì— ì˜í–¥ì„ ë°›ê¸° ë•Œë¬¸ì—, **ì†ì‹¤í•¨ìˆ˜ê°€ ìµœì†Œê°€ ë˜ëŠ” ê°€ì¤‘ì¹˜ Wì™€ ë°”ì´ì–´ìŠ¤ bë¥¼ ì°¾ëŠ” ê²ƒ**ì´ regressionì„ êµ¬í˜„í•˜ëŠ” ìµœì¢… ëª©í‘œ.

#### ì†ì‹¤í•¨ìˆ˜ ì‹

$$
loss\ function= E(W,\ b) = \frac{\left({t}_1\ -\ y_1\right)^2\ +\left({t}_2\ -\ y_2\right)^2\ +\ \left({t}_3\ -\ y_3\right)^2\ +\ ...\ \ +\ \left({t}_n\ -\ y_n\right)^2\ \ }{n}
$$   
   
$$
=\frac{\left[{t}_1\ -\ \left(Wx_1\ +\ b\right)\right]^2\ +\ \left[{t}_2\ -\ \left(Wx_2\ +\ b\right)\right]^2\ +\ ...\ +\ \left[{t}_n\ -\ \left(Wx_n\ +\ b\right)\right]^2\ \ }{n}
$$
   
$$
=\frac{1}{n}\sum _{i=1}^n\left[t_i\ -\ \left(Wx_i\ +\ b\right)\right]^2
$$

### ì†ì‹¤í•¨ìˆ˜ ìµœì í™” ê¸°ìˆ 
 - ê²½ì‚¬í•˜ê°•ë²•
 - ë°°ì¹˜ ê²½ì‚¬í•˜ê°•ë²•
 - í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•
 - ë¯¸ë‹ˆ ë°°ì¹˜ ê²½ì‚¬í•˜ê°•ë²•
 - ëª¨ë©˜í…€
 - ì•„ë‹´   
 - ì°¸ê³  : [https://truman.tistory.com/164](https://truman.tistory.com/164)

## ê²½ì‚¬í•˜ê°•ë²• ì´í•´í•˜ê¸°
 **ê°€ì •**   
 E(W, b)ì—ì„œ ë°”ì´ì–´ìŠ¤ b = 0ìœ¼ë¡œ ê°€ì • (ê³„ì‚°ì„ ì‰½ê²Œí•˜ê³  ì†ì‹¤í•¨ìˆ˜ì˜ ëª¨ì–‘ì„ íŒŒì•…í•˜ê¸° ìœ„í•´)

 - ë‹¤ìŒê³¼ ê°™ì€ training data setê°€ ìˆì„ë•Œ,   
![gradient_training_data](/assets/images/blog_images/Gradient/gradient_training_data.png)
ì¶œì²˜ : NeoWizard
 - W ê°’ì—ëŒ€í•œ ì†ì‹¤í•¨ìˆ˜ E(W, b) ê³„ì‚° ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.   
![gradient_loss_function_result](/assets/images/blog_images/Gradient/gradient_loss_functiono_result.png)
ì¶œì²˜ : NeoWizard
 - loss functionì˜ ê³„ì‚° ê²°ê³¼ë¥¼ ê·¸ë˜í”„ë¡œ ë‚˜íƒ€ëƒˆì„ë•Œ ë‹¤ìŒê³¼ ê°™ë‹¤.   
![gradient_data_of_loss_result](/assets/images/blog_images/Gradient/gradient_data_of_loss_fuction_result.png)
ì¶œì²˜ : NeoWizard
![gradient_graph](/assets/images/blog_images/Gradient/gradient_graph.png)
ì¶œì²˜ : NeoWizard

 - ê·¸ëŸ¬ë©´, ìš°ë¦¬ê°€ ì–´ë–¤ ê°€ì¤‘ì¹˜ë¥¼ ì„ì˜ë¡œ ì„¤ì •í–ˆì„ë•Œ ìµœì†Œê°’ì„ ì°¾ì•„ê°€ëŠ” ë°©ë²•ì€ ë­˜ê¹Œ? 
 - ì•„ë˜ì™€ê°™ì€ ë°©ë²•ì„ ìƒê°í•´ ë³¼ ìˆ˜ ìˆë‹¤.   
 ![gradient_graph](/assets/images/blog_images/Gradient/gradient_decent.png)
 ì¶œì²˜ : NeoWizard
 - ë°©ë²•ì€ ì•„ë˜ì™€ ê°™ì´ ê¸°ìš¸ê¸°ê°€ **ì‘ì•„ì§€ëŠ”ìª½**ìœ¼ë¡œ ì´ë™ ì‹œí‚¤ë©´ëœë‹¤. (ê°€ì¥ë‚®ì€ìª½ìœ¼ë¡œ ì´ë™)     
 W ì—ì„œì˜ í¸ë¯¸ë¶„ ğœ•E(W)/ğœ•W ê°€ í•´ë‹¹ W ì—ì„œ ê¸°ìš¸ê¸°ë¥¼ ë‚˜íƒ€ëƒ„   
 ğœ•E(W)/ğœ•W ì–‘ìˆ˜ê°’ì„ ê°–ëŠ”ë‹¤ë©´ WëŠ” ì™¼ìª½ìœ¼ë¡œ ì´ë™   
 ğœ•E(W)/ğœ•W ìŒìˆ˜ê°’ì„ ê°–ëŠ”ë‹¤ë©´ WëŠ” ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™     
 - ì‹,  
 W(weight) ê°€ì¤‘ì¹˜ : 
 $$
w\ =\ w\ -\ \alpha \ \cdot \ \frac{\partial E\left(W,\ b\right)}{\partial w}   
 $$
, b(bias) ë°”ì´ì–´ìŠ¤ : 
$$
b\ =\ b\ -\ \alpha \ \cdot \ \frac{\partial E\left(W,\ b\right)}{\partial b}
$$
    
(â€» $$ \alpha $$ ëŠ” í•™ìŠµìœ¨(learning rate)ë¼ê³  ë¶€ë¥´ë©°, W ê°’ì˜ ê°ì†Œ ë˜ëŠ” ì¦ê°€ ë˜ëŠ” ë¹„ìœ¨ì„ ë‚˜íƒ€ëƒ„)   
 ![gradient_graph](/assets/images/blog_images/Gradient/gradient_decent2.png)
 ì¶œì²˜ : NeoWizard
 - ì´ì²˜ëŸ¼, Wì—ì„œì˜ ì§ì„ ì˜ ê¸°ìš¸ê¸°ì¸ ë¯¸ë¶„ ê°’ì„ ì´ìš©í•˜ì—¬, ê·¸ ê°’ì´ ì‘ì•„ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ ì§„í–‰í•˜ì—¬ ì†ì‹¤í•¨ìˆ˜ ìµœì†Œê°’ì„ ì°¾ëŠ” ë°©ë²•ì„ <font color=red> ê²½ì‚¬í•˜ê°•ë²• (gradient decent algorithm) </font> ì´ë¼ê³  í•œë‹¤.
 
### ì „ì²´ í•™ìŠµ Flow (linear regression)
![gradient_flow](/assets/images/blog_images/Gradient/Gradient_decent_flow(linear_regression).png){: width="100%" height="100%"}



 














