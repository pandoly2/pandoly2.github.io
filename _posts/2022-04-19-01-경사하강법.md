---
title: "경사하강법 (Gradient decent algorithm)"
date: 2022-04-19T00:00:00+00:00
author: pandoly2
layout: post
permalink: /gradient-decent/
categories: Genel
tags: [pandoly2, machine learning, AI]
use_math: true
---

## 손실함수 
 - 손실함수는 오차의 평균값을 나타내기 때문에, 손실함수가 최소값을 갖는다는 것은 실제 정답과 계산 값의 차이인 오차가 최소가 되어, 미지의 데이터에 대해서 결과를 잘 예측 할 수 있다는 것을 의미함.

 - 손실함수는 W, b 에 영향을 받기 때문에, **손실함수가 최소가 되는 가중치 W와 바이어스 b를 찾는 것**이 regression을 구현하는 최종 목표.

#### 손실함수 식

$$
loss\ function= E(W,\ b) = \frac{\left({t}_1\ -\ y_1\right)^2\ +\left({t}_2\ -\ y_2\right)^2\ +\ \left({t}_3\ -\ y_3\right)^2\ +\ ...\ \ +\ \left({t}_n\ -\ y_n\right)^2\ \ }{n}
$$   
   
$$
=\frac{\left[{t}_1\ -\ \left(Wx_1\ +\ b\right)\right]^2\ +\ \left[{t}_2\ -\ \left(Wx_2\ +\ b\right)\right]^2\ +\ ...\ +\ \left[{t}_n\ -\ \left(Wx_n\ +\ b\right)\right]^2\ \ }{n}
$$
   
$$
=\frac{1}{n}\sum _{i=1}^n\left[t_i\ -\ \left(Wx_i\ +\ b\right)\right]^2
$$

### 손실함수 최적화 기술
 - 경사하강법
 - 배치 경사하강법
 - 확률적 경사하강법
 - 미니 배치 경사하강법
 - 모멘텀
 - 아담   
 - 참고 : [https://truman.tistory.com/164](https://truman.tistory.com/164)

## 경사하강법 이해하기
 **가정**   
 E(W, b)에서 바이어스 b = 0으로 가정 (계산을 쉽게하고 손실함수의 모양을 파악하기 위해)

 - 다음과 같은 training data set가 있을때,   
![gradient_training_data](/assets/images/blog_images/Gradient/gradient_training_data.png)
출처 : NeoWizard
 - W 값에대한 손실함수 E(W, b) 계산 결과는 다음과 같다.   
![gradient_loss_function_result](/assets/images/blog_images/Gradient/gradient_loss_functiono_result.png)
출처 : NeoWizard
 - loss function의 계산 결과를 그래프로 나타냈을때 다음과 같다.   
![gradient_data_of_loss_result](/assets/images/blog_images/Gradient/gradient_data_of_loss_fuction_result.png)
출처 : NeoWizard
![gradient_graph](/assets/images/blog_images/Gradient/gradient_graph.png)
출처 : NeoWizard

 - 그러면, 우리가 어떤 가중치를 임의로 설정했을때 최소값을 찾아가는 방법은 뭘까? 
 - 아래와같은 방법을 생각해 볼 수 있다.   
 ![gradient_graph](/assets/images/blog_images/Gradient/gradient_decent.png)
 출처 : NeoWizard
 - 방법은 아래와 같이 기울기가 **작아지는쪽**으로 이동 시키면된다. (가장낮은쪽으로 이동)     
 W 에서의 편미분 𝜕E(W)/𝜕W 가 해당 W 에서 기울기를 나타냄   
 𝜕E(W)/𝜕W 양수값을 갖는다면 W는 왼쪽으로 이동   
 𝜕E(W)/𝜕W 음수값을 갖는다면 W는 오른쪽으로 이동     
 - 식,  
 W(weight) 가중치 : 
 $$
w\ =\ w\ -\ \alpha \ \cdot \ \frac{\partial E\left(W,\ b\right)}{\partial w}   
 $$
, b(bias) 바이어스 : 
$$
b\ =\ b\ -\ \alpha \ \cdot \ \frac{\partial E\left(W,\ b\right)}{\partial b}
$$
    
(※ $$ \alpha $$ 는 학습율(learning rate)라고 부르며, W 값의 감소 또는 증가 되는 비율을 나타냄)   
 ![gradient_graph](/assets/images/blog_images/Gradient/gradient_decent2.png)
 출처 : NeoWizard
 - 이처럼, W에서의 직선의 기울기인 미분 값을 이용하여, 그 값이 작아지는 방향으로 진행하여 손실함수 최소값을 찾는 방법을 <font color=red> 경사하강법 (gradient decent algorithm) </font> 이라고 한다.
 
### 전체 학습 Flow (linear regression)
![gradient_flow](/assets/images/blog_images/Gradient/Gradient_decent_flow(linear_regression).png){: width="100%" height="100%"}



 














